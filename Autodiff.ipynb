{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automatic Differentiation in UnifyML\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/holl-/UnifyML/blob/main/docs/Autodiff.ipynb)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üåê **UnifyML**](https://github.com/holl-/UnifyML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://holl-.github.io/UnifyML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://holl-.github.io/UnifyML/unifyml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/holl-/UnifyML/blob/main/docs/Examples.ipynb) [**Examples**](https://holl-.github.io/UnifyML/Examples.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unifyml\n",
    "\n",
    "from unifyml import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like Jax, UnifyML provides a functional approach to automatic differentiation.\n",
    "You can obtain the derivative of a function using [`math.gradient()`](unifyml/math#unifyml.math.gradient).\n",
    "Note that we have to set the backend to either Jax, PyTorch or TensorFlow since NumPy does not support automatic differentiation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PhD\\UnifyML\\unifyml\\math\\_functional.py:473: RuntimeWarning: Using torch for gradient computation because numpy does not support jacobian()\n",
      "  warnings.warn(f\"Using {math.default_backend()} for gradient computation because {key.backend} does not support jacobian()\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor(1., device='cuda:0', grad_fn=<MulBackward0>),\n tensor(2., device='cuda:0'))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.use('torch')\n",
    "\n",
    "def loss_function(x, y):\n",
    "    return x ** 2 * y\n",
    "\n",
    "dx_function = math.gradient(loss_function, wrt='x')\n",
    "dx_function(x=1., y=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, the gradient function also returns the output of the original function.\n",
    "In the above case, the loss value is `1` and the gradient is `2`.\n",
    "\n",
    "We can get the gradient only by passing `get_output=False`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2., device='cuda:0')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx_function = math.gradient(loss_function, wrt='x', get_output=False)\n",
    "dx_function(x=1., y=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we passed in native types (not UnifyML tensors), we also get native types as a result.\n",
    "Let's pass a tensor for `x` instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss must be reduced to a scalar\n"
     ]
    }
   ],
   "source": [
    "x = math.wrap([0, 1, 2], math.channel('values'))\n",
    "try:\n",
    "    dx_function(x, y=1.)\n",
    "except Exception as exc:\n",
    "    print(exc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This failed because `gradient()` requires our function to return a scalar or batched scalar, but we returned three values along a spatial axis.\n",
    "This restriction applies to all dimension types except for batch dimensions which are automatically summed over."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(0.000, 2.000, 4.000)\u001B[0m along \u001B[92mvalues·µá\u001B[0m"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = math.wrap([0, 1, 2], math.batch('values'))\n",
    "dx_function(x, y=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A simple way to reduce all non-batch dimensions is [`l2_loss()`](unifyml/math#unifyml.math.l2_loss) or simply [`sum`](unifyml/math#unifyml.math.sum).\n",
    "Both of these operations reduce all dimensions except for batch dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(0.000, 2.000, 16.000)\u001B[0m along \u001B[92mvalues·µá\u001B[0m"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_function(x, y):\n",
    "    return math.l2_loss(x ** 2 * y)\n",
    "\n",
    "dx_function = math.gradient(loss_function, wrt='x', get_output=False)\n",
    "dx_function(x, y=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can get the gradients w.r.t. multiple values by passing multiple strings or a comma-separated `str`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(2., device='cuda:0'), tensor(1., device='cuda:0')]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.gradient(loss_function, wrt='x,y', get_output=False)(1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also compute the gradient w.r.t. pytrees and dataclasses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vec' object has no attribute '__value_attrs__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-e94b4183409b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;34m'x1'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'x2'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mdx_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mVec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\PhD\\UnifyML\\unifyml\\math\\_functional.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    479\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraces\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    480\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraces\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_trace_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrt_natives\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 481\u001B[1;33m         \u001B[0mnative_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraces\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnatives\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    482\u001B[0m         \u001B[0moutput_key\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatch_output_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecorded_mappings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    483\u001B[0m         \u001B[0mjac_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutput_key\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshapes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnon_batch\u001B[0m  \u001B[1;31m# ToDo prepend this to all wrt shapes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\PhD\\UnifyML\\unifyml\\backend\\torch\\_torch_backend.py\u001B[0m in \u001B[0;36meval_grad\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    878\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0meval_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    879\u001B[0m             \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrt_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_graph_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 880\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    881\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstaticshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    882\u001B[0m                 \u001B[0mgrads\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrt_args\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# grad() cannot be called during jit trace\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\PhD\\UnifyML\\unifyml\\math\\_functional.py\u001B[0m in \u001B[0;36mf_native\u001B[1;34m(*natives)\u001B[0m\n\u001B[0;32m    447\u001B[0m             \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0massemble_tree\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0min_key\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0min_tensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    448\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mfunctional_derivative_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 449\u001B[1;33m                 \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# Tensor or tuple/list of Tensors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    450\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    451\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-6-e9064e4ed4ba>\u001B[0m in \u001B[0;36mloss_function\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ml2_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m \u001B[1;33m**\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdx_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwrt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'x'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mget_output\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdx_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\PhD\\UnifyML\\unifyml\\math\\_nd.py\u001B[0m in \u001B[0;36ml2_loss\u001B[1;34m(x, reduce)\u001B[0m\n\u001B[0;32m    256\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m \u001B[1;33m**\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mPhiTreeNode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 258\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ml2_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mvariable_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    259\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    260\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\PhD\\UnifyML\\unifyml\\math\\_magic_ops.py\u001B[0m in \u001B[0;36mvariable_values\u001B[1;34m(obj)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mvariable_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m...\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    632\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'__variable_attrs__'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 633\u001B[1;33m         \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__value_attrs__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    634\u001B[0m         \u001B[0mvariables\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__variable_attrs__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    635\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mvalues\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0ma\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mvariables\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Vec' object has no attribute '__value_attrs__'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Vec:\n",
    "    x1: math.Tensor\n",
    "    x2: math.Tensor\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Vec(self.x1 * other, self.x2 * other)\n",
    "\n",
    "    def __pow__(self, power, modulo=None):\n",
    "        return Vec(self.x1 ** power, self.x2 ** power)\n",
    "\n",
    "    def __value_attrs__(self):\n",
    "        return 'x1', 'x2'\n",
    "\n",
    "dx_function(x=Vec(1, 2), y=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we create the custom class `Vec` which holds two properties, `x1` and `x2`.\n",
    "In [`__value_attrs__`](unifyml/math/magic#unifyml.math.magic.PhiTreeNode.__value_attrs__), we declare that both members should be considered as values for value operations, such as `l2_loss`.\n",
    "The analog method [`__variable_attrs__`](unifyml/math/magic#unifyml.math.magic.PhiTreeNode.__variable_attrs__) defines which values should be considered for automatic differentiation.\n",
    "This defaults to all variables if not implemented."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Reading\n",
    "\n",
    "UnifyML also provides [finite difference differential operators](N_Dimensional.html).\n",
    "\n",
    "Another important function transformation is [JIT-compilation](JIT.html).\n",
    "\n",
    "When [training neural networks](Networks.html), the gradient is typically computed under-the-hood.\n",
    "\n",
    "[üåê **UnifyML**](https://github.com/holl-/UnifyML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://holl-.github.io/UnifyML/unifyml/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://holl-.github.io/UnifyML/unifyml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/holl-/UnifyML/blob/main/docs/Examples.ipynb) [**Examples**](https://holl-.github.io/UnifyML/Examples.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}