{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Œ¶<sub>ML</sub> Performance\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Convert.ipynb)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install phiml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance and JIT-compilation\n",
    "\n",
    "Œ¶<sub>ML</sub> provides many conveniences to make your code more concise and expressive.\n",
    "It also includes various performance optimization checks.\n",
    "All of this comes at the cost of added overhead for dimension matching, under-the-hood reshaping, etc.\n",
    "However, this overhead gets eliminated once your code is [JIT-compiled](JIT.html) with PyTorch, TensorFlow or Jax.\n",
    "\n",
    "In this notebook, we measure the differences on a rigid body simulation inspired by Billiards.\n",
    "Spheres are moving on a table and can collide with the boundary as well as with other spheres.\n",
    "The below function defines the physics step using the Œ¶-ML API."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ¶-ML + torch JIT compilation: \u001B[93mfloat64\u001B[0m \u001B[94m0.122501\u001B[0m\n",
      "Œ¶-ML + torch execution average: 0.00500738387927413 +- 0.0007334057590924203\n",
      "Œ¶-ML + jax JIT compilation: \u001B[93mfloat64\u001B[0m \u001B[94m0.16479\u001B[0m\n",
      "Œ¶-ML + jax execution average: 0.016138767823576927 +- 0.0008516904781572521\n",
      "Œ¶-ML + tensorflow JIT compilation: \u001B[93mfloat64\u001B[0m \u001B[94m0.9380557\u001B[0m\n",
      "Œ¶-ML + tensorflow execution average: 0.024349015206098557 +- 0.0010612220503389835\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from phiml.math import math, rename_dims, instance, dual, iterate, batch, channel\n",
    "\n",
    "\n",
    "@math.jit_compile\n",
    "def physics_step(x, v, dt: float, elasticity=0.8, radius=.03):\n",
    "    x_next = x + v * dt\n",
    "    deltas = -math.pairwise_distances(x_next)\n",
    "    dist = math.vec_length(deltas, eps=1e-4)  # eps to avoid NaN during backprop of sqrt\n",
    "    rel_v = -math.pairwise_distances(v)\n",
    "    dist_dir = math.safe_div(deltas, dist)\n",
    "    projected_v = dist_dir.vector * rel_v.vector\n",
    "    has_impact = (projected_v < 0) & (dist < 2 * radius)\n",
    "    impulse = -(1 + elasticity) * .5 * projected_v * dist_dir\n",
    "    radius_sum = radius + rename_dims(radius, instance, dual)\n",
    "    impact_time = math.safe_div(dist - radius_sum, projected_v)\n",
    "    x_inc_contrib = math.sum(math.where(has_impact, math.minimum(impact_time - dt, 0) * impulse, 0), dual)\n",
    "    x += x_inc_contrib\n",
    "    v += math.sum(math.where(has_impact, impulse, 0), dual)\n",
    "    v = math.where((x < 0) | (x > 2), -v, v)\n",
    "    return x + v * dt, v\n",
    "\n",
    "\n",
    "for backend in ['torch', 'jax', 'tensorflow']:\n",
    "    math.use(backend)\n",
    "    x0 = math.random_uniform(instance(points=1000), channel(vector='x,y'), high=2)\n",
    "    v0 = math.random_normal(x0.shape)\n",
    "    (x_trj, v_trj), dt = iterate(physics_step, batch(t=200), x0, v0, f_kwargs={'dt': .05}, measure=time.perf_counter)\n",
    "    print(f\"Œ¶-ML + {backend} JIT compilation: {dt.t[0]}\")\n",
    "    print(f\"Œ¶-ML + {backend} execution average: {dt.t[2:].mean} +- {dt.t[2:].std}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The performance numbers printed above were measured on GitHub Actions and may fluctuate a lot, depending on the allotted CPU processing power.\n",
    "We provide reference GPU numbers below.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Native Implementations\n",
    "\n",
    "Next, let's implement the same simulation natively, without using PhiML to compare the performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax JIT compilation: 0.1478613000000002\n",
      "jax execution average: 0.011604835279285908\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "\n",
    "\n",
    "def safe_div(x, y):\n",
    "    return np.where(y == 0, 0, x / y)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def physics_step(x, v, dt: float, elasticity=0.8, radius=.03):\n",
    "    x_next = x + v * dt\n",
    "    deltas = x_next[..., None, :] - x_next[..., None, :, :]\n",
    "    dist = np.sqrt(np.maximum(np.sum(deltas ** 2, -1), 1e-4))  # eps=1e-4 to avoid NaN during backprop of sqrt\n",
    "    rel_v = v[..., None, :] - v[..., None, :, :]\n",
    "    dist_dir = safe_div(deltas, dist[..., None])\n",
    "    projected_v = np.sum(dist_dir * rel_v, -1)\n",
    "    has_impact = (projected_v < 0) & (dist < 2 * radius)\n",
    "    impulse = -(1 + elasticity) * .5 * projected_v[..., None] * dist_dir\n",
    "    radius_sum = radius + radius  # this only supports equal radii\n",
    "    impact_time = safe_div(dist - radius_sum, projected_v)\n",
    "    x_inc_contrib = np.sum(np.where(has_impact[..., None], np.minimum(impact_time[..., None] - dt, 0) * impulse, 0), -2)\n",
    "    x += x_inc_contrib\n",
    "    v += np.sum(np.where(has_impact[..., None], impulse, 0), -2)\n",
    "    v = np.where((x < 0) | (x > 2), -v, v)\n",
    "    return x + v * dt, v\n",
    "\n",
    "\n",
    "x0 = jax.random.uniform(jax.random.PRNGKey(0), shape=(1000, 2)) * 2\n",
    "v0 = jax.random.normal(jax.random.PRNGKey(1), shape=x0.shape)\n",
    "x_trj = [x0]\n",
    "v_trj = [v0]\n",
    "dt_jax = []\n",
    "t0 = time.perf_counter()\n",
    "for i in range(200):\n",
    "    x, v = physics_step(x_trj[-1], v_trj[-1], dt=.05)\n",
    "    x_trj.append(x)\n",
    "    v_trj.append(v)\n",
    "    t = time.perf_counter()\n",
    "    dt_jax.append(t - t0)\n",
    "    t0 = t\n",
    "x_trj = np.stack(x_trj)\n",
    "v_trj = np.stack(v_trj)\n",
    "print(f\"jax JIT compilation: {dt_jax[0]}\")\n",
    "print(f\"jax execution average: {np.mean(np.asarray(dt_jax[2:]))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-501d4811bbb3>:12: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  dist = torch.sqrt(torch.maximum(torch.sum(deltas ** 2, -1), torch.tensor(1e-4)))  # eps=1e-4 to avoid NaN during backprop of sqrt\n",
      "<ipython-input-3-501d4811bbb3>:20: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_inc_contrib = torch.sum(torch.where(has_impact.unsqueeze(-1), torch.minimum(impact_time.unsqueeze(-1) - dt, torch.tensor(0.0)) * impulse, torch.tensor(0.0)), -2)\n",
      "<ipython-input-3-501d4811bbb3>:22: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  v += torch.sum(torch.where(has_impact.unsqueeze(-1), impulse, torch.tensor(0.0)), -2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch JIT compilation: 0.06732069700956345\n",
      "torch execution average: 0.0452803298830986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-501d4811bbb3>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(f\"torch execution average: {torch.mean(torch.tensor(dt_torch[2:]))}\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def safe_div(x, y):\n",
    "    return torch.where(y == 0, torch.zeros_like(x), x / y)\n",
    "\n",
    "\n",
    "def physics_step_torch(x, v, dt: float = .05, elasticity=0.8, radius=0.03):\n",
    "    x_next = x + v * dt\n",
    "    deltas = x_next.unsqueeze(-2) - x_next.unsqueeze(-3)\n",
    "    dist = torch.sqrt(torch.maximum(torch.sum(deltas ** 2, -1), torch.tensor(1e-4)))  # eps=1e-4 to avoid NaN during backprop of sqrt\n",
    "    rel_v = v.unsqueeze(-2) - v.unsqueeze(-3)\n",
    "    dist_dir = safe_div(deltas, dist.unsqueeze(-1))\n",
    "    projected_v = torch.sum(dist_dir * rel_v, -1)\n",
    "    has_impact = (projected_v < 0) & (dist < 2 * radius)\n",
    "    impulse = -(1 + elasticity) * 0.5 * projected_v.unsqueeze(-1) * dist_dir\n",
    "    radius_sum = radius + radius  # this only supports equal radii\n",
    "    impact_time = safe_div(dist - radius_sum, projected_v)\n",
    "    x_inc_contrib = torch.sum(torch.where(has_impact.unsqueeze(-1), torch.minimum(impact_time.unsqueeze(-1) - dt, torch.tensor(0.0)) * impulse, torch.tensor(0.0)), -2)\n",
    "    x += x_inc_contrib\n",
    "    v += torch.sum(torch.where(has_impact.unsqueeze(-1), impulse, torch.tensor(0.0)), -2)\n",
    "    v = torch.where((x < 0) | (x > 2), -v, v)\n",
    "    return x + v * dt, v\n",
    "\n",
    "\n",
    "x0 = torch.rand((1000, 2)) * 2\n",
    "v0 = torch.randn_like(x0)\n",
    "physics_step_torch = torch.jit.trace(physics_step_torch, [x0, v0])\n",
    "x_trj = [x0]\n",
    "v_trj = [v0]\n",
    "dt_torch = []\n",
    "t0 = time.perf_counter()\n",
    "for i in range(200):\n",
    "    x, v = physics_step_torch(x_trj[-1], v_trj[-1])\n",
    "    x_trj.append(x)\n",
    "    v_trj.append(v)\n",
    "    t = time.perf_counter()\n",
    "    dt_torch.append(t - t0)\n",
    "    t0 = t\n",
    "x_trj = torch.stack(x_trj)\n",
    "v_trj = torch.stack(v_trj)\n",
    "dt_torch = torch.tensor(dt_torch)\n",
    "print(f\"torch JIT compilation: {dt_torch[0]}\")\n",
    "print(f\"torch execution average: {torch.mean(torch.tensor(dt_torch[2:]))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow JIT compilation: 0.18679189682006836\n",
      "tensorflow execution average: 0.023667974397540092\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def physics_step(x, v, dt: float, elasticity=0.8, radius=0.03):\n",
    "    x_next = x + v * dt\n",
    "    deltas = tf.expand_dims(x_next, -2) - tf.expand_dims(x_next, -3)\n",
    "    dist = tf.sqrt(tf.maximum(tf.reduce_sum(deltas ** 2, -1), 1e-4))  # eps=1e-4 to avoid NaN during backprop of sqrt\n",
    "    rel_v = tf.expand_dims(v, -2) - tf.expand_dims(v, -3)\n",
    "    dist_dir = tf.math.divide_no_nan(deltas, tf.expand_dims(dist, -1))\n",
    "    projected_v = tf.reduce_sum(dist_dir * rel_v, -1)\n",
    "    has_impact = tf.logical_and(projected_v < 0, dist < 2 * radius)\n",
    "    impulse = -(1 + elasticity) * 0.5 * tf.expand_dims(projected_v, -1) * dist_dir\n",
    "    radius_sum = radius + radius  # this only supports equal radii\n",
    "    impact_time = tf.math.divide_no_nan(dist - radius_sum, projected_v)\n",
    "    x_inc_contrib = tf.reduce_sum(tf.where(tf.expand_dims(has_impact, -1), tf.minimum(tf.expand_dims(impact_time, -1) - dt, 0) * impulse, 0), -2)\n",
    "    x += x_inc_contrib\n",
    "    v += tf.reduce_sum(tf.where(tf.expand_dims(has_impact, -1), impulse, 0), -2)\n",
    "    v = tf.where(tf.logical_or(x < 0, x > 2), -v, v)\n",
    "    return x + v * dt, v\n",
    "\n",
    "x0 = tf.random.uniform((1000, 2)) * 2\n",
    "v0 = tf.random.normal(shape=x0.shape)\n",
    "x_trj = [x0]\n",
    "v_trj = [v0]\n",
    "dt_tf = []\n",
    "t0 = time.perf_counter()\n",
    "for i in range(200):\n",
    "    x, v = physics_step(x_trj[-1], v_trj[-1], dt=.05)\n",
    "    x_trj.append(x)\n",
    "    v_trj.append(v)\n",
    "    t = time.perf_counter()\n",
    "    dt_tf.append(t - t0)\n",
    "    t0 = t\n",
    "x_trj = tf.stack(x_trj)\n",
    "v_trj = tf.stack(v_trj)\n",
    "dt_tf = tf.constant(dt_tf)\n",
    "print(f\"tensorflow JIT compilation: {dt_tf[0]}\")\n",
    "print(f\"tensorflow execution average: {tf.reduce_mean(dt_tf)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Let's compare the JIT compilation performance first. The following numbers were captured on a 12-core AMD processor.\n",
    "\n",
    "| Library    | Native | Œ¶-ML |\n",
    "|------------|--------|------|\n",
    "| PyTorch    | 59     | 123  |\n",
    "| Jax        | 152    | 165  |\n",
    "| TensorFlow | 187    | 938  |\n",
    "\n",
    "Evidently, Œ¶-ML adds a small overhead to the JIT compilation in PyTorch and Jax, due to the additional shape handling operations.\n",
    "The overhead with TensorFlow is much larger. This is due to the way `@tf.function` is implemented. It does not simply trace the function with placeholder tensors but utilizes a custom Python code interpreter. While this allows it to capture control flow (`if/else/for`) more accurately, this results in a large part of the Œ¶-ML codebase being executed at a much reduced speed.\n",
    "Importantly, this compilation usually needs to performed only once. All later function invocations can call the already-compiled code.\n",
    "\n",
    "For the execution performance, we measured the following numbers on an NVIDIA RTX 2070 SUPER (ms/step).\n",
    "\n",
    "| Library    | Native | Œ¶-ML |\n",
    "|------------|--------|------|\n",
    "| PyTorch    | 45.2   | 5.0  |\n",
    "| Jax        | 19.5   | 16.1 |\n",
    "| TensorFlow | 23.6   | 24.3 |\n",
    "\n",
    "Here, Œ¶-ML beats the performance of our native PyTorch implementation and is on-par with TensorFlow and Jax.\n",
    "As discussed above, all extra code of Œ¶-ML is completely optimized out during JIT compilation, resulting in similar compiled code.\n",
    "\n",
    "The fact that Œ¶-ML is faster than PyTorch may be down to some inefficiency in the native PyTorch implementation above.\n",
    "This was largely generated by ChatGPT based on our function, but we had to make some changes in order for the function to output the correct result.\n",
    "If you find a problem with the code, please let us know!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Reading\n",
    "\n",
    "As discussed, optimal performance requires the use of [just-in-time compilation](JIT.html).\n",
    "\n",
    "Œ¶<sub>ML</sub> is compatible with PyTorch, TensorFlow, and Jax. Which backend and [device](Devices.html) you use has a major impact on performance.\n",
    "\n",
    "[üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}