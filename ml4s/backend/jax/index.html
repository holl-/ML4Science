<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ml4s.backend.jax API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ml4s.backend.jax</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ._jax_backend import JaxBackend
&#34;&#34;&#34;Backend for Jax operations.&#34;&#34;&#34;

JAX = JaxBackend()

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ml4s.backend.jax.stax_nets" href="stax_nets.html">ml4s.backend.jax.stax_nets</a></code></dt>
<dd>
<div class="desc"><p>Stax implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks â€¦</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ml4s.backend.jax.JaxBackend"><code class="flex name class">
<span>class <span class="ident">JaxBackend</span></span>
</code></dt>
<dd>
<div class="desc"><p>Backends delegate low-level operations to a ML or numerics library or emulate them.
The methods of <code>Backend</code> form a comprehensive list of available operations.</p>
<p>To support a library, subclass <code>Backend</code> and register it by adding it to <code>BACKENDS</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Human-readable string</dd>
<dt><strong><code>default_device</code></strong></dt>
<dd><code>ComputeDevice</code> being used by default</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JaxBackend(Backend):

    def __init__(self):
        devices = []
        for device_type in [&#39;cpu&#39;, &#39;gpu&#39;, &#39;tpu&#39;]:
            try:
                for jax_dev in jax.devices(device_type):
                    devices.append(ComputeDevice(self, device_type.upper(), jax_dev.platform.upper(), -1, -1, f&#34;id={jax_dev.id}&#34;, jax_dev))
            except RuntimeError as err:
                pass  # this is just Jax not finding anything. jaxlib.xla_client._get_local_backends() could help but isn&#39;t currently available on GitHub actions
        Backend.__init__(self, &#39;jax&#39;, devices, devices[-1])
        try:
            self.rnd_key = jax.random.PRNGKey(seed=0)
        except RuntimeError as err:
            warnings.warn(f&#34;{err}&#34;, RuntimeWarning)
            self.rnd_key = None

    def prefers_channels_last(self) -&gt; bool:
        return True

    def requires_fixed_shapes_when_tracing(self) -&gt; bool:
        return True

    def nn_library(self):
        from . import stax_nets
        return stax_nets

    def _check_float64(self):
        if self.precision == 64:
            if not jax.config.read(&#39;jax_enable_x64&#39;):
                jax.config.update(&#39;jax_enable_x64&#39;, True)
            assert jax.config.read(&#39;jax_enable_x64&#39;), &#34;FP64 is disabled for Jax.&#34;

    def seed(self, seed: int):
        self.rnd_key = jax.random.PRNGKey(seed)

    def as_tensor(self, x, convert_external=True):
        self._check_float64()
        if self.is_tensor(x, only_native=convert_external):
            array = x
        else:
            array = jnp.array(x)
        # --- Enforce Precision ---
        if not isinstance(array, numbers.Number):
            if self.dtype(array).kind == float:
                array = self.to_float(array)
            elif self.dtype(array).kind == complex:
                array = self.to_complex(array)
        return array

    def is_module(self, obj):
        return False

    def is_tensor(self, x, only_native=False):
        if isinstance(x, jnp.ndarray) and not isinstance(x, np.ndarray):  # NumPy arrays inherit from Jax arrays
            return True
        if isinstance(x, jnp.bool_) and not isinstance(x, np.bool_):
            return True
        if self.is_sparse(x):
            return True
        # --- Above considered native ---
        if only_native:
            return False
        # --- Non-native types ---
        if isinstance(x, np.ndarray):
            return True
        if isinstance(x, np.bool_):
            return True
        if isinstance(x, (numbers.Number, bool)):
            return True
        if isinstance(x, (tuple, list)):
            return all([self.is_tensor(item, False) for item in x])
        return False

    def is_sparse(self, x) -&gt; bool:
        return isinstance(x, (COO, BCOO, CSR, CSC))

    def is_available(self, tensor):
        return not isinstance(tensor, Tracer)

    def numpy(self, tensor):
        if isinstance(tensor, COO):
            raise NotImplementedError
        elif isinstance(tensor, BCOO):
            indices = np.array(tensor.indices)
            values = np.array(tensor.data)
            indices = indices[..., 0], indices[..., 1]
            assert values.ndim == 1, f&#34;Cannot convert batched COO to NumPy&#34;
            from scipy.sparse import coo_matrix
            return coo_matrix((values, indices), shape=self.staticshape(tensor))
        elif isinstance(tensor, CSR):
            raise NotImplementedError
        elif isinstance(tensor, CSC):
            raise NotImplementedError
        else:
            return np.array(tensor)

    def to_dlpack(self, tensor):
        from jax import dlpack
        return dlpack.to_dlpack(tensor)

    def from_dlpack(self, capsule):
        from jax import dlpack
        return dlpack.from_dlpack(capsule)

    def copy(self, tensor, only_mutable=False):
        return jnp.array(tensor, copy=True)

    def get_device(self, tensor: TensorType) -&gt; ComputeDevice:
        return self.get_device_by_ref(tensor.device())

    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -&gt; TensorType:
        return jax.device_put(tensor, device.ref)

    sqrt = staticmethod(jnp.sqrt)
    exp = staticmethod(jnp.exp)
    softplus = staticmethod(jax.nn.softplus)
    sin = staticmethod(jnp.sin)
    arcsin = staticmethod(jnp.arcsin)
    cos = staticmethod(jnp.cos)
    arccos = staticmethod(jnp.arccos)
    tan = staticmethod(jnp.tan)
    arctan = staticmethod(np.arctan)
    arctan2 = staticmethod(np.arctan2)
    sinh = staticmethod(np.sinh)
    arcsinh = staticmethod(np.arcsinh)
    cosh = staticmethod(np.cosh)
    arccosh = staticmethod(np.arccosh)
    tanh = staticmethod(np.tanh)
    arctanh = staticmethod(np.arctanh)
    log = staticmethod(jnp.log)
    log2 = staticmethod(jnp.log2)
    log10 = staticmethod(jnp.log10)
    isfinite = staticmethod(jnp.isfinite)
    isnan = staticmethod(jnp.isnan)
    isinf = staticmethod(jnp.isinf)
    abs = staticmethod(jnp.abs)
    sign = staticmethod(jnp.sign)
    round = staticmethod(jnp.round)
    ceil = staticmethod(jnp.ceil)
    floor = staticmethod(jnp.floor)
    flip = staticmethod(jnp.flip)
    stop_gradient = staticmethod(jax.lax.stop_gradient)
    transpose = staticmethod(jnp.transpose)
    equal = staticmethod(jnp.equal)
    tile = staticmethod(jnp.tile)
    stack = staticmethod(jnp.stack)
    concat = staticmethod(jnp.concatenate)
    maximum = staticmethod(jnp.maximum)
    minimum = staticmethod(jnp.minimum)
    clip = staticmethod(jnp.clip)
    shape = staticmethod(jnp.shape)
    staticshape = staticmethod(jnp.shape)
    imag = staticmethod(jnp.imag)
    real = staticmethod(jnp.real)
    conj = staticmethod(jnp.conjugate)
    einsum = staticmethod(jnp.einsum)
    cumsum = staticmethod(jnp.cumsum)

    def nonzero(self, values, length=None, fill_value=-1):
        result = jnp.nonzero(values, size=length, fill_value=fill_value)
        return jnp.stack(result, -1)

    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
        batch_size = self.determine_size(args, 0)
        args = [self.tile_to(t, 0, batch_size) for t in args]
        def f_positional(*args):
            return f(*args, **aux_args)
        vec_f = jax.vmap(f_positional, 0, 0)
        return vec_f(*args)

    def numpy_call(self, f, output_shapes, output_dtypes, *args, **aux_args):
        @dataclasses.dataclass
        class OutputTensor:
            shape: Tuple[int]
            dtype: np.dtype
        output_specs = map_structure(lambda t, s: OutputTensor(s, to_numpy_dtype(t)), output_dtypes, output_shapes)
        if hasattr(jax, &#39;pure_callback&#39;):
            def aux_f(*args):
                return f(*args, **aux_args)
            return jax.pure_callback(aux_f, output_specs, *args)
        else:
            def aux_f(args):
                if isinstance(args, tuple):
                    return f(*args, **aux_args)
                else:
                    return f(args, **aux_args)
            from jax.experimental.host_callback import call
            return call(aux_f, args, result_shape=output_specs)

    def jit_compile(self, f: Callable) -&gt; Callable:
        def run_jit_f(*args):
            # print(jax.make_jaxpr(f)(*args))
            ML_LOGGER.debug(f&#34;JaxBackend: running jit-compiled &#39;{f.__name__}&#39; with shapes {[self.shape(arg) for arg in args]} and dtypes {[self.dtype(arg) for arg in args]}&#34;)
            return self.as_registered.call(jit_f, *args, name=f&#34;run jit-compiled &#39;{f.__name__}&#39;&#34;)

        run_jit_f.__name__ = f&#34;Jax-Jit({f.__name__})&#34;
        jit_f = jax.jit(f, device=self._default_device.ref)
        return run_jit_f

    def block_until_ready(self, values):
        if hasattr(values, &#39;block_until_ready&#39;):
            values.block_until_ready()
        if isinstance(values, (tuple, list)):
            for v in values:
                self.block_until_ready(v)

    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
        if get_output:
            jax_grad_f = jax.value_and_grad(f, argnums=wrt, has_aux=True)
            @wraps(f)
            def unwrap_outputs(*args):
                args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
                (_, output_tuple), grads = jax_grad_f(*args)
                return (*output_tuple, *grads)
            return unwrap_outputs
        else:
            @wraps(f)
            def nonaux_f(*args):
                loss, output = f(*args)
                return loss
            jax_grad = jax.grad(nonaux_f, argnums=wrt, has_aux=False)
            @wraps(f)
            def call_jax_grad(*args):
                args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
                return jax_grad(*args)
            return call_jax_grad

    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -&gt; Callable:
        jax_fun = jax.custom_vjp(f)  # custom vector-Jacobian product (reverse-mode differentiation)

        def forward(*x):
            y = f(*x)
            return y, (x, y)

        def backward(x_y, dy):
            x, y = x_y
            dx = gradient(x, y, dy)
            return tuple(dx)

        jax_fun.defvjp(forward, backward)
        return jax_fun

    def divide_no_nan(self, x, y):
        return jnp.where(y == 0, 0, x / y)
        # jnp.nan_to_num(x / y, copy=True, nan=0) covers up NaNs from before

    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
        self._check_float64()
        self.rnd_key, subkey = jax.random.split(self.rnd_key)

        dtype = dtype or self.float_type
        jdt = to_numpy_dtype(dtype)
        if dtype.kind == float:
            tensor = random.uniform(subkey, shape, minval=low, maxval=high, dtype=jdt)
        elif dtype.kind == complex:
            real = random.uniform(subkey, shape, minval=low.real, maxval=high.real, dtype=to_numpy_dtype(DType(float, dtype.precision)))
            imag = random.uniform(subkey, shape, minval=low.imag, maxval=high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision)))
            return real + 1j * imag
        elif dtype.kind == int:
            tensor = random.randint(subkey, shape, low, high, dtype=jdt)
            if tensor.dtype != jdt:
                warnings.warn(f&#34;Jax failed to sample random integers with dtype {dtype}, returned {tensor.dtype} instead.&#34;, RuntimeWarning)
        else:
            raise ValueError(dtype)
        return jax.device_put(tensor, self._default_device.ref)

    def random_normal(self, shape, dtype: DType):
        self._check_float64()
        self.rnd_key, subkey = jax.random.split(self.rnd_key)
        dtype = dtype or self.float_type
        return jax.device_put(random.normal(subkey, shape, dtype=to_numpy_dtype(dtype)), self._default_device.ref)

    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
        if limit is None:
            start, limit = 0, start
        return jnp.arange(start, limit, delta, to_numpy_dtype(dtype))

    def pad(self, value, pad_width, mode=&#39;constant&#39;, constant_values=0):
        assert mode in (&#39;constant&#39;, &#39;symmetric&#39;, &#39;periodic&#39;, &#39;reflect&#39;, &#39;boundary&#39;), mode
        if mode == &#39;constant&#39;:
            constant_values = jnp.array(constant_values, dtype=value.dtype)
            return jnp.pad(value, pad_width, &#39;constant&#39;, constant_values=constant_values)
        else:
            if mode in (&#39;periodic&#39;, &#39;boundary&#39;):
                mode = {&#39;periodic&#39;: &#39;wrap&#39;, &#39;boundary&#39;: &#39;edge&#39;}[mode]
            return jnp.pad(value, pad_width, mode)

    def reshape(self, value, shape):
        return jnp.reshape(value, shape)

    def sum(self, value, axis=None, keepdims=False):
        if isinstance(value, (tuple, list)):
            assert axis == 0
            return sum(value[1:], value[0])
        return jnp.sum(value, axis=axis, keepdims=keepdims)

    def prod(self, value, axis=None):
        if not isinstance(value, jnp.ndarray):
            value = jnp.array(value)
        if value.dtype == bool:
            return jnp.all(value, axis=axis)
        return jnp.prod(value, axis=axis)

    def where(self, condition, x=None, y=None):
        if x is None or y is None:
            return jnp.argwhere(condition)
        return jnp.where(condition, x, y)

    def zeros(self, shape, dtype: DType = None):
        self._check_float64()
        return jax.device_put(jnp.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)

    def zeros_like(self, tensor):
        return jax.device_put(jnp.zeros_like(tensor), self._default_device.ref)

    def ones(self, shape, dtype: DType = None):
        self._check_float64()
        return jax.device_put(jnp.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)

    def ones_like(self, tensor):
        return jax.device_put(jnp.ones_like(tensor), self._default_device.ref)

    def meshgrid(self, *coordinates):
        self._check_float64()
        coordinates = [self.as_tensor(c) for c in coordinates]
        return [jax.device_put(c, self._default_device.ref) for c in jnp.meshgrid(*coordinates, indexing=&#39;ij&#39;)]

    def linspace(self, start, stop, number):
        self._check_float64()
        return jax.device_put(jnp.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)

    def linspace_without_last(self, start, stop, number):
        self._check_float64()
        return jax.device_put(jnp.linspace(start, stop, number, endpoint=False, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)

    def mean(self, value, axis=None, keepdims=False):
        return jnp.mean(value, axis, keepdims=keepdims)

    def log_gamma(self, x):
        return jax.lax.lgamma(self.to_float(x))

    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
        return jnp.tensordot(a, b, (a_axes, b_axes))

    def mul(self, a, b):
        # if scipy.sparse.issparse(a):  # TODO sparse?
        #     return a.multiply(b)
        # elif scipy.sparse.issparse(b):
        #     return b.multiply(a)
        # else:
            return Backend.mul(self, a, b)

    def mul_matrix_batched_vector(self, A, b):
        from jax.experimental.sparse import BCOO
        if isinstance(A, BCOO):
            return(A @ b.T).T
        return jnp.stack([A.dot(b[i]) for i in range(b.shape[0])])

    def get_diagonal(self, matrices, offset=0):
        result = jnp.diagonal(matrices, offset=offset, axis1=1, axis2=2)
        return jnp.transpose(result, [0, 2, 1])

    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
        if all(self.is_available(t) for t in values):
            return self.stop_gradient_tree(Backend.while_loop(self, loop, values, max_iter))
        if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
            values = self.stop_gradient_tree(values)
            trj = [values] if 0 in max_iter else []
            for i in range(1, max(max_iter) + 1):
                values = loop(*values)
                if i in max_iter:
                    trj.append(values)  # values are not mutable so no need to copy
            return self.stop_gradient_tree(self.stack_leaves(trj))
        else:
            if max_iter is None:
                cond = lambda vals: jnp.any(vals[0])
                body = lambda vals: loop(*vals)
                return jax.lax.while_loop(cond, body, values)
            else:
                cond = lambda vals: jnp.any(vals[1][0]) &amp; (vals[0] &lt; max_iter)
                body = lambda vals: (vals[0] + 1, loop(*vals[1]))
                return jax.lax.while_loop(cond, body, (self.as_tensor(0), values))[1]

    def max(self, x, axis=None, keepdims=False):
        return jnp.max(x, axis, keepdims=keepdims)

    def min(self, x, axis=None, keepdims=False):
        return jnp.min(x, axis, keepdims=keepdims)

    def conv(self, value, kernel, zero_padding=True):
        assert kernel.shape[0] in (1, value.shape[0])
        assert value.shape[1] == kernel.shape[2], f&#34;value has {value.shape[1]} channels but kernel has {kernel.shape[2]}&#34;
        assert value.ndim + 1 == kernel.ndim
        # AutoDiff may require jax.lax.conv_general_dilated
        result = []
        for b in range(value.shape[0]):
            b_kernel = kernel[min(b, kernel.shape[0] - 1)]
            result_b = []
            for o in range(kernel.shape[1]):
                result_b.append(0)
                for i in range(value.shape[1]):
                    # result.at[b, o, ...].set(scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=&#39;same&#39; if zero_padding else &#39;valid&#39;))
                    result_b[-1] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=&#39;same&#39; if zero_padding else &#39;valid&#39;)
            result.append(jnp.stack(result_b, 0))
        return jnp.stack(result, 0)

    def expand_dims(self, a, axis=0, number=1):
        for _i in range(number):
            a = jnp.expand_dims(a, axis)
        return a

    def cast(self, x, dtype: DType):
        if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
            return x
        else:
            return jnp.array(x, to_numpy_dtype(dtype))

    def unravel_index(self, flat_index, shape):
        return jnp.stack(jnp.unravel_index(flat_index, shape), -1)

    def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = &#39;undefined&#39;):
        if not self.is_available(shape):
            return Backend.ravel_multi_index(self, multi_index, shape, mode)
        mode = mode if isinstance(mode, int) else {&#39;undefined&#39;: &#39;clip&#39;, &#39;periodic&#39;: &#39;wrap&#39;, &#39;clamp&#39;: &#39;clip&#39;}[mode]
        idx_first = jnp.transpose(multi_index, (self.ndims(multi_index)-1,) + tuple(range(self.ndims(multi_index)-1)))
        result = jnp.ravel_multi_index(idx_first, shape, mode=&#39;wrap&#39; if isinstance(mode, int) else mode)
        if isinstance(mode, int):
            outside = self.any((multi_index &lt; 0) | (multi_index &gt;= jnp.asarray(shape, dtype=multi_index.dtype)), -1)
            result = self.where(outside, mode, result)
        return result

    def gather(self, values, indices, axis: int):
        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
        return values[tuple(slices)]

    def batched_gather_nd(self, values, indices):
        values = self.as_tensor(values)
        indices = self.as_tensor(indices)
        assert indices.shape[-1] == self.ndims(values) - 2
        batch_size = combined_dim(values.shape[0], indices.shape[0])
        results = []
        for b in range(batch_size):
            b_values = values[min(b, values.shape[0] - 1)]
            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
            results.append(b_values[b_indices])
        return jnp.stack(results)

    def repeat(self, x, repeats, axis: int, new_length=None):
        return jnp.repeat(x, self.as_tensor(repeats), axis, total_repeat_length=new_length)

    def std(self, x, axis=None, keepdims=False):
        return jnp.std(x, axis, keepdims=keepdims)

    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
        if new_length is None:
            slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
            return x[tuple(slices)]
        else:
            indices = jnp.argwhere(mask, size=new_length, fill_value=-1)[..., 0]
            valid = indices &gt;= 0
            valid = valid[tuple([slice(None) if i == axis else None for i in range(len(x.shape))])]
            result = self.gather(x, jnp.maximum(0, indices), axis)
            return jnp.where(valid, result, fill_value)

    def any(self, boolean_tensor, axis=None, keepdims=False):
        if isinstance(boolean_tensor, (tuple, list)):
            boolean_tensor = jnp.stack(boolean_tensor)
        return jnp.any(boolean_tensor, axis=axis, keepdims=keepdims)

    def all(self, boolean_tensor, axis=None, keepdims=False):
        if isinstance(boolean_tensor, (tuple, list)):
            boolean_tensor = jnp.stack(boolean_tensor)
        return jnp.all(boolean_tensor, axis=axis, keepdims=keepdims)

    def scatter(self, base_grid, indices, values, mode: str):
        base_grid, values = self.auto_cast(base_grid, values)
        batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
        spatial_dims = tuple(range(base_grid.ndim - 2))
        dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(1,),  # channel dim of updates (batch dim removed)
                                                inserted_window_dims=spatial_dims,  # no idea what this does but spatial_dims seems to work
                                                scatter_dims_to_operand_dims=spatial_dims)  # spatial dims of base_grid (batch dim removed)
        scatter = jax.lax.scatter_add if mode == &#39;add&#39; else jax.lax.scatter
        result = []
        for b in range(batch_size):
            b_grid = base_grid[b, ...]
            b_indices = indices[min(b, indices.shape[0] - 1), ...]
            b_values = values[min(b, values.shape[0] - 1), ...]
            result.append(scatter(b_grid, b_indices, b_values, dnums))
        return jnp.stack(result)

    def histogram1d(self, values, weights, bin_edges):
        def unbatched_hist(values, weights, bin_edges):
            hist, _ = jnp.histogram(values, bin_edges, weights=weights)
            return hist
        return jax.vmap(unbatched_hist)(values, weights, bin_edges)

    def bincount(self, x, weights: Optional[TensorType], bins: int, x_sorted=False):
        if x_sorted:
            return jax.ops.segment_sum(weights or 1, x, bins, indices_are_sorted=True)
        else:
            return jnp.bincount(x, weights=weights, minlength=bins, length=bins)

    def quantile(self, x, quantiles):
        return jnp.quantile(x, quantiles, axis=-1)

    def argsort(self, x, axis=-1):
        return jnp.argsort(x, axis)

    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
        if self.ndims(sorted_sequence) == 1:
            return jnp.searchsorted(sorted_sequence, search_values, side=side).astype(to_numpy_dtype(dtype))
        else:
            return jax.vmap(partial(self.searchsorted, side=side, dtype=dtype))(sorted_sequence, search_values)

    def fft(self, x, axes: Union[tuple, list]):
        x = self.to_complex(x)
        if not axes:
            return x
        if len(axes) == 1:
            return jnp.fft.fft(x, axis=axes[0]).astype(x.dtype)
        elif len(axes) == 2:
            return jnp.fft.fft2(x, axes=axes).astype(x.dtype)
        else:
            return jnp.fft.fftn(x, axes=axes).astype(x.dtype)

    def ifft(self, k, axes: Union[tuple, list]):
        if not axes:
            return k
        if len(axes) == 1:
            return jnp.fft.ifft(k, axis=axes[0]).astype(k.dtype)
        elif len(axes) == 2:
            return jnp.fft.ifft2(k, axes=axes).astype(k.dtype)
        else:
            return jnp.fft.ifftn(k, axes=axes).astype(k.dtype)

    def dtype(self, array) -&gt; DType:
        if isinstance(array, int):
            return DType(int, 32)
        if isinstance(array, float):
            return DType(float, 64)
        if isinstance(array, complex):
            return DType(complex, 128)
        if not isinstance(array, jnp.ndarray):
            array = jnp.array(array)
        return from_numpy_dtype(array.dtype)

    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -&gt; Tuple[TensorType, TensorType, TensorType, TensorType]:
        solution, residuals, rank, singular_values = lstsq_batched(matrix, rhs)
        return solution, residuals, rank, singular_values

    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
        x = jax.lax.linalg.triangular_solve(matrix, rhs, lower=lower, unit_diagonal=unit_diagonal, left_side=True)
        return x

    def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
        return BCOO((values, indices), shape=shape)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>ml4s.backend._backend.Backend</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ml4s.backend.jax.JaxBackend.arccosh"><code class="name">var <span class="ident">arccosh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arcsinh"><code class="name">var <span class="ident">arcsinh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arctan"><code class="name">var <span class="ident">arctan</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arctan2"><code class="name">var <span class="ident">arctan2</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arctanh"><code class="name">var <span class="ident">arctanh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.cosh"><code class="name">var <span class="ident">cosh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sinh"><code class="name">var <span class="ident">sinh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.tanh"><code class="name">var <span class="ident">tanh</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ml4s.backend.jax.JaxBackend.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.absolute, module=&#39;numpy&#39;)
@partial(jit, inline=True)
def absolute(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#39;absolute&#39;, x)
  dt = dtypes.dtype(x)
  return lax.asarray(x) if dt == np.bool_ or dtypes.issubdtype(dt, np.unsignedinteger) else lax.abs(x)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arccos"><code class="name flex">
<span>def <span class="ident">arccos</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.arcsin"><code class="name flex">
<span>def <span class="ident">arcsin</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>a:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], a_min:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex,Â None]Â =Â None, a_max:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex,Â None]Â =Â None, out:Â NoneÂ =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.clip, skip_params=[&#39;out&#39;])
@jit
def clip(a: ArrayLike, a_min: Optional[ArrayLike] = None,
         a_max: Optional[ArrayLike] = None, out: None = None) -&gt; Array:
  util.check_arraylike(&#34;clip&#34;, a)
  if out is not None:
    raise NotImplementedError(&#34;The &#39;out&#39; argument to jnp.clip is not supported.&#34;)
  if a_min is None and a_max is None:
    raise ValueError(&#34;At most one of a_min and a_max may be None&#34;)
  if a_min is not None:
    a = ufuncs.maximum(a_min, a)
  if a_max is not None:
    a = ufuncs.minimum(a_max, a)
  return asarray(a)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>arrays:Â Union[numpy.ndarray,Â jax.Array,Â Sequence[Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex]]], axis:Â Optional[int]Â =Â 0, dtype:Â Union[Any,Â str,Â numpy.dtype,Â jax._src.SupportsDType,Â None]Â =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Join a sequence of arrays along an existing axis.</p>
<p>LAX-backend implementation of :func:<code>numpy.concatenate</code>.</p>
<p><em>Original docstring below.</em></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>axis</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The axis along which the arrays will be joined.
If axis is None,
arrays are flattened before use.
Default is 0.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>str</code> or <code>dtype</code></dt>
<dd>If provided, the destination array will have this dtype. Cannot be
provided together with <code>out</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>res</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The concatenated array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.concatenate)
def concatenate(arrays: Union[np.ndarray, Array, Sequence[ArrayLike]],
                axis: Optional[int] = 0, dtype: Optional[DTypeLike] = None) -&gt; Array:
  if isinstance(arrays, (np.ndarray, Array)):
    return _concatenate_array(arrays, axis, dtype=dtype)
  util.check_arraylike(&#34;concatenate&#34;, *arrays)
  if not len(arrays):
    raise ValueError(&#34;Need at least one array to concatenate.&#34;)
  if ndim(arrays[0]) == 0:
    raise ValueError(&#34;Zero-dimensional arrays cannot be concatenated.&#34;)
  if axis is None:
    return concatenate([ravel(a) for a in arrays], axis=0, dtype=dtype)
  axis = _canonicalize_axis(axis, ndim(arrays[0]))
  if dtype is None:
    arrays_out = util.promote_dtypes(*arrays)
  else:
    arrays_out = [asarray(arr, dtype=dtype) for arr in arrays]
  # lax.concatenate can be slow to compile for wide concatenations, so form a
  # tree of concatenations as a workaround especially for op-by-op mode.
  # (https://github.com/google/jax/issues/653).
  k = 16
  while len(arrays_out) &gt; 1:
    arrays_out = [lax.concatenate(arrays_out[i:i+k], axis)
                  for i in range(0, len(arrays_out), k)]
  return arrays_out[0]</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.conj"><code class="name flex">
<span>def <span class="ident">conj</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.conjugate, module=&#39;numpy&#39;)
@partial(jit, inline=True)
def conjugate(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;conjugate&#34;, x)
  return lax.conj(x) if np.iscomplexobj(x) else lax.asarray(x)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.cumsum"><code class="name flex">
<span>def <span class="ident">cumsum</span></span>(<span>a:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], axis:Â Union[None,Â int,Â Sequence[int]]Â =Â None, dtype:Â Union[Any,Â str,Â numpy.dtype,Â jax._src.SupportsDType]Â =Â None, out:Â NoneÂ =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Return the cumulative sum of the elements along a given axis.</p>
<p>LAX-backend implementation of :func:<code>numpy.cumsum</code>.</p>
<p><em>Original docstring below.</em></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>array_like</code></dt>
<dd>Input array.</dd>
<dt><strong><code>axis</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Axis along which the cumulative sum is computed. The default
(None) is to compute the cumsum over the flattened array.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>dtype</code>, optional</dt>
<dd>Type of the returned array and of the accumulator in which the
elements are summed.
If <code>dtype</code> is not specified, it defaults
to the dtype of <code>a</code>, unless <code>a</code> has an integer dtype with a
precision less than that of the default platform integer.
In
that case, the default platform integer is used.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>cumsum_along_axis : ndarray.
A new array holding the result is returned unless <code>out</code> is
specified, in which case a reference to <code>out</code> is returned. The
result has the same size as <code>a</code>, and the same shape as <code>a</code> if
<code>axis</code> is not None or <code>a</code> is a 1-d array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np_reduction, skip_params=[&#39;out&#39;])
def cumulative_reduction(a: ArrayLike, axis: Axis = None,
                         dtype: DTypeLike = None, out: None = None) -&gt; Array:
  return _cumulative_reduction(a, _ensure_optional_axes(axis), dtype, out)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.einsum"><code class="name flex">
<span>def <span class="ident">einsum</span></span>(<span>subscripts, /, *operands, out:Â NoneÂ =Â None, optimize:Â strÂ =Â 'optimal', precision:Â Union[None,Â str,Â jax._src.lax.lax.Precision,Â Tuple[str,Â str],Â Tuple[jax._src.lax.lax.Precision,Â jax._src.lax.lax.Precision]]Â =Â None, preferred_element_type:Â Union[Any,Â str,Â numpy.dtype,Â jax._src.SupportsDType,Â None]Â =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the Einstein summation convention on the operands.</p>
<p>LAX-backend implementation of :func:<code>numpy.einsum</code>.</p>
<p>In addition to the original NumPy arguments listed below, also supports
<code>precision</code> for extra control over matrix-multiplication precision
on supported devices. <code>precision</code> may be set to <code>None</code>, which means
default precision for the backend, a :class:<code>~jax.lax.Precision</code> enum value
(<code>Precision.DEFAULT</code>, <code>Precision.HIGH</code> or <code>Precision.HIGHEST</code>) or a tuple
of two :class:<code>~jax.lax.Precision</code> enums indicating separate precision for each argument.
A tuple <code>precision</code> does not necessarily map to multiple arguments of <code>einsum()</code>;
rather, the specified <code>precision</code> is forwarded to each <code>dot_general</code> call used in
the implementation.</p>
<p><em>Original docstring below.</em></p>
<p>Using the Einstein summation convention, many common multi-dimensional,
linear algebraic array operations can be represented in a simple fashion.
In <em>implicit</em> mode <code>einsum</code> computes these values.</p>
<p>In <em>explicit</em> mode, <code>einsum</code> provides further flexibility to compute
other array operations that might not be considered classical Einstein
summation operations, by disabling, or forcing summation over specified
subscript labels.</p>
<p>See the notes and examples for clarification.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subscripts</code></strong> :&ensp;<code>str</code></dt>
<dd>Specifies the subscripts for summation as comma separated list of
subscript labels. An implicit (classical Einstein summation)
calculation is performed unless the explicit indicator '-&gt;' is
included as well as subscript labels of the precise output form.</dd>
<dt><strong><code>operands</code></strong> :&ensp;<code>list</code> of <code>array_like</code></dt>
<dd>These are the arrays for the operation.</dd>
<dt><strong><code>optimize</code></strong> :&ensp;<code>{False, True, 'greedy', 'optimal'}</code>, optional</dt>
<dd>Controls if intermediate optimization should occur. No optimization
will occur if False and True will default to the 'greedy' algorithm.
Also accepts an explicit contraction list from the <code>np.einsum_path</code>
function. See <code>np.einsum_path</code> for more details. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The calculation based on the Einstein summation convention.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.einsum, lax_description=_EINSUM_DOC, skip_params=[&#39;out&#39;])
def einsum(
    subscripts, /,
    *operands,
    out: None = None,
    optimize: str = &#34;optimal&#34;,
    precision: PrecisionLike = None,
    preferred_element_type: Optional[DTypeLike] = None,
    _use_xeinsum: bool = False,
    _dot_general: Callable[..., Array] = lax.dot_general,
) -&gt; Array:
  operands = (subscripts, *operands)
  if out is not None:
    raise NotImplementedError(&#34;The &#39;out&#39; argument to jnp.einsum is not supported.&#34;)

  spec = operands[0] if isinstance(operands[0], str) else None

  if (_use_xeinsum or spec is not None and &#39;{&#39; in spec):
    return jax.named_call(lax.xeinsum, name=spec)(*operands)

  optimize = &#39;optimal&#39; if optimize is True else optimize
  # using einsum_call=True here is an internal api for opt_einsum

  # Allow handling of shape polymorphism
  non_constant_dim_types = {
      type(d) for op in operands if not isinstance(op, str)
      for d in np.shape(op) if not core.is_constant_dim(d)
  }
  if not non_constant_dim_types:
    contract_path = opt_einsum.contract_path
  else:
    ty = next(iter(non_constant_dim_types))
    contract_path = _poly_einsum_handlers.get(ty, _default_poly_einsum_handler)
  operands, contractions = contract_path(
        *operands, einsum_call=True, use_blas=True, optimize=optimize)

  contractions = tuple((a, frozenset(b), c) for a, b, c, *_ in contractions)

  _einsum_computation = jax.named_call(
      _einsum, name=spec) if spec is not None else _einsum
  return _einsum_computation(operands, contractions, precision,  # type: ignore[operator]
                             preferred_element_type, _dot_general)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.equal"><code class="name flex">
<span>def <span class="ident">equal</span></span>(<span>x1, x2, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x1, x2, /: lax_fn(*promote_args(numpy_fn.__name__, x1, x2))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>m:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], axis:Â Union[int,Â Tuple[int,Â ...],Â None]Â =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Reverse the order of elements in an array along the given axis.</p>
<p>LAX-backend implementation of :func:<code>numpy.flip</code>.</p>
<p>The JAX version of this function may in some cases return a copy rather than a
view of the input.</p>
<p><em>Original docstring below.</em></p>
<p>The shape of the array is preserved, but the elements are reordered.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.12.0</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>array_like</code></dt>
<dd>Input array.</dd>
<dt><strong><code>axis</code></strong> :&ensp;<code>None</code> or <code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>
<p>Axis or axes along which to flip over. The default,
axis=None, will flip over all of the axes of the input array.
If axis is negative it counts from the last to the first axis.</p>
<p>If axis is a tuple of ints, flipping is performed on all of the axes
specified in the tuple.</p>
<p>!!! versionchanged "Changed in version:&ensp;1.15.0"
None and tuples of axes are supported</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>array_like</code></dt>
<dd>A view of <code>m</code> with the entries of axis reversed.
Since a view is
returned, this operation is done in constant time.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.flip, lax_description=_ARRAY_VIEW_DOC)
def flip(m: ArrayLike, axis: Optional[Union[int, Tuple[int, ...]]] = None) -&gt; Array:
  util.check_arraylike(&#34;flip&#34;, m)
  return _flip(asarray(m), reductions._ensure_optional_axes(axis))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>val:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.imag)
@partial(jit, inline=True)
def imag(val: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;imag&#34;, val)
  return lax.imag(val) if np.iscomplexobj(val) else lax.full_like(val, 0)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.isfinite, module=&#39;numpy&#39;)
@jit
def isfinite(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;isfinite&#34;, x)
  dtype = dtypes.dtype(x)
  if dtypes.issubdtype(dtype, np.floating):
    return lax.is_finite(x)
  elif dtypes.issubdtype(dtype, np.complexfloating):
    return lax.bitwise_and(lax.is_finite(real(x)), lax.is_finite(imag(x)))
  else:
    return lax.full_like(x, True, dtype=np.bool_)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.isinf"><code class="name flex">
<span>def <span class="ident">isinf</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.isinf, module=&#39;numpy&#39;)
@jit
def isinf(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;isinf&#34;, x)
  dtype = dtypes.dtype(x)
  if dtypes.issubdtype(dtype, np.floating):
    return lax.eq(lax.abs(x), _constant_like(x, np.inf))
  elif dtypes.issubdtype(dtype, np.complexfloating):
    re = lax.real(x)
    im = lax.imag(x)
    return lax.bitwise_or(lax.eq(lax.abs(re), _constant_like(re, np.inf)),
                          lax.eq(lax.abs(im), _constant_like(im, np.inf)))
  else:
    return lax.full_like(x, False, dtype=np.bool_)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.isnan"><code class="name flex">
<span>def <span class="ident">isnan</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.isnan, module=&#39;numpy&#39;)
@jit
def isnan(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;isnan&#34;, x)
  return lax.ne(x, x)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.log10"><code class="name flex">
<span>def <span class="ident">log10</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.log10, module=&#39;numpy&#39;)
@partial(jit, inline=True)
def log10(x: ArrayLike, /) -&gt; Array:
  x, = promote_args_inexact(&#34;log10&#34;, x)
  return lax.div(lax.log(x), lax.log(_constant_like(x, 10)))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.log2"><code class="name flex">
<span>def <span class="ident">log2</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.log2, module=&#39;numpy&#39;)
@partial(jit, inline=True)
def log2(x: ArrayLike, /) -&gt; Array:
  x, = promote_args_inexact(&#34;log2&#34;, x)
  return lax.div(lax.log(x), lax.log(_constant_like(x, 2)))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x1, x2, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x1, x2, /: lax_fn(*promote_args(numpy_fn.__name__, x1, x2))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x1, x2, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x1, x2, /: lax_fn(*promote_args(numpy_fn.__name__, x1, x2))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>val:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.real)
@partial(jit, inline=True)
def real(val: ArrayLike, /) -&gt; Array:
  check_arraylike(&#34;real&#34;, val)
  return lax.real(val) if np.iscomplexobj(val) else lax.asarray(val)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>a:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], decimals:Â intÂ =Â 0, out:Â NoneÂ =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.around, skip_params=[&#39;out&#39;])
@partial(jit, static_argnames=(&#39;decimals&#39;,))
def round(a: ArrayLike, decimals: int = 0, out: None = None) -&gt; Array:
  util.check_arraylike(&#34;round&#34;, a)
  decimals = core.concrete_or_error(operator.index, decimals, &#34;&#39;decimals&#39; argument of jnp.round&#34;)
  if out is not None:
    raise NotImplementedError(&#34;The &#39;out&#39; argument to jnp.round is not supported.&#34;)
  dtype = _dtype(a)
  if issubdtype(dtype, integer):
    if decimals &lt; 0:
      raise NotImplementedError(
        &#34;integer np.round not implemented for decimals &lt; 0&#34;)
    return asarray(a)  # no-op on integer types

  def _round_float(x: ArrayLike) -&gt; Array:
    if decimals == 0:
      return lax.round(x, lax.RoundingMethod.TO_NEAREST_EVEN)

    # TODO(phawkins): the strategy of rescaling the value isn&#39;t necessarily a
    # good one since we may be left with an incorrectly rounded value at the
    # end due to precision problems. As a workaround for float16, convert to
    # float32,
    x = lax.convert_element_type(x, np.float32) if dtype == np.float16 else x
    factor = _lax_const(x, 10 ** decimals)
    out = lax.div(lax.round(lax.mul(x, factor),
                            lax.RoundingMethod.TO_NEAREST_EVEN), factor)
    return lax.convert_element_type(out, dtype) if dtype == np.float16 else out

  if issubdtype(dtype, complexfloating):
    return lax.complex(_round_float(lax.real(a)), _round_float(lax.imag(a)))
  else:
    return _round_float(a)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>a)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the shape of an array.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>array_like</code></dt>
<dd>Input array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple</code> of <code>ints</code></dt>
<dd>The elements of the shape tuple give the lengths of the
corresponding array dimensions.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>len</code></dt>
<dd><code>len(a)</code> is equivalent to <code>np.shape(a)[0]</code> for N-D arrays with <code>N&gt;=1</code>.</dd>
<dt><code>ndarray.shape</code></dt>
<dd>Equivalent array method.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.shape(np.eye(3))
(3, 3)
&gt;&gt;&gt; np.shape([[1, 3]])
(1, 2)
&gt;&gt;&gt; np.shape([0])
(1,)
&gt;&gt;&gt; np.shape(0)
()
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = np.array([(1, 2), (3, 4), (5, 6)],
...              dtype=[('x', 'i4'), ('y', 'i4')])
&gt;&gt;&gt; np.shape(a)
(3,)
&gt;&gt;&gt; a.shape
(3,)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@array_function_dispatch(_shape_dispatcher)
def shape(a):
    &#34;&#34;&#34;
    Return the shape of an array.

    Parameters
    ----------
    a : array_like
        Input array.

    Returns
    -------
    shape : tuple of ints
        The elements of the shape tuple give the lengths of the
        corresponding array dimensions.

    See Also
    --------
    len : ``len(a)`` is equivalent to ``np.shape(a)[0]`` for N-D arrays with
          ``N&gt;=1``.
    ndarray.shape : Equivalent array method.

    Examples
    --------
    &gt;&gt;&gt; np.shape(np.eye(3))
    (3, 3)
    &gt;&gt;&gt; np.shape([[1, 3]])
    (1, 2)
    &gt;&gt;&gt; np.shape([0])
    (1,)
    &gt;&gt;&gt; np.shape(0)
    ()

    &gt;&gt;&gt; a = np.array([(1, 2), (3, 4), (5, 6)],
    ...              dtype=[(&#39;x&#39;, &#39;i4&#39;), (&#39;y&#39;, &#39;i4&#39;)])
    &gt;&gt;&gt; np.shape(a)
    (3,)
    &gt;&gt;&gt; a.shape
    (3,)

    &#34;&#34;&#34;
    try:
        result = a.shape
    except AttributeError:
        result = asarray(a).shape
    return result</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], /) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_wraps(np.sign, module=&#39;numpy&#39;)
@jit
def sign(x: ArrayLike, /) -&gt; Array:
  check_arraylike(&#39;sign&#39;, x)
  dtype = dtypes.dtype(x)
  if dtypes.issubdtype(dtype, np.complexfloating):
    re = lax.real(x)
    return lax.complex(
      lax.sign(_where(re != 0, re, lax.imag(x))), _constant_like(re, 0))
  return lax.sign(x)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.softplus"><code class="name flex">
<span>def <span class="ident">softplus</span></span>(<span>x:Â Any) â€‘>Â Any</span>
</code></dt>
<dd>
<div class="desc"><p>Softplus activation function.</p>
<p>Computes the element-wise function</p>
<p>[ \mathrm{softplus}(x) = \log(1 + e^x) ]</p>
<h2 id="args">Args</h2>
<p>x : input array</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jax.jit
def softplus(x: Array) -&gt; Array:
  r&#34;&#34;&#34;Softplus activation function.

  Computes the element-wise function

  .. math::
    \mathrm{softplus}(x) = \log(1 + e^x)

  Args:
    x : input array
  &#34;&#34;&#34;
  return jnp.logaddexp(x, 0)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.stack"><code class="name flex">
<span>def <span class="ident">stack</span></span>(<span>arrays:Â Union[numpy.ndarray,Â jax.Array,Â Sequence[Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex]]], axis:Â intÂ =Â 0, out:Â NoneÂ =Â None, dtype:Â Union[Any,Â str,Â numpy.dtype,Â jax._src.SupportsDType,Â None]Â =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Join a sequence of arrays along a new axis.</p>
<p>LAX-backend implementation of :func:<code>numpy.stack</code>.</p>
<p><em>Original docstring below.</em></p>
<p>The <code>axis</code> parameter specifies the index of the new axis in the
dimensions of the result. For example, if <code>axis=0</code> it will be the first
dimension and if <code>axis=-1</code> it will be the last dimension.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.10.0</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>arrays</code></strong> :&ensp;<code>sequence</code> of <code>array_like</code></dt>
<dd>Each array must have the same shape.</dd>
<dt><strong><code>axis</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The axis in the result array along which the input arrays are stacked.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>str</code> or <code>dtype</code></dt>
<dd>If provided, the destination array will have this dtype. Cannot be
provided together with <code>out</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stacked</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The stacked array has one more dimension than the input arrays.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.stack, skip_params=[&#39;out&#39;])
def stack(arrays: Union[np.ndarray, Array, Sequence[ArrayLike]],
          axis: int = 0, out: None = None, dtype: Optional[DTypeLike] = None) -&gt; Array:
  if not len(arrays):
    raise ValueError(&#34;Need at least one array to stack.&#34;)
  if out is not None:
    raise NotImplementedError(&#34;The &#39;out&#39; argument to jnp.stack is not supported.&#34;)
  if isinstance(arrays, (np.ndarray, Array)):
    axis = _canonicalize_axis(axis, arrays.ndim)
    return concatenate(expand_dims(arrays, axis + 1), axis=axis, dtype=dtype)
  else:
    util.check_arraylike(&#34;stack&#34;, *arrays)
    shape0 = shape(arrays[0])
    axis = _canonicalize_axis(axis, len(shape0) + 1)
    new_arrays = []
    for a in arrays:
      if shape(a) != shape0:
        raise ValueError(&#34;All input arrays must have the same shape.&#34;)
      new_arrays.append(expand_dims(a, axis))
    return concatenate(new_arrays, axis=axis, dtype=dtype)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.staticshape"><code class="name flex">
<span>def <span class="ident">staticshape</span></span>(<span>a)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the shape of an array.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>array_like</code></dt>
<dd>Input array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple</code> of <code>ints</code></dt>
<dd>The elements of the shape tuple give the lengths of the
corresponding array dimensions.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>len</code></dt>
<dd><code>len(a)</code> is equivalent to <code>np.shape(a)[0]</code> for N-D arrays with <code>N&gt;=1</code>.</dd>
<dt><code>ndarray.shape</code></dt>
<dd>Equivalent array method.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.shape(np.eye(3))
(3, 3)
&gt;&gt;&gt; np.shape([[1, 3]])
(1, 2)
&gt;&gt;&gt; np.shape([0])
(1,)
&gt;&gt;&gt; np.shape(0)
()
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; a = np.array([(1, 2), (3, 4), (5, 6)],
...              dtype=[('x', 'i4'), ('y', 'i4')])
&gt;&gt;&gt; np.shape(a)
(3,)
&gt;&gt;&gt; a.shape
(3,)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@array_function_dispatch(_shape_dispatcher)
def shape(a):
    &#34;&#34;&#34;
    Return the shape of an array.

    Parameters
    ----------
    a : array_like
        Input array.

    Returns
    -------
    shape : tuple of ints
        The elements of the shape tuple give the lengths of the
        corresponding array dimensions.

    See Also
    --------
    len : ``len(a)`` is equivalent to ``np.shape(a)[0]`` for N-D arrays with
          ``N&gt;=1``.
    ndarray.shape : Equivalent array method.

    Examples
    --------
    &gt;&gt;&gt; np.shape(np.eye(3))
    (3, 3)
    &gt;&gt;&gt; np.shape([[1, 3]])
    (1, 2)
    &gt;&gt;&gt; np.shape([0])
    (1,)
    &gt;&gt;&gt; np.shape(0)
    ()

    &gt;&gt;&gt; a = np.array([(1, 2), (3, 4), (5, 6)],
    ...              dtype=[(&#39;x&#39;, &#39;i4&#39;), (&#39;y&#39;, &#39;i4&#39;)])
    &gt;&gt;&gt; np.shape(a)
    (3,)
    &gt;&gt;&gt; a.shape
    (3,)

    &#34;&#34;&#34;
    try:
        result = a.shape
    except AttributeError:
        result = asarray(a).shape
    return result</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>x:Â ~T) â€‘>Â ~T</span>
</code></dt>
<dd>
<div class="desc"><p>Stops gradient computation.</p>
<p>Operationally <code>stop_gradient</code> is the identity function, that is, it returns
argument <code>x</code> unchanged. However, <code>stop_gradient</code> prevents the flow of
gradients during forward or reverse-mode automatic differentiation. If there
are multiple nested gradient computations, <code>stop_gradient</code> stops gradients
for all of them.</p>
<p>For example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; jax.grad(lambda x: x**2)(3.)
Array(6., dtype=float32, weak_type=True)
&gt;&gt;&gt; jax.grad(lambda x: jax.lax.stop_gradient(x)**2)(3.)
Array(0., dtype=float32, weak_type=True)
&gt;&gt;&gt; jax.grad(jax.grad(lambda x: x**2))(3.)
Array(2., dtype=float32, weak_type=True)
&gt;&gt;&gt; jax.grad(jax.grad(lambda x: jax.lax.stop_gradient(x)**2))(3.)
Array(0., dtype=float32, weak_type=True)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_gradient(x: T) -&gt; T:
  &#34;&#34;&#34;Stops gradient computation.

  Operationally ``stop_gradient`` is the identity function, that is, it returns
  argument `x` unchanged. However, ``stop_gradient`` prevents the flow of
  gradients during forward or reverse-mode automatic differentiation. If there
  are multiple nested gradient computations, ``stop_gradient`` stops gradients
  for all of them.

  For example:

  &gt;&gt;&gt; jax.grad(lambda x: x**2)(3.)
  Array(6., dtype=float32, weak_type=True)
  &gt;&gt;&gt; jax.grad(lambda x: jax.lax.stop_gradient(x)**2)(3.)
  Array(0., dtype=float32, weak_type=True)
  &gt;&gt;&gt; jax.grad(jax.grad(lambda x: x**2))(3.)
  Array(2., dtype=float32, weak_type=True)
  &gt;&gt;&gt; jax.grad(jax.grad(lambda x: jax.lax.stop_gradient(x)**2))(3.)
  Array(0., dtype=float32, weak_type=True)
  &#34;&#34;&#34;
  def stop(x):
    # only bind primitive on inexact dtypes, to avoid some staging
    if core.has_opaque_dtype(x):
      return x
    elif (dtypes.issubdtype(_dtype(x), np.floating) or
        dtypes.issubdtype(_dtype(x), np.complexfloating)):
      return ad_util.stop_gradient_p.bind(x)
    else:
      return x
  return tree_map(stop, x)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.tan"><code class="name flex">
<span>def <span class="ident">tan</span></span>(<span>x, /)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">fn = lambda x, /: lax_fn(*promote_args_inexact(numpy_fn.__name__, x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.tile"><code class="name flex">
<span>def <span class="ident">tile</span></span>(<span>A:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], reps:Â Union[int,Â Any,Â Sequence[Union[int,Â Any]]]) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Construct an array by repeating A the number of times given by reps.</p>
<p>LAX-backend implementation of :func:<code>numpy.tile</code>.</p>
<p><em>Original docstring below.</em></p>
<p>If <code>reps</code> has length <code>d</code>, the result will have dimension of
<code>max(d, A.ndim)</code>.</p>
<p>If <code>A.ndim &lt; d</code>, <code>A</code> is promoted to be d-dimensional by prepending new
axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication,
or shape (1, 1, 3) for 3-D replication. If this is not the desired
behavior, promote <code>A</code> to d-dimensions manually before calling this
function.</p>
<p>If <code>A.ndim &gt; d</code>, <code>reps</code> is promoted to <code>A</code>.ndim by pre-pending 1's to it.
Thus for an <code>A</code> of shape (2, 3, 4, 5), a <code>reps</code> of (2, 2) is treated as
(1, 1, 2, 2).</p>
<p>Note : Although tile may be used for broadcasting, it is strongly
recommended to use numpy's broadcasting operations and functions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The input array.</dd>
<dt><strong><code>reps</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The number of repetitions of <code>A</code> along each axis.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>c</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The tiled output array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.tile)
def tile(A: ArrayLike, reps: Union[DimSize, Sequence[DimSize]]) -&gt; Array:
  util.check_arraylike(&#34;tile&#34;, A)
  try:
    iter(reps)  # type: ignore[arg-type]
  except TypeError:
    reps_tup: Tuple[DimSize, ...] = (reps,)
  else:
    reps_tup = tuple(reps)  # type: ignore[assignment,arg-type]
  reps_tup = tuple(operator.index(rep) if core.is_constant_dim(rep) else rep
                   for rep in reps_tup)
  A_shape = (1,) * (len(reps_tup) - ndim(A)) + shape(A)
  reps_tup = (1,) * (len(A_shape) - len(reps_tup)) + reps_tup
  result = broadcast_to(reshape(A, [j for i in A_shape for j in [1, i]]),
                        [k for pair in zip(reps_tup, A_shape) for k in pair])
  return reshape(result, tuple(np.multiply(A_shape, reps_tup)))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>a:Â Union[jax.Array,Â numpy.ndarray,Â numpy.bool_,Â numpy.number,Â bool,Â int,Â float,Â complex], axes:Â Optional[Sequence[int]]Â =Â None) â€‘>Â jax.Array</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an array with axes transposed.</p>
<p>LAX-backend implementation of :func:<code>numpy.transpose</code>.</p>
<p>The JAX version of this function may in some cases return a copy rather than a
view of the input.</p>
<p><em>Original docstring below.</em></p>
<p>For a 1-D array, this returns an unchanged view of the original array, as a
transposed vector is simply the same vector.
To convert a 1-D array into a 2-D column vector, an additional dimension
must be added, e.g., <code>np.atleast2d(a).T</code> achieves this, as does
<code>a[:, np.newaxis]</code>.
For a 2-D array, this is the standard matrix transpose.
For an n-D array, if axes are given, their order indicates how the
axes are permuted (see Examples). If axes are not provided, then
<code>transpose(a).shape == a.shape[::-1]</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>array_like</code></dt>
<dd>Input array.</dd>
<dt><strong><code>axes</code></strong> :&ensp;<code>tuple</code> or <code>list</code> of <code>ints</code>, optional</dt>
<dd>If specified, it must be a tuple or list which contains a permutation
of [0,1,&hellip;,N-1] where N is the number of axes of <code>a</code>. The <code>i</code>'th axis
of the returned array will correspond to the axis numbered <code>axes[i]</code>
of the input. If not specified, defaults to <code>range(a.ndim)[::-1]</code>,
which reverses the order of the axes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code></dt>
<dd><code>a</code> with its axes permuted. A view is returned whenever possible.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@util._wraps(np.transpose, lax_description=_ARRAY_VIEW_DOC)
def transpose(a: ArrayLike, axes: Optional[Sequence[int]] = None) -&gt; Array:
  util.check_arraylike(&#34;transpose&#34;, a)
  axes_ = list(range(ndim(a))[::-1]) if axes is None else axes
  axes_ = [_canonicalize_axis(i, ndim(a)) for i in axes_]
  return lax.transpose(a, axes_)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ml4s.backend.jax.JaxBackend.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>self, boolean_tensor, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all(self, boolean_tensor, axis=None, keepdims=False):
    if isinstance(boolean_tensor, (tuple, list)):
        boolean_tensor = jnp.stack(boolean_tensor)
    return jnp.all(boolean_tensor, axis=axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.allocate_on_device"><code class="name flex">
<span>def <span class="ident">allocate_on_device</span></span>(<span>self, tensor:Â ~TensorType, device:Â ml4s.backend._backend.ComputeDevice) â€‘>Â ~TensorType</span>
</code></dt>
<dd>
<div class="desc"><p>Moves <code>tensor</code> to <code>device</code>. May copy the tensor if it is already on the device.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Existing tensor native to this backend.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Target device, associated with this backend.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -&gt; TensorType:
    return jax.device_put(tensor, device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>self, boolean_tensor, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any(self, boolean_tensor, axis=None, keepdims=False):
    if isinstance(boolean_tensor, (tuple, list)):
        boolean_tensor = jnp.stack(boolean_tensor)
    return jnp.any(boolean_tensor, axis=axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.argsort"><code class="name flex">
<span>def <span class="ident">argsort</span></span>(<span>self, x, axis=-1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def argsort(self, x, axis=-1):
    return jnp.argsort(x, axis)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.as_tensor"><code class="name flex">
<span>def <span class="ident">as_tensor</span></span>(<span>self, x, convert_external=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a tensor-like object to the native tensor representation of this backend.
If x is a native tensor of this backend, it is returned without modification.
If x is a Python number (numbers.Number instance), <code>convert_numbers</code> decides whether to convert it unless the backend cannot handle Python numbers.</p>
<p><em>Note:</em> There may be objects that are considered tensors by this backend but are not native and thus, will be converted by this method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor-like, e.g. list, tuple, Python number, tensor</dd>
<dt><strong><code>convert_external</code></strong></dt>
<dd>if False and <code>x</code> is a Python number that is understood by this backend, this method returns the number as-is. This can help prevent type clashes like int32 vs int64. (Default value = True)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor representation of <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_tensor(self, x, convert_external=True):
    self._check_float64()
    if self.is_tensor(x, only_native=convert_external):
        array = x
    else:
        array = jnp.array(x)
    # --- Enforce Precision ---
    if not isinstance(array, numbers.Number):
        if self.dtype(array).kind == float:
            array = self.to_float(array)
        elif self.dtype(array).kind == complex:
            array = self.to_complex(array)
    return array</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.batched_gather_nd"><code class="name flex">
<span>def <span class="ident">batched_gather_nd</span></span>(<span>self, values, indices)</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers values from the tensor <code>values</code> at locations <code>indices</code>.
The first dimension of <code>values</code> and <code>indices</code> is the batch dimension which must be either equal for both or one for either.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensor of shape (batch, spatial&hellip;, channel)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>int tensor of shape (batch, any&hellip;, multi_index) where the size of multi_index is values.rank - 2.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Gathered values as tensor of shape (batch, any&hellip;, channel)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batched_gather_nd(self, values, indices):
    values = self.as_tensor(values)
    indices = self.as_tensor(indices)
    assert indices.shape[-1] == self.ndims(values) - 2
    batch_size = combined_dim(values.shape[0], indices.shape[0])
    results = []
    for b in range(batch_size):
        b_values = values[min(b, values.shape[0] - 1)]
        b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
        results.append(b_values[b_indices])
    return jnp.stack(results)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.bincount"><code class="name flex">
<span>def <span class="ident">bincount</span></span>(<span>self, x, weights:Â Optional[~TensorType], bins:Â int, x_sorted=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bincount(self, x, weights: Optional[TensorType], bins: int, x_sorted=False):
    if x_sorted:
        return jax.ops.segment_sum(weights or 1, x, bins, indices_are_sorted=True)
    else:
        return jnp.bincount(x, weights=weights, minlength=bins, length=bins)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.block_until_ready"><code class="name flex">
<span>def <span class="ident">block_until_ready</span></span>(<span>self, values)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def block_until_ready(self, values):
    if hasattr(values, &#39;block_until_ready&#39;):
        values.block_until_ready()
    if isinstance(values, (tuple, list)):
        for v in values:
            self.block_until_ready(v)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>self, x, mask, axis=0, new_length=None, fill_value=0)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor with any number of dimensions</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>1D mask tensor</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis index &gt;= 0</dd>
<dt><strong><code>new_length</code></strong></dt>
<dd>Maximum size of the output along <code>axis</code>. This must be set when jit-compiling with Jax.</dd>
<dt><strong><code>fill_value</code></strong></dt>
<dd>If <code>new_length</code> is larger than the filtered result, the remaining values will be set to <code>fill_value</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
    if new_length is None:
        slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
        return x[tuple(slices)]
    else:
        indices = jnp.argwhere(mask, size=new_length, fill_value=-1)[..., 0]
        valid = indices &gt;= 0
        valid = valid[tuple([slice(None) if i == axis else None for i in range(len(x.shape))])]
        result = self.gather(x, jnp.maximum(0, indices), axis)
        return jnp.where(valid, result, fill_value)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>self, x, dtype:Â ml4s.backend._dtype.DType)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cast(self, x, dtype: DType):
    if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
        return x
    else:
        return jnp.array(x, to_numpy_dtype(dtype))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>self, value, kernel, zero_padding=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Convolve value with kernel.
Depending on the tensor rank, the convolution is either 1D (rank=3), 2D (rank=4) or 3D (rank=5).
Higher dimensions may not be supported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor of shape (batch_size, in_channel, spatial&hellip;)</dd>
<dt><strong><code>kernel</code></strong></dt>
<dd>tensor of shape (batch_size or 1, out_channel, in_channel, spatial&hellip;)</dd>
<dt><strong><code>zero_padding</code></strong></dt>
<dd>If True, pads the edges of <code>value</code> with zeros so that the result has the same shape as <code>value</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Convolution result as tensor of shape (batch_size, out_channel, spatial&hellip;)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f&#34;value has {value.shape[1]} channels but kernel has {kernel.shape[2]}&#34;
    assert value.ndim + 1 == kernel.ndim
    # AutoDiff may require jax.lax.conv_general_dilated
    result = []
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        result_b = []
        for o in range(kernel.shape[1]):
            result_b.append(0)
            for i in range(value.shape[1]):
                # result.at[b, o, ...].set(scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=&#39;same&#39; if zero_padding else &#39;valid&#39;))
                result_b[-1] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=&#39;same&#39; if zero_padding else &#39;valid&#39;)
        result.append(jnp.stack(result_b, 0))
    return jnp.stack(result, 0)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, tensor, only_mutable=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, tensor, only_mutable=False):
    return jnp.array(tensor, copy=True)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.custom_gradient"><code class="name flex">
<span>def <span class="ident">custom_gradient</span></span>(<span>self, f:Â Callable, gradient:Â Callable, get_external_cache:Â CallableÂ =Â None, on_call_skipped:Â CallableÂ =Â None) â€‘>Â Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function based on <code>f</code> that uses a custom gradient for backprop.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Forward function.</dd>
<dt><strong><code>gradient</code></strong></dt>
<dd>Function for backprop. Will be called as <code>gradient(*d_out)</code> to compute the gradient of <code>f</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>. However, the returned function does not support keyword arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -&gt; Callable:
    jax_fun = jax.custom_vjp(f)  # custom vector-Jacobian product (reverse-mode differentiation)

    def forward(*x):
        y = f(*x)
        return y, (x, y)

    def backward(x_y, dy):
        x, y = x_y
        dx = gradient(x, y, dy)
        return tuple(dx)

    jax_fun.defvjp(forward, backward)
    return jax_fun</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes x/y but returns 0 if y=0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def divide_no_nan(self, x, y):
    return jnp.where(y == 0, 0, x / y)
    # jnp.nan_to_num(x / y, copy=True, nan=0) covers up NaNs from before</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>self, array) â€‘>Â ml4s.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(self, array) -&gt; DType:
    if isinstance(array, int):
        return DType(int, 32)
    if isinstance(array, float):
        return DType(float, 64)
    if isinstance(array, complex):
        return DType(complex, 128)
    if not isinstance(array, jnp.ndarray):
        array = jnp.array(array)
    return from_numpy_dtype(array.dtype)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.expand_dims"><code class="name flex">
<span>def <span class="ident">expand_dims</span></span>(<span>self, a, axis=0, number=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_dims(self, a, axis=0, number=1):
    for _i in range(number):
        a = jnp.expand_dims(a, axis)
    return a</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>self, x, axes:Â Union[tuple,Â list])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the n-dimensional FFT along all but the first and last dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of dimension 3 or higher</dd>
<dt><strong><code>axes</code></strong></dt>
<dd>Along which axes to perform the FFT</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex tensor <code>k</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft(self, x, axes: Union[tuple, list]):
    x = self.to_complex(x)
    if not axes:
        return x
    if len(axes) == 1:
        return jnp.fft.fft(x, axis=axes[0]).astype(x.dtype)
    elif len(axes) == 2:
        return jnp.fft.fft2(x, axes=axes).astype(x.dtype)
    else:
        return jnp.fft.fftn(x, axes=axes).astype(x.dtype)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.from_dlpack"><code class="name flex">
<span>def <span class="ident">from_dlpack</span></span>(<span>self, capsule)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_dlpack(self, capsule):
    from jax import dlpack
    return dlpack.from_dlpack(capsule)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>self, values, indices, axis:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers values from the tensor <code>values</code> at locations <code>indices</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensor</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>1D tensor</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis along which to gather slices</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor, with size along <code>axis</code> being the length of <code>indices</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather(self, values, indices, axis: int):
    slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
    return values[tuple(slices)]</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.get_device"><code class="name flex">
<span>def <span class="ident">get_device</span></span>(<span>self, tensor:Â ~TensorType) â€‘>Â ml4s.backend._backend.ComputeDevice</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the device <code>tensor</code> is located on.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device(self, tensor: TensorType) -&gt; ComputeDevice:
    return self.get_device_by_ref(tensor.device())</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.get_diagonal"><code class="name flex">
<span>def <span class="ident">get_diagonal</span></span>(<span>self, matrices, offset=0)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrices</code></strong></dt>
<dd>(batch, rows, cols, channels)</dd>
<dt><strong><code>offset</code></strong></dt>
<dd>0=diagonal, positive=above diagonal, negative=below diagonal</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>diagonal</code></dt>
<dd>(batch, max(rows,cols), channels)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_diagonal(self, matrices, offset=0):
    result = jnp.diagonal(matrices, offset=offset, axis1=1, axis2=2)
    return jnp.transpose(result, [0, 2, 1])</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.histogram1d"><code class="name flex">
<span>def <span class="ident">histogram1d</span></span>(<span>self, values, weights, bin_edges)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>(batch, values)</dd>
<dt><strong><code>bin_edges</code></strong></dt>
<dd>(batch, edges)</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>(batch, values)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(batch, edges) with dtype matching weights</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def histogram1d(self, values, weights, bin_edges):
    def unbatched_hist(values, weights, bin_edges):
        hist, _ = jnp.histogram(values, bin_edges, weights=weights)
        return hist
    return jax.vmap(unbatched_hist)(values, weights, bin_edges)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>self, k, axes:Â Union[tuple,Â list])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the n-dimensional inverse FFT along all but the first and last dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>tensor of dimension 3 or higher</dd>
<dt><strong><code>axes</code></strong></dt>
<dd>Along which axes to perform the inverse FFT</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex tensor <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft(self, k, axes: Union[tuple, list]):
    if not axes:
        return k
    if len(axes) == 1:
        return jnp.fft.ifft(k, axis=axes[0]).astype(k.dtype)
    elif len(axes) == 2:
        return jnp.fft.ifft2(k, axes=axes).astype(k.dtype)
    else:
        return jnp.fft.ifftn(k, axes=axes).astype(k.dtype)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.is_available"><code class="name flex">
<span>def <span class="ident">is_available</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the value of the tensor is known and can be read at this point.
If true, <code>numpy(tensor)</code> must return a valid NumPy representation of the value.</p>
<p>Tensors are typically available when the backend operates in eager mode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>backend-compatible tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>bool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_available(self, tensor):
    return not isinstance(tensor, Tracer)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.is_module"><code class="name flex">
<span>def <span class="ident">is_module</span></span>(<span>self, obj)</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if <code>obj</code> is of a type that is specific to this backend, e.g. a neural network.
If <code>True</code>, this backend will be chosen for operations involving <code>obj</code>.</p>
<p>See Also:
<code>Backend.is_tensor()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>Object to test.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_module(self, obj):
    return False</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.is_sparse"><code class="name flex">
<span>def <span class="ident">is_sparse</span></span>(<span>self, x) â€‘>Â bool</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor native to this <code>Backend</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_sparse(self, x) -&gt; bool:
    return isinstance(x, (COO, BCOO, CSR, CSC))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.is_tensor"><code class="name flex">
<span>def <span class="ident">is_tensor</span></span>(<span>self, x, only_native=False)</span>
</code></dt>
<dd>
<div class="desc"><p>An object is considered a native tensor by a backend if no internal conversion is required by backend methods.
An object is considered a tensor (nativer or otherwise) by a backend if it is not a struct (e.g. tuple, list) and all methods of the backend accept it as a tensor argument.</p>
<p>If <code>True</code>, this backend will be chosen for operations involving <code>x</code>.</p>
<p>See Also:
<code>Backend.is_module()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>object to check</dd>
<dt><strong><code>only_native</code></strong></dt>
<dd>If True, only accepts true native tensor representations, not Python numbers or others that are also supported as tensors (Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>whether <code>x</code> is considered a tensor by this backend</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_tensor(self, x, only_native=False):
    if isinstance(x, jnp.ndarray) and not isinstance(x, np.ndarray):  # NumPy arrays inherit from Jax arrays
        return True
    if isinstance(x, jnp.bool_) and not isinstance(x, np.bool_):
        return True
    if self.is_sparse(x):
        return True
    # --- Above considered native ---
    if only_native:
        return False
    # --- Non-native types ---
    if isinstance(x, np.ndarray):
        return True
    if isinstance(x, np.bool_):
        return True
    if isinstance(x, (numbers.Number, bool)):
        return True
    if isinstance(x, (tuple, list)):
        return all([self.is_tensor(item, False) for item in x])
    return False</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.jacobian"><code class="name flex">
<span>def <span class="ident">jacobian</span></span>(<span>self, f, wrt:Â Union[tuple,Â list], get_output:Â bool, is_f_scalar:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to differentiate. Returns a tuple containing <code>(reduced_loss, output)</code></dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Argument indices for which to compute the gradient.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the derivative function should return the output of <code>f</code> in addition to the gradient.</dd>
<dt><strong><code>is_f_scalar</code></strong></dt>
<dd>Whether <code>f</code> is guaranteed to return a scalar output.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A function <code>g</code> with the same arguments as <code>f</code>.
If <code>get_output=True</code>, <code>g</code> returns a <code>tuple</code>containing the outputs of <code>f</code> followed by the gradients.
The gradients retain the dimensions of <code>reduced_loss</code> in order as outer (first) dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
    if get_output:
        jax_grad_f = jax.value_and_grad(f, argnums=wrt, has_aux=True)
        @wraps(f)
        def unwrap_outputs(*args):
            args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
            (_, output_tuple), grads = jax_grad_f(*args)
            return (*output_tuple, *grads)
        return unwrap_outputs
    else:
        @wraps(f)
        def nonaux_f(*args):
            loss, output = f(*args)
            return loss
        jax_grad = jax.grad(nonaux_f, argnums=wrt, has_aux=False)
        @wraps(f)
        def call_jax_grad(*args):
            args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
            return jax_grad(*args)
        return call_jax_grad</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.jit_compile"><code class="name flex">
<span>def <span class="ident">jit_compile</span></span>(<span>self, f:Â Callable) â€‘>Â Callable</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit_compile(self, f: Callable) -&gt; Callable:
    def run_jit_f(*args):
        # print(jax.make_jaxpr(f)(*args))
        ML_LOGGER.debug(f&#34;JaxBackend: running jit-compiled &#39;{f.__name__}&#39; with shapes {[self.shape(arg) for arg in args]} and dtypes {[self.dtype(arg) for arg in args]}&#34;)
        return self.as_registered.call(jit_f, *args, name=f&#34;run jit-compiled &#39;{f.__name__}&#39;&#34;)

    run_jit_f.__name__ = f&#34;Jax-Jit({f.__name__})&#34;
    jit_f = jax.jit(f, device=self._default_device.ref)
    return run_jit_f</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>self, start, stop, number)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace(self, start, stop, number):
    self._check_float64()
    return jax.device_put(jnp.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.linspace_without_last"><code class="name flex">
<span>def <span class="ident">linspace_without_last</span></span>(<span>self, start, stop, number)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace_without_last(self, start, stop, number):
    self._check_float64()
    return jax.device_put(jnp.linspace(start, stop, number, endpoint=False, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.log_gamma"><code class="name flex">
<span>def <span class="ident">log_gamma</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_gamma(self, x):
    return jax.lax.lgamma(self.to_float(x))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.matrix_solve_least_squares"><code class="name flex">
<span>def <span class="ident">matrix_solve_least_squares</span></span>(<span>self, matrix:Â ~TensorType, rhs:Â ~TensorType) â€‘>Â Tuple[~TensorType,Â ~TensorType,Â ~TensorType,Â ~TensorType]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Shape (batch, vec, constraints)</dd>
<dt><strong><code>rhs</code></strong></dt>
<dd>Shape (batch, vec, batch_per_matrix)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>solution</code></dt>
<dd>Solution vector of Shape (batch, constraints, batch_per_matrix)</dd>
<dt><code>residuals</code></dt>
<dd>Optional, can be <code>None</code></dd>
<dt><code>rank</code></dt>
<dd>Optional, can be <code>None</code></dd>
<dt><code>singular_values</code></dt>
<dd>Optional, can be <code>None</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -&gt; Tuple[TensorType, TensorType, TensorType, TensorType]:
    solution, residuals, rank, singular_values = lstsq_batched(matrix, rhs)
    return solution, residuals, rank, singular_values</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max(self, x, axis=None, keepdims=False):
    return jnp.max(x, axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, value, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self, value, axis=None, keepdims=False):
    return jnp.mean(value, axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self, *coordinates)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self, *coordinates):
    self._check_float64()
    coordinates = [self.as_tensor(c) for c in coordinates]
    return [jax.device_put(c, self._default_device.ref) for c in jnp.meshgrid(*coordinates, indexing=&#39;ij&#39;)]</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def min(self, x, axis=None, keepdims=False):
    return jnp.min(x, axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.mul"><code class="name flex">
<span>def <span class="ident">mul</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mul(self, a, b):
    # if scipy.sparse.issparse(a):  # TODO sparse?
    #     return a.multiply(b)
    # elif scipy.sparse.issparse(b):
    #     return b.multiply(a)
    # else:
        return Backend.mul(self, a, b)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.mul_matrix_batched_vector"><code class="name flex">
<span>def <span class="ident">mul_matrix_batched_vector</span></span>(<span>self, A, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mul_matrix_batched_vector(self, A, b):
    from jax.experimental.sparse import BCOO
    if isinstance(A, BCOO):
        return(A @ b.T).T
    return jnp.stack([A.dot(b[i]) for i in range(b.shape[0])])</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.nn_library"><code class="name flex">
<span>def <span class="ident">nn_library</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nn_library(self):
    from . import stax_nets
    return stax_nets</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>self, values, length=None, fill_value=-1)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor with only spatial dimensions</dd>
<dt><strong><code>length</code></strong></dt>
<dd>(Optional) Length of the resulting array. If specified, the result array will be padded with <code>fill_value</code> or trimmed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>non-zero multi-indices as tensor of shape (nnz/length, vector)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonzero(self, values, length=None, fill_value=-1):
    result = jnp.nonzero(values, size=length, fill_value=fill_value)
    return jnp.stack(result, -1)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a NumPy representation of the given tensor.
If <code>tensor</code> is already a NumPy array, it is returned without modification.</p>
<p>This method raises an error if the value of the tensor is not known at this point, e.g. because it represents a node in a graph.
Use <code>is_available(tensor)</code> to check if the value can be represented as a NumPy array.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>backend-compatible tensor or sparse tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation of the values stored in the tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self, tensor):
    if isinstance(tensor, COO):
        raise NotImplementedError
    elif isinstance(tensor, BCOO):
        indices = np.array(tensor.indices)
        values = np.array(tensor.data)
        indices = indices[..., 0], indices[..., 1]
        assert values.ndim == 1, f&#34;Cannot convert batched COO to NumPy&#34;
        from scipy.sparse import coo_matrix
        return coo_matrix((values, indices), shape=self.staticshape(tensor))
    elif isinstance(tensor, CSR):
        raise NotImplementedError
    elif isinstance(tensor, CSC):
        raise NotImplementedError
    else:
        return np.array(tensor)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.numpy_call"><code class="name flex">
<span>def <span class="ident">numpy_call</span></span>(<span>self, f, output_shapes, output_dtypes, *args, **aux_args)</span>
</code></dt>
<dd>
<div class="desc"><p>This call can be used in jit-compiled code but is not differentiable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function operating on numpy arrays.</dd>
<dt><strong><code>output_shapes</code></strong></dt>
<dd>Single shape <code>tuple</code> or tuple of shapes declaring the shapes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>output_dtypes</code></strong></dt>
<dd>Single <code>DType</code> or tuple of DTypes declaring the dtypes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Tensor arguments to be converted to NumPy arrays and then passed to <code>f</code>.</dd>
<dt><strong><code>**aux_args</code></strong></dt>
<dd>Keyword arguments to be passed to <code>f</code> without conversion.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Returned arrays of <code>f</code> converted to tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy_call(self, f, output_shapes, output_dtypes, *args, **aux_args):
    @dataclasses.dataclass
    class OutputTensor:
        shape: Tuple[int]
        dtype: np.dtype
    output_specs = map_structure(lambda t, s: OutputTensor(s, to_numpy_dtype(t)), output_dtypes, output_shapes)
    if hasattr(jax, &#39;pure_callback&#39;):
        def aux_f(*args):
            return f(*args, **aux_args)
        return jax.pure_callback(aux_f, output_specs, *args)
    else:
        def aux_f(args):
            if isinstance(args, tuple):
                return f(*args, **aux_args)
            else:
                return f(args, **aux_args)
        from jax.experimental.host_callback import call
        return call(aux_f, args, result_shape=output_specs)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>self, shape, dtype:Â ml4s.backend._dtype.DTypeÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(self, shape, dtype: DType = None):
    self._check_float64()
    return jax.device_put(jnp.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_like(self, tensor):
    return jax.device_put(jnp.ones_like(tensor), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, value, pad_width, mode='constant', constant_values=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Pad a tensor with values as specified by <code>mode</code> and <code>constant_values</code>.</p>
<p>If the mode is not supported, returns NotImplemented.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor</dd>
<dt><strong><code>pad_width</code></strong></dt>
<dd>2D tensor specifying the number of values padded to the edges of each axis in the form [[axis 0 lower, axis 0 upper], &hellip;] including batch and component axes.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>constant', 'boundary', 'periodic', 'symmetric', 'reflect'</dd>
<dt><strong><code>constant_values</code></strong></dt>
<dd>used for out-of-bounds points if mode='constant' (Default value = 0)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>str:
(Default value = 'constant')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>padded tensor or NotImplemented</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(self, value, pad_width, mode=&#39;constant&#39;, constant_values=0):
    assert mode in (&#39;constant&#39;, &#39;symmetric&#39;, &#39;periodic&#39;, &#39;reflect&#39;, &#39;boundary&#39;), mode
    if mode == &#39;constant&#39;:
        constant_values = jnp.array(constant_values, dtype=value.dtype)
        return jnp.pad(value, pad_width, &#39;constant&#39;, constant_values=constant_values)
    else:
        if mode in (&#39;periodic&#39;, &#39;boundary&#39;):
            mode = {&#39;periodic&#39;: &#39;wrap&#39;, &#39;boundary&#39;: &#39;edge&#39;}[mode]
        return jnp.pad(value, pad_width, mode)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.prefers_channels_last"><code class="name flex">
<span>def <span class="ident">prefers_channels_last</span></span>(<span>self) â€‘>Â bool</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefers_channels_last(self) -&gt; bool:
    return True</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>self, value, axis=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(self, value, axis=None):
    if not isinstance(value, jnp.ndarray):
        value = jnp.array(value)
    if value.dtype == bool:
        return jnp.all(value, axis=axis)
    return jnp.prod(value, axis=axis)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.quantile"><code class="name flex">
<span>def <span class="ident">quantile</span></span>(<span>self, x, quantiles)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces the last / inner axis of x.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor</dd>
<dt><strong><code>quantiles</code></strong></dt>
<dd>List or 1D tensor of quantiles to compute.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor with shape (quantiles, *x.shape[:-1])</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantile(self, x, quantiles):
    return jnp.quantile(x, quantiles, axis=-1)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>self, shape, dtype:Â ml4s.backend._dtype.DType)</span>
</code></dt>
<dd>
<div class="desc"><p>Float tensor of selected precision containing random values sampled from a normal distribution with mean 0 and std 1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal(self, shape, dtype: DType):
    self._check_float64()
    self.rnd_key, subkey = jax.random.split(self.rnd_key)
    dtype = dtype or self.float_type
    return jax.device_put(random.normal(subkey, shape, dtype=to_numpy_dtype(dtype)), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>self, shape, low, high, dtype:Â Optional[ml4s.backend._dtype.DType])</span>
</code></dt>
<dd>
<div class="desc"><p>Float tensor of selected precision containing random values in the range [0, 1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
    self._check_float64()
    self.rnd_key, subkey = jax.random.split(self.rnd_key)

    dtype = dtype or self.float_type
    jdt = to_numpy_dtype(dtype)
    if dtype.kind == float:
        tensor = random.uniform(subkey, shape, minval=low, maxval=high, dtype=jdt)
    elif dtype.kind == complex:
        real = random.uniform(subkey, shape, minval=low.real, maxval=high.real, dtype=to_numpy_dtype(DType(float, dtype.precision)))
        imag = random.uniform(subkey, shape, minval=low.imag, maxval=high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision)))
        return real + 1j * imag
    elif dtype.kind == int:
        tensor = random.randint(subkey, shape, low, high, dtype=jdt)
        if tensor.dtype != jdt:
            warnings.warn(f&#34;Jax failed to sample random integers with dtype {dtype}, returned {tensor.dtype} instead.&#34;, RuntimeWarning)
    else:
        raise ValueError(dtype)
    return jax.device_put(tensor, self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.range"><code class="name flex">
<span>def <span class="ident">range</span></span>(<span>self, start, limit=None, delta=1, dtype:Â ml4s.backend._dtype.DTypeÂ =Â int32)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
    if limit is None:
        start, limit = 0, start
    return jnp.arange(start, limit, delta, to_numpy_dtype(dtype))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.ravel_multi_index"><code class="name flex">
<span>def <span class="ident">ravel_multi_index</span></span>(<span>self, multi_index, shape, mode:Â Union[str,Â int]Â =Â 'undefined')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>multi_index</code></strong></dt>
<dd>(batch&hellip;, index_dim)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>1D tensor or tuple/list</dd>
<dt><strong><code>mode</code></strong></dt>
<dd><code>'undefined'</code>, <code>'periodic'</code>, <code>'clamp'</code> or an <code>int</code> to use for all invalid indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Integer tensor of shape (batch&hellip;)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = &#39;undefined&#39;):
    if not self.is_available(shape):
        return Backend.ravel_multi_index(self, multi_index, shape, mode)
    mode = mode if isinstance(mode, int) else {&#39;undefined&#39;: &#39;clip&#39;, &#39;periodic&#39;: &#39;wrap&#39;, &#39;clamp&#39;: &#39;clip&#39;}[mode]
    idx_first = jnp.transpose(multi_index, (self.ndims(multi_index)-1,) + tuple(range(self.ndims(multi_index)-1)))
    result = jnp.ravel_multi_index(idx_first, shape, mode=&#39;wrap&#39; if isinstance(mode, int) else mode)
    if isinstance(mode, int):
        outside = self.any((multi_index &lt; 0) | (multi_index &gt;= jnp.asarray(shape, dtype=multi_index.dtype)), -1)
        result = self.where(outside, mode, result)
    return result</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.repeat"><code class="name flex">
<span>def <span class="ident">repeat</span></span>(<span>self, x, repeats, axis:Â int, new_length=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeats the elements along <code>axis</code> <code>repeats</code> times.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor</dd>
<dt><strong><code>repeats</code></strong></dt>
<dd>How often to repeat each element. 1D tensor of length x.shape[axis]</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Which axis to repeat elements along</dd>
<dt><strong><code>new_length</code></strong></dt>
<dd>Set the length of <code>axis</code> after repeating. This is required for jit compilation with Jax.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>repeated Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def repeat(self, x, repeats, axis: int, new_length=None):
    return jnp.repeat(x, self.as_tensor(repeats), axis, total_repeat_length=new_length)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.requires_fixed_shapes_when_tracing"><code class="name flex">
<span>def <span class="ident">requires_fixed_shapes_when_tracing</span></span>(<span>self) â€‘>Â bool</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def requires_fixed_shapes_when_tracing(self) -&gt; bool:
    return True</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.reshape"><code class="name flex">
<span>def <span class="ident">reshape</span></span>(<span>self, value, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshape(self, value, shape):
    return jnp.reshape(value, shape)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>self, base_grid, indices, values, mode:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Depending on <code>mode</code>, performs scatter_update or scatter_add.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd>Tensor into which scatter values are inserted at indices. Tensor of shape (batch_size, spatial&hellip;, channels)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>Tensor of shape (batch_size or 1, update_count, index_vector)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Values to scatter at indices. Tensor of shape (batch_size or 1, update_count or 1, channels or 1)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>One of ('update', 'add')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of base_grid with values at <code>indices</code> updated by <code>values</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scatter(self, base_grid, indices, values, mode: str):
    base_grid, values = self.auto_cast(base_grid, values)
    batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
    spatial_dims = tuple(range(base_grid.ndim - 2))
    dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(1,),  # channel dim of updates (batch dim removed)
                                            inserted_window_dims=spatial_dims,  # no idea what this does but spatial_dims seems to work
                                            scatter_dims_to_operand_dims=spatial_dims)  # spatial dims of base_grid (batch dim removed)
    scatter = jax.lax.scatter_add if mode == &#39;add&#39; else jax.lax.scatter
    result = []
    for b in range(batch_size):
        b_grid = base_grid[b, ...]
        b_indices = indices[min(b, indices.shape[0] - 1), ...]
        b_values = values[min(b, values.shape[0] - 1), ...]
        result.append(scatter(b_grid, b_indices, b_values, dnums))
    return jnp.stack(result)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.searchsorted"><code class="name flex">
<span>def <span class="ident">searchsorted</span></span>(<span>self, sorted_sequence, search_values, side:Â str, dtype=int32)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
    if self.ndims(sorted_sequence) == 1:
        return jnp.searchsorted(sorted_sequence, search_values, side=side).astype(to_numpy_dtype(dtype))
    else:
        return jax.vmap(partial(self.searchsorted, side=side, dtype=dtype))(sorted_sequence, search_values)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>self, seed:Â int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seed(self, seed: int):
    self.rnd_key = jax.random.PRNGKey(seed)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.solve_triangular_dense"><code class="name flex">
<span>def <span class="ident">solve_triangular_dense</span></span>(<span>self, matrix, rhs, lower:Â bool, unit_diagonal:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>(batch_size, rows, cols)</dd>
<dt><strong><code>rhs</code></strong></dt>
<dd>(batch_size, cols)</dd>
</dl>
<p>lower:
unit_diagonal:</p>
<h2 id="returns">Returns</h2>
<p>(batch_size, cols)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
    matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
    x = jax.lax.linalg.triangular_solve(matrix, rhs, lower=lower, unit_diagonal=unit_diagonal, left_side=True)
    return x</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sparse_coo_tensor"><code class="name flex">
<span>def <span class="ident">sparse_coo_tensor</span></span>(<span>self, indices:Â Union[tuple,Â list], values, shape:Â tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a sparse matrix in coordinate list (COO) format.</p>
<p>Optional feature.</p>
<p>See Also:
<code>Backend.csr_matrix()</code>, <code>Backend.csc_matrix()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>2D tensor of shape <code>(nnz, dims)</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>1D values tensor matching <code>indices</code></dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the sparse matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native representation of the sparse matrix</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
    return BCOO((values, indices), shape=shape)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(self, x, axis=None, keepdims=False):
    return jnp.std(x, axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>self, value, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum(self, value, axis=None, keepdims=False):
    if isinstance(value, (tuple, list)):
        assert axis == 0
        return sum(value[1:], value[0])
    return jnp.sum(value, axis=axis, keepdims=keepdims)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.tensordot"><code class="name flex">
<span>def <span class="ident">tensordot</span></span>(<span>self, a, a_axes:Â Union[tuple,Â list], b, b_axes:Â Union[tuple,Â list])</span>
</code></dt>
<dd>
<div class="desc"><p>Multiply-sum-reduce a_axes of a with b_axes of b.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
    return jnp.tensordot(a, b, (a_axes, b_axes))</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.to_dlpack"><code class="name flex">
<span>def <span class="ident">to_dlpack</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dlpack(self, tensor):
    from jax import dlpack
    return dlpack.to_dlpack(tensor)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.unravel_index"><code class="name flex">
<span>def <span class="ident">unravel_index</span></span>(<span>self, flat_index, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unravel_index(self, flat_index, shape):
    return jnp.stack(jnp.unravel_index(flat_index, shape), -1)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.vectorized_call"><code class="name flex">
<span>def <span class="ident">vectorized_call</span></span>(<span>self, f, *args, output_dtypes=None, **aux_args)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function with only positional tensor argument, returning one or multiple tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Batched inputs for <code>f</code>. The first dimension of all <code>args</code> is vectorized.
All tensors in <code>args</code> must have the same size or <code>1</code> in their first dimension.</dd>
<dt><strong><code>output_dtypes</code></strong></dt>
<dd>Single <code>DType</code> or tuple of DTypes declaring the dtypes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>**aux_args</code></strong></dt>
<dd>Non-vectorized keyword arguments to be passed to <code>f</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
    batch_size = self.determine_size(args, 0)
    args = [self.tile_to(t, 0, batch_size) for t in args]
    def f_positional(*args):
        return f(*args, **aux_args)
    vec_f = jax.vmap(f_positional, 0, 0)
    return vec_f(*args)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>self, condition, x=None, y=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def where(self, condition, x=None, y=None):
    if x is None or y is None:
        return jnp.argwhere(condition)
    return jnp.where(condition, x, y)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.while_loop"><code class="name flex">
<span>def <span class="ident">while_loop</span></span>(<span>self, loop:Â Callable, values:Â tuple, max_iter:Â Union[int,Â Tuple[int,Â ...],Â List[int]])</span>
</code></dt>
<dd>
<div class="desc"><p>If <code>max_iter is None</code>, runs</p>
<pre><code class="language-python">while any(values[0]):
    values = loop(*values)
return values
</code></pre>
<p>This operation does not support backpropagation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>loop</code></strong></dt>
<dd>Loop function, must return a <code>tuple</code> with entries equal to <code>values</code> in shape and data type.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Initial values of loop variables.</dd>
<dt><strong><code>max_iter</code></strong></dt>
<dd>Maximum number of iterations to run, single <code>int</code> or sequence of integers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Loop variables upon loop completion if <code>max_iter</code> is a single integer.
If <code>max_iter</code> is a sequence, stacks the variables after each entry in <code>max_iter</code>, adding an outer dimension of size <code>&lt;= len(max_iter)</code>.
If the condition is fulfilled before the maximum max_iter is reached, the loop may be broken or not, depending on the implementation.
If the loop is broken, the values returned by the last loop are expected to be constant and filled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
    if all(self.is_available(t) for t in values):
        return self.stop_gradient_tree(Backend.while_loop(self, loop, values, max_iter))
    if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
        values = self.stop_gradient_tree(values)
        trj = [values] if 0 in max_iter else []
        for i in range(1, max(max_iter) + 1):
            values = loop(*values)
            if i in max_iter:
                trj.append(values)  # values are not mutable so no need to copy
        return self.stop_gradient_tree(self.stack_leaves(trj))
    else:
        if max_iter is None:
            cond = lambda vals: jnp.any(vals[0])
            body = lambda vals: loop(*vals)
            return jax.lax.while_loop(cond, body, values)
        else:
            cond = lambda vals: jnp.any(vals[1][0]) &amp; (vals[0] &lt; max_iter)
            body = lambda vals: (vals[0] + 1, loop(*vals[1]))
            return jax.lax.while_loop(cond, body, (self.as_tensor(0), values))[1]</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>self, shape, dtype:Â ml4s.backend._dtype.DTypeÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(self, shape, dtype: DType = None):
    self._check_float64()
    return jax.device_put(jnp.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)</code></pre>
</details>
</dd>
<dt id="ml4s.backend.jax.JaxBackend.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_like(self, tensor):
    return jax.device_put(jnp.zeros_like(tensor), self._default_device.ref)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ml4s.backend" href="../index.html">ml4s.backend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ml4s.backend.jax.stax_nets" href="stax_nets.html">ml4s.backend.jax.stax_nets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ml4s.backend.jax.JaxBackend" href="#ml4s.backend.jax.JaxBackend">JaxBackend</a></code></h4>
<ul class="">
<li><code><a title="ml4s.backend.jax.JaxBackend.abs" href="#ml4s.backend.jax.JaxBackend.abs">abs</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.all" href="#ml4s.backend.jax.JaxBackend.all">all</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.allocate_on_device" href="#ml4s.backend.jax.JaxBackend.allocate_on_device">allocate_on_device</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.any" href="#ml4s.backend.jax.JaxBackend.any">any</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arccos" href="#ml4s.backend.jax.JaxBackend.arccos">arccos</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arccosh" href="#ml4s.backend.jax.JaxBackend.arccosh">arccosh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arcsin" href="#ml4s.backend.jax.JaxBackend.arcsin">arcsin</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arcsinh" href="#ml4s.backend.jax.JaxBackend.arcsinh">arcsinh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arctan" href="#ml4s.backend.jax.JaxBackend.arctan">arctan</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arctan2" href="#ml4s.backend.jax.JaxBackend.arctan2">arctan2</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.arctanh" href="#ml4s.backend.jax.JaxBackend.arctanh">arctanh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.argsort" href="#ml4s.backend.jax.JaxBackend.argsort">argsort</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.as_tensor" href="#ml4s.backend.jax.JaxBackend.as_tensor">as_tensor</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.batched_gather_nd" href="#ml4s.backend.jax.JaxBackend.batched_gather_nd">batched_gather_nd</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.bincount" href="#ml4s.backend.jax.JaxBackend.bincount">bincount</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.block_until_ready" href="#ml4s.backend.jax.JaxBackend.block_until_ready">block_until_ready</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.boolean_mask" href="#ml4s.backend.jax.JaxBackend.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.cast" href="#ml4s.backend.jax.JaxBackend.cast">cast</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.ceil" href="#ml4s.backend.jax.JaxBackend.ceil">ceil</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.clip" href="#ml4s.backend.jax.JaxBackend.clip">clip</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.concat" href="#ml4s.backend.jax.JaxBackend.concat">concat</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.conj" href="#ml4s.backend.jax.JaxBackend.conj">conj</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.conv" href="#ml4s.backend.jax.JaxBackend.conv">conv</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.copy" href="#ml4s.backend.jax.JaxBackend.copy">copy</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.cos" href="#ml4s.backend.jax.JaxBackend.cos">cos</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.cosh" href="#ml4s.backend.jax.JaxBackend.cosh">cosh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.cumsum" href="#ml4s.backend.jax.JaxBackend.cumsum">cumsum</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.custom_gradient" href="#ml4s.backend.jax.JaxBackend.custom_gradient">custom_gradient</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.divide_no_nan" href="#ml4s.backend.jax.JaxBackend.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.dtype" href="#ml4s.backend.jax.JaxBackend.dtype">dtype</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.einsum" href="#ml4s.backend.jax.JaxBackend.einsum">einsum</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.equal" href="#ml4s.backend.jax.JaxBackend.equal">equal</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.exp" href="#ml4s.backend.jax.JaxBackend.exp">exp</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.expand_dims" href="#ml4s.backend.jax.JaxBackend.expand_dims">expand_dims</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.fft" href="#ml4s.backend.jax.JaxBackend.fft">fft</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.flip" href="#ml4s.backend.jax.JaxBackend.flip">flip</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.floor" href="#ml4s.backend.jax.JaxBackend.floor">floor</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.from_dlpack" href="#ml4s.backend.jax.JaxBackend.from_dlpack">from_dlpack</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.gather" href="#ml4s.backend.jax.JaxBackend.gather">gather</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.get_device" href="#ml4s.backend.jax.JaxBackend.get_device">get_device</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.get_diagonal" href="#ml4s.backend.jax.JaxBackend.get_diagonal">get_diagonal</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.histogram1d" href="#ml4s.backend.jax.JaxBackend.histogram1d">histogram1d</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.ifft" href="#ml4s.backend.jax.JaxBackend.ifft">ifft</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.imag" href="#ml4s.backend.jax.JaxBackend.imag">imag</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.is_available" href="#ml4s.backend.jax.JaxBackend.is_available">is_available</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.is_module" href="#ml4s.backend.jax.JaxBackend.is_module">is_module</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.is_sparse" href="#ml4s.backend.jax.JaxBackend.is_sparse">is_sparse</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.is_tensor" href="#ml4s.backend.jax.JaxBackend.is_tensor">is_tensor</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.isfinite" href="#ml4s.backend.jax.JaxBackend.isfinite">isfinite</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.isinf" href="#ml4s.backend.jax.JaxBackend.isinf">isinf</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.isnan" href="#ml4s.backend.jax.JaxBackend.isnan">isnan</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.jacobian" href="#ml4s.backend.jax.JaxBackend.jacobian">jacobian</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.jit_compile" href="#ml4s.backend.jax.JaxBackend.jit_compile">jit_compile</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.linspace" href="#ml4s.backend.jax.JaxBackend.linspace">linspace</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.linspace_without_last" href="#ml4s.backend.jax.JaxBackend.linspace_without_last">linspace_without_last</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.log" href="#ml4s.backend.jax.JaxBackend.log">log</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.log10" href="#ml4s.backend.jax.JaxBackend.log10">log10</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.log2" href="#ml4s.backend.jax.JaxBackend.log2">log2</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.log_gamma" href="#ml4s.backend.jax.JaxBackend.log_gamma">log_gamma</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.matrix_solve_least_squares" href="#ml4s.backend.jax.JaxBackend.matrix_solve_least_squares">matrix_solve_least_squares</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.max" href="#ml4s.backend.jax.JaxBackend.max">max</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.maximum" href="#ml4s.backend.jax.JaxBackend.maximum">maximum</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.mean" href="#ml4s.backend.jax.JaxBackend.mean">mean</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.meshgrid" href="#ml4s.backend.jax.JaxBackend.meshgrid">meshgrid</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.min" href="#ml4s.backend.jax.JaxBackend.min">min</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.minimum" href="#ml4s.backend.jax.JaxBackend.minimum">minimum</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.mul" href="#ml4s.backend.jax.JaxBackend.mul">mul</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.mul_matrix_batched_vector" href="#ml4s.backend.jax.JaxBackend.mul_matrix_batched_vector">mul_matrix_batched_vector</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.nn_library" href="#ml4s.backend.jax.JaxBackend.nn_library">nn_library</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.nonzero" href="#ml4s.backend.jax.JaxBackend.nonzero">nonzero</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.numpy" href="#ml4s.backend.jax.JaxBackend.numpy">numpy</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.numpy_call" href="#ml4s.backend.jax.JaxBackend.numpy_call">numpy_call</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.ones" href="#ml4s.backend.jax.JaxBackend.ones">ones</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.ones_like" href="#ml4s.backend.jax.JaxBackend.ones_like">ones_like</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.pad" href="#ml4s.backend.jax.JaxBackend.pad">pad</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.prefers_channels_last" href="#ml4s.backend.jax.JaxBackend.prefers_channels_last">prefers_channels_last</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.prod" href="#ml4s.backend.jax.JaxBackend.prod">prod</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.quantile" href="#ml4s.backend.jax.JaxBackend.quantile">quantile</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.random_normal" href="#ml4s.backend.jax.JaxBackend.random_normal">random_normal</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.random_uniform" href="#ml4s.backend.jax.JaxBackend.random_uniform">random_uniform</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.range" href="#ml4s.backend.jax.JaxBackend.range">range</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.ravel_multi_index" href="#ml4s.backend.jax.JaxBackend.ravel_multi_index">ravel_multi_index</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.real" href="#ml4s.backend.jax.JaxBackend.real">real</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.repeat" href="#ml4s.backend.jax.JaxBackend.repeat">repeat</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.requires_fixed_shapes_when_tracing" href="#ml4s.backend.jax.JaxBackend.requires_fixed_shapes_when_tracing">requires_fixed_shapes_when_tracing</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.reshape" href="#ml4s.backend.jax.JaxBackend.reshape">reshape</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.round" href="#ml4s.backend.jax.JaxBackend.round">round</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.scatter" href="#ml4s.backend.jax.JaxBackend.scatter">scatter</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.searchsorted" href="#ml4s.backend.jax.JaxBackend.searchsorted">searchsorted</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.seed" href="#ml4s.backend.jax.JaxBackend.seed">seed</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.shape" href="#ml4s.backend.jax.JaxBackend.shape">shape</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sign" href="#ml4s.backend.jax.JaxBackend.sign">sign</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sin" href="#ml4s.backend.jax.JaxBackend.sin">sin</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sinh" href="#ml4s.backend.jax.JaxBackend.sinh">sinh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.softplus" href="#ml4s.backend.jax.JaxBackend.softplus">softplus</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.solve_triangular_dense" href="#ml4s.backend.jax.JaxBackend.solve_triangular_dense">solve_triangular_dense</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sparse_coo_tensor" href="#ml4s.backend.jax.JaxBackend.sparse_coo_tensor">sparse_coo_tensor</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sqrt" href="#ml4s.backend.jax.JaxBackend.sqrt">sqrt</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.stack" href="#ml4s.backend.jax.JaxBackend.stack">stack</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.staticshape" href="#ml4s.backend.jax.JaxBackend.staticshape">staticshape</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.std" href="#ml4s.backend.jax.JaxBackend.std">std</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.stop_gradient" href="#ml4s.backend.jax.JaxBackend.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.sum" href="#ml4s.backend.jax.JaxBackend.sum">sum</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.tan" href="#ml4s.backend.jax.JaxBackend.tan">tan</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.tanh" href="#ml4s.backend.jax.JaxBackend.tanh">tanh</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.tensordot" href="#ml4s.backend.jax.JaxBackend.tensordot">tensordot</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.tile" href="#ml4s.backend.jax.JaxBackend.tile">tile</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.to_dlpack" href="#ml4s.backend.jax.JaxBackend.to_dlpack">to_dlpack</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.transpose" href="#ml4s.backend.jax.JaxBackend.transpose">transpose</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.unravel_index" href="#ml4s.backend.jax.JaxBackend.unravel_index">unravel_index</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.vectorized_call" href="#ml4s.backend.jax.JaxBackend.vectorized_call">vectorized_call</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.where" href="#ml4s.backend.jax.JaxBackend.where">where</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.while_loop" href="#ml4s.backend.jax.JaxBackend.while_loop">while_loop</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.zeros" href="#ml4s.backend.jax.JaxBackend.zeros">zeros</a></code></li>
<li><code><a title="ml4s.backend.jax.JaxBackend.zeros_like" href="#ml4s.backend.jax.JaxBackend.zeros_like">zeros_like</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>