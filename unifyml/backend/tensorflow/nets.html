<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>unifyml.backend.tensorflow.nets API documentation</title>
<meta name="description" content="TensorFlow implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>unifyml.backend.tensorflow.nets</code></h1>
</header>
<section id="section-intro">
<p>TensorFlow implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.</p>
<p>For API documentation, see <code><a title="unifyml.nn" href="../../nn.html">unifyml.nn</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
TensorFlow implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.

For API documentation, see `unifyml.nn`.
&#34;&#34;&#34;
import pickle
from typing import Callable
from typing import Union, Sequence

import numpy
import numpy as np
import tensorflow as tf
from tensorflow import Tensor
from tensorflow import keras
from tensorflow.keras import layers as kl

from ... import math


def get_parameters(model: keras.Model, wrap=True) -&gt; dict:
    result = {}
    for var in model.trainable_weights:
        name: str = var.name
        layer = name[:name.index(&#39;/&#39;)].replace(&#39;_&#39;, &#39;&#39;).replace(&#39;dense&#39;, &#39;linear&#39;)
        try:
            int(layer[-1:])
        except ValueError:
            layer += &#39;0&#39;
        prop = name[name.index(&#39;/&#39;) + 1:].replace(&#39;kernel&#39;, &#39;weight&#39;)
        if prop.endswith(&#39;:0&#39;):
            prop = prop[:-2]
        name = f&#34;{layer}.{prop}&#34;
        var = var.numpy()
        if not wrap:
            result[name] = var
        else:
            if name.endswith(&#39;.weight&#39;):
                if var.ndim == 2:
                    uml_tensor = math.wrap(var, math.channel(&#39;input,output&#39;))
                elif var.ndim == 3:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,input,output&#39;))
                elif var.ndim == 4:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,y,input,output&#39;))
                elif var.ndim == 5:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,y,z,input,output&#39;))
            elif name.endswith(&#39;.bias&#39;):
                uml_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            elif var.ndim == 1:
                uml_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            else:
                raise NotImplementedError(name, var)
            result[name] = uml_tensor
    return result


def save_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.save_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        weights = obj.get_weights()
        with open(path, &#39;wb&#39;) as f:
            pickle.dump(weights, f)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)


def load_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.load_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        with open(path, &#39;rb&#39;) as f:
            weights = pickle.load(f)
        obj.set_weights(weights)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)


def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
    with tf.GradientTape() as tape:
        output = loss_function(*loss_args, **loss_kwargs)
        loss = output[0] if isinstance(output, tuple) else output
        gradients = tape.gradient(loss.sum, net.trainable_variables)
    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
    return output


def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)


def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    assert dampening == 0
    assert weight_decay == 0
    return keras.optimizers.SGD(learning_rate, momentum, nesterov)


def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    assert lr_decay == 0
    assert weight_decay == 0
    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)


def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    assert weight_decay == 0
    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)


def mlp(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation=&#39;ReLU&#39;,
              softmax=False) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    keras_layers = []
    for neuron_count in layers:
        keras_layers.append(kl.Dense(neuron_count, activation=activation))
        if batch_norm:
            keras_layers.append(kl.BatchNormalization())
    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
                                    *keras_layers,
                                    kl.Dense(out_channels, activation=&#39;linear&#39;),
                                    *([kl.Softmax()] if softmax else [])])


def _inv_net_dense_resnet_block(in_channels: int,
                                layers: Sequence[int],
                                out_channels: int = None,
                                batch_norm: bool = False,
                                activation: Union[str, Callable] = &#39;ReLU&#39;,
                                softmax=False):
    assert not softmax, &#34;Softmax not supported inside invertible net&#34;
    assert out_channels is None or out_channels == in_channels, &#34;out_channels must match in_channels or be unspecified&#34;
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    # --- Build the dense network ---
    x = inputs = kl.Input(shape=(in_channels,))
    for neuron_count in [*layers, out_channels]:
        x = kl.Dense(neuron_count, activation=activation)(x)
        if batch_norm:
            x = kl.BatchNormalization()(x)
    x = kl.Add()([x, inputs])
    return keras.Model(inputs, x)


def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, tuple, list] = 16,
          batch_norm: bool = True,
          activation: Union[str, Callable] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    # --- Construct the U-Net ---
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    x = resnet_block(x.shape[-1], filters[0], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation, periodic)
    xs = [x]
    for i in range(1, levels):
        x = MAX_POOL[d](2, padding=&#34;same&#34;)(x)
        x = resnet_block(x.shape[-1], filters[i], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation, periodic)
        xs.insert(0, x)
    for i in range(1, levels):
        x = UPSAMPLE[d](2)(x)
        x = kl.Concatenate()([x, xs[i]])
        x = resnet_block(x.shape[-1], filters[i - 1], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)


CONV = [None, kl.Conv1D, kl.Conv2D, kl.Conv3D]
MAX_POOL = [None, kl.MaxPool1D, kl.MaxPool2D, kl.MaxPool3D]
UPSAMPLE = [None, kl.UpSampling1D, kl.UpSampling2D, kl.UpSampling3D]
ACTIVATIONS = {&#39;tanh&#39;: keras.activations.tanh, &#39;ReLU&#39;: keras.activations.relu, &#39;Sigmoid&#39;: keras.activations.sigmoid, &#39;SiLU&#39;: keras.activations.selu}


def pad_periodic(x: Tensor):
    d = len(x.shape) - 2
    if d &gt;= 1:
        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
    if d &gt;= 2:
        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
    if d &gt;= 3:
        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
                      axis=3)
    return x


def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
    x = CONV[d](mid_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](mid_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    return x


def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, Callable] = &#39;ReLU&#39;,
             periodic=False,
             in_spatial: Union[int, tuple] = 2) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    if len(layers) &lt; 1:
        layers.append(out_channels)
    for i in range(len(layers)):
        x = CONV[d](layers[i], 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](layers[i], 3, padding=&#39;same&#39;)(x)
        if batch_norm:
            x = kl.BatchNormalization()(x)
        x = activation(x)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)


def resnet_block(in_channels: int,
                 out_channels: int,
                 periodic: bool,
                 batch_norm: bool = False,
                 activation: Union[str, Callable] = &#39;ReLU&#39;,
                 in_spatial: Union[int, tuple] = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = x_1 = inputs = keras.Input(shape=(None,) * d + (in_channels,))
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    if in_channels != out_channels:
        x_1 = CONV[d](out_channels, 1)(x_1)
        if batch_norm:
            x_1 = kl.BatchNormalization()(x_1)
    x = kl.Add()([x, x_1])
    return keras.Model(inputs, x)


def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, Callable] = &#39;ReLU&#39;,
            periodic=False,
            in_spatial: Union[int, tuple] = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    if len(layers) &lt; 1:
        layers.append(out_channels)
    out = resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d)(x)
    for i in range(1, len(layers)):
        out = resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d)(out)
    out = CONV[d](out_channels, 1)(out)
    return keras.Model(inputs, out)


def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_features,))
    for i, (next, block_size) in enumerate(zip(blocks, block_sizes)):
        for j in range(block_size):
            x = CONV[d](next, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](next, 3, padding=&#39;same&#39;)(x)
            if batch_norm:
                x = kl.BatchNormalization()(x)
            x = activation(x)
        x = MAX_POOL[d](2)(x)
    x = kl.Flatten()(x)
    flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
    x = mlp(flat_size, num_classes, dense_layers, batch_norm, activation, softmax)(x)
    return keras.Model(inputs, x)


def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = tf.range(0, N)
        even_ind = range_n % 2
        checker = tf.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]
        range_h = tf.range(0, H)
        range_w = tf.range(0, W)
        even_ind_h = tf.cast(range_h % 2, dtype=tf.bool)
        even_ind_w = tf.cast(range_w % 2, dtype=tf.bool)
        ind_h = tf.tile(tf.expand_dims(even_ind_h, -1), [1, W])
        ind_w = tf.tile(tf.expand_dims(even_ind_w, 0), [H, 1])
        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
        checker = tf.math.logical_xor(ind_h, ind_w)
        reshape = [-1, 1, H, W] if data_format == &#39;NCHW&#39; else [-1, H, W, 1]
        checker = tf.reshape(checker, reshape)
        checker = tf.cast(checker, dtype=tf.float32)
    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be 2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))
    if reverse_mask:
        checker = 1 - checker
    return checker


class CouplingLayer(keras.Model):

    def __init__(self, construct_net: Callable, construction_kwargs: dict, reverse_mask):
        super().__init__()
        self.reverse_mask = reverse_mask
        self.s1 = construct_net(**construction_kwargs)
        self.t1 = construct_net(**construction_kwargs)
        self.s2 = construct_net(**construction_kwargs)
        self.t2 = construct_net(**construction_kwargs)

    def call(self, x, invert=False):
        mask = tf.cast(get_mask(x, self.reverse_mask, &#39;NCHW&#39;), x.dtype)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * tf.math.exp(tf.tanh(-self.s1(v1)))
            u1 = mask * (v1 - self.t2(u2)) * tf.math.exp(tf.tanh(-self.s2(u2)))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * tf.math.exp(tf.tanh(self.s2(u2))) + self.t2(u2))
            v2 = (1 - mask) * (u2 * tf.math.exp(tf.tanh(self.s1(v1))) + self.t1(v1))
            return v1 + v2


class InvertibleNet(keras.Model):

    def __init__(self, num_blocks: int, construct_net, construction_kwargs: dict):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        self.layer_dict = {}
        for i in range(num_blocks):
            self.layer_dict[f&#39;coupling_block{i + 1}&#39;] = CouplingLayer(construct_net, construction_kwargs, (i % 2 == 0))

    def call(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = self.layer_dict[f&#39;coupling_block{i}&#39;](x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = self.layer_dict[f&#39;coupling_block{i}&#39;](x)
        return x


def invertible_net(num_blocks: int,
                   construct_net: Union[str, Callable],
                   **construct_kwargs):  # mlp, u_net, res_net, conv_net
    if construct_net == &#39;mlp&#39;:
        construct_net = &#39;_inv_net_dense_resnet_block&#39;
    if isinstance(construct_net, str):
        construct_net = globals()[construct_net]
    if &#39;in_channels&#39; in construct_kwargs and &#39;out_channels&#39; not in construct_kwargs:
        construct_kwargs[&#39;out_channels&#39;] = construct_kwargs[&#39;in_channels&#39;]
    return InvertibleNet(num_blocks, construct_net, construct_kwargs)


# RFFT = [None, tf.signal.rfft, tf.signal.rfft2d, tf.signal.rfft3d]
# FFT = [None, tf.signal.fft, tf.signal.fft2d, tf.signal.fft3d]
# IRFFT = [None, tf.signal.irfft, tf.signal.irfft2d, tf.signal.irfft3d]
#
#
# class SpectralConv(keras.Model):
#
#     def __init__(self, in_channels, out_channels, modes, in_spatial):
#         super(SpectralConv, self).__init__()
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.in_spatial = in_spatial
#         assert 1 &lt;= in_spatial &lt;= 3
#         if isinstance(modes, int):
#             mode = modes
#             modes = [mode for i in range(in_spatial)]
#         self.scale = 1 / (in_channels * out_channels)
#         self.modes = {i + 1: modes[i] for i in range(len(modes))}
#         self.weights_ = {}
#         rand_shape = [in_channels, out_channels]
#         rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
#         for i in range(2 ** (in_spatial - 1)):
#             self.weights_[f&#39;w{i + 1}&#39;] = tf.complex(tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True),
#                                                     tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True))
#
#     def complex_mul(self, input, weights):
#         if self.in_spatial == 1:
#             return tf.einsum(&#34;bix,iox-&gt;box&#34;, input, weights)
#         elif self.in_spatial == 2:
#             return tf.einsum(&#34;bixy,ioxy-&gt;boxy&#34;, input, weights)
#         elif self.in_spatial == 3:
#             return tf.einsum(&#34;bixyz,ioxyz-&gt;boxyz&#34;, input, weights)
#
#     def call(self, x):
#         batch_size = x.shape[0]
#         x_ft = RFFT[self.in_spatial](x)
#         outft_dims = [batch_size, self.out_channels] + [x.shape[-i] for i in range(self.in_spatial, 1, -1)] + [x.shape[-1] // 2 + 1]
#         out_ft0 = tf.complex(tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)), tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)))
#         if self.in_spatial == 1:
#             out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1]], self.weights_[&#39;w1&#39;])
#             out_ft = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:]], axis=-1)
#         elif self.in_spatial == 2:
#             out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]], self.weights_[&#39;w1&#39;])
#             out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]], self.weights_[&#39;w2&#39;])
#             out_ft3 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1], :self.modes[2]], out_ft2], axis=-2)
#             out_ft = tf.concat([out_ft3, out_ft0[:, :, :, self.modes[2]:]], axis=-1)
#         elif self.in_spatial == 3:
#             out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]], self.weights_[&#39;w1&#39;])
#             out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]], self.weights_[&#39;w2&#39;])
#             out_ft3 = self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]], self.weights_[&#39;w3&#39;])
#             out_ft4 = self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]], self.weights_[&#39;w4&#39;])
#             out_ft5 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1], :self.modes[2], :self.modes[3]], out_ft2], axis=-3)
#             out_ft6 = tf.concat([out_ft3, out_ft0[:, :, self.modes[1]:-self.modes[1], -self.modes[2]:, :self.modes[3]], out_ft4], axis=-3)
#             out_ft7 = tf.concat([out_ft5, out_ft0[:, :, :, self.modes[2]:-self.modes[2], :self.modes[3]], out_ft6], axis=-2)
#             out_ft = tf.concat([out_ft7, out_ft0[:, :, :, :, self.modes[3]:]], axis=-1)
#         # --- Return to Physical Space ---
#         x = IRFFT[self.in_spatial](out_ft)
#         return x
#
#
# class FNO(keras.Model):
#     &#34;&#34;&#34;
#     Fourier Neural Operators
#     source: https://github.com/zongyi-li/fourier_neural_operator
#     &#34;&#34;&#34;
#
#     def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
#         super(FNO, self).__init__()
#         &#34;&#34;&#34;
#         The overall network. It contains 4 layers of the Fourier layer.
#         1. Lift the input to the desire channel dimension by self.fc0 .
#         2. 4 layers of the integral operators u&#39; = (W + K)(u).
#             W defined by self.w; K defined by self.conv .
#         3. Project from the channel space to the output space by self.fc1 and self.fc2.
#
#         input shape and output shape: (batchsize b, channels c, *spatial)
#         &#34;&#34;&#34;
#         self.activation = activation
#         self.width = width
#         self.in_spatial = in_spatial
#         self.batch_norm = batch_norm
#         self.fc0 = kl.Dense(self.width)
#         self.model_dict = {}
#         for i in range(4):
#             self.model_dict[f&#39;conv{i}&#39;] = SpectralConv(self.width, self.width, modes, in_spatial)
#             self.model_dict[f&#39;w{i}&#39;] = CONV[self.in_spatial](self.width, kernel_size=1)
#             if batch_norm:
#                 self.model_dict[f&#39;bn{i}&#39;] = kl.BatchNormalization()
#         self.fc1 = kl.Dense(128)
#         self.fc2 = kl.Dense(out_channels)
#
#     # Adding extra spatial channels eg. x, y, z, .... to input x
#     def get_grid(self, shape, device):
#         batch_size = shape[0]
#         grid_channel_sizes = shape[1:-1]  # shape =  (batch_size, *spatial, channels)
#         self.grid_channels = {}
#         for i in range(self.in_spatial):
#             self.grid_channels[f&#39;dim{i}&#39;] = tf.cast(tf.linspace(0, 1, grid_channel_sizes[i]), dtype=tf.dtypes.float32)  #tf.tensor(tf.linspace(0, 1, grid_channel_sizes[i]), dtype=tf.dtypes.float32)
#             reshape_dim_tuple = [1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)] + [1]
#             repeat_dim_tuple = [batch_size] + [1 if i == j else grid_channel_sizes[j] for j in range(self.in_spatial)] + [1]
#             self.grid_channels[f&#39;dim{i}&#39;] = tf.tile(tf.reshape(self.grid_channels[f&#39;dim{i}&#39;], reshape_dim_tuple), repeat_dim_tuple)
#         return tf.concat([self.grid_channels[f&#39;dim{i}&#39;] for i in range(self.in_spatial)], axis=-1)
#
#     def call(self, x):
#         grid = self.get_grid(x.shape, x.device)
#         x = tf.concat([x, grid], axis=-1)
#         permute_tuple = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
#         permute_tuple_reverse = [0] + [2 + i for i in range(self.in_spatial)] + [1]
#         # No need to Transpose x such that channels shape lies
#         # at the end to pass it through linear layers as it&#39;s the default in tf
#         # x = tf.transpose(x, permute_tuple)
#         x = self.fc0(x)
#         for i in range(4):
#             x1 = self.model_dict[f&#39;w{i}&#39;](x)
#             # Spectral conv expects a shape : [batch, channel, *spatial]
#             # hence the transpose:
#             x2 = self.model_dict[f&#39;conv{i}&#39;](tf.transpose(x, permute_tuple))
#             x2 = tf.transpose(x2, permute_tuple_reverse)
#             if self.batch_norm:
#                 x = self.model_dict[f&#39;bn{i}&#39;](x1) + self.model_dict[f&#39;bn{i}&#39;](x2)
#             x = self.activation(x)
#         x = self.activation(self.fc1(x))
#         x = self.fc2(x)
#         return x
#
#
# def fno(in_channels: int,
#         out_channels: int,
#         mid_channels: int,
#         modes: Sequence[int],
#         activation: Union[str, type] = &#39;ReLU&#39;,
#         batch_norm: bool = False,
#         in_spatial: int = 2):
#     activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
#     return FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="unifyml.backend.tensorflow.nets.adagrad"><code class="name flex">
<span>def <span class="ident">adagrad</span></span>(<span>net:Â keras.src.engine.training.Model, learning_rate:Â floatÂ =Â 0.001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    assert lr_decay == 0
    assert weight_decay == 0
    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.adam"><code class="name flex">
<span>def <span class="ident">adam</span></span>(<span>net:Â keras.src.engine.training.Model, learning_rate:Â floatÂ =Â 0.001, betas=(0.9, 0.999), epsilon=1e-07)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.conv_classifier"><code class="name flex">
<span>def <span class="ident">conv_classifier</span></span>(<span>in_features:Â int, in_spatial:Â Union[tuple,Â list], num_classes:Â int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_features,))
    for i, (next, block_size) in enumerate(zip(blocks, block_sizes)):
        for j in range(block_size):
            x = CONV[d](next, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](next, 3, padding=&#39;same&#39;)(x)
            if batch_norm:
                x = kl.BatchNormalization()(x)
            x = activation(x)
        x = MAX_POOL[d](2)(x)
    x = kl.Flatten()(x)
    flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
    x = mlp(flat_size, num_classes, dense_layers, batch_norm, activation, softmax)(x)
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.conv_net"><code class="name flex">
<span>def <span class="ident">conv_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â Callable]Â =Â 'ReLU', periodic=False, in_spatial:Â Union[int,Â tuple]Â =Â 2) â€‘>Â keras.src.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, Callable] = &#39;ReLU&#39;,
             periodic=False,
             in_spatial: Union[int, tuple] = 2) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    if len(layers) &lt; 1:
        layers.append(out_channels)
    for i in range(len(layers)):
        x = CONV[d](layers[i], 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](layers[i], 3, padding=&#39;same&#39;)(x)
        if batch_norm:
            x = kl.BatchNormalization()(x)
        x = activation(x)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.double_conv"><code class="name flex">
<span>def <span class="ident">double_conv</span></span>(<span>x, d:Â int, out_channels:Â int, mid_channels:Â int, batch_norm:Â bool, activation:Â Callable, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
    x = CONV[d](mid_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](mid_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    return x</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.get_mask"><code class="name flex">
<span>def <span class="ident">get_mask</span></span>(<span>inputs, reverse_mask, data_format='NHWC')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute mask for slicing input feature map for Invertible Nets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = tf.range(0, N)
        even_ind = range_n % 2
        checker = tf.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]
        range_h = tf.range(0, H)
        range_w = tf.range(0, W)
        even_ind_h = tf.cast(range_h % 2, dtype=tf.bool)
        even_ind_w = tf.cast(range_w % 2, dtype=tf.bool)
        ind_h = tf.tile(tf.expand_dims(even_ind_h, -1), [1, W])
        ind_w = tf.tile(tf.expand_dims(even_ind_w, 0), [H, 1])
        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
        checker = tf.math.logical_xor(ind_h, ind_w)
        reshape = [-1, 1, H, W] if data_format == &#39;NCHW&#39; else [-1, H, W, 1]
        checker = tf.reshape(checker, reshape)
        checker = tf.cast(checker, dtype=tf.float32)
    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be 2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))
    if reverse_mask:
        checker = 1 - checker
    return checker</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>model:Â keras.src.engine.training.Model, wrap=True) â€‘>Â dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_parameters(model: keras.Model, wrap=True) -&gt; dict:
    result = {}
    for var in model.trainable_weights:
        name: str = var.name
        layer = name[:name.index(&#39;/&#39;)].replace(&#39;_&#39;, &#39;&#39;).replace(&#39;dense&#39;, &#39;linear&#39;)
        try:
            int(layer[-1:])
        except ValueError:
            layer += &#39;0&#39;
        prop = name[name.index(&#39;/&#39;) + 1:].replace(&#39;kernel&#39;, &#39;weight&#39;)
        if prop.endswith(&#39;:0&#39;):
            prop = prop[:-2]
        name = f&#34;{layer}.{prop}&#34;
        var = var.numpy()
        if not wrap:
            result[name] = var
        else:
            if name.endswith(&#39;.weight&#39;):
                if var.ndim == 2:
                    uml_tensor = math.wrap(var, math.channel(&#39;input,output&#39;))
                elif var.ndim == 3:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,input,output&#39;))
                elif var.ndim == 4:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,y,input,output&#39;))
                elif var.ndim == 5:
                    uml_tensor = math.wrap(var, math.channel(&#39;x,y,z,input,output&#39;))
            elif name.endswith(&#39;.bias&#39;):
                uml_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            elif var.ndim == 1:
                uml_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            else:
                raise NotImplementedError(name, var)
            result[name] = uml_tensor
    return result</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.invertible_net"><code class="name flex">
<span>def <span class="ident">invertible_net</span></span>(<span>num_blocks:Â int, construct_net:Â Union[str,Â Callable], **construct_kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invertible_net(num_blocks: int,
                   construct_net: Union[str, Callable],
                   **construct_kwargs):  # mlp, u_net, res_net, conv_net
    if construct_net == &#39;mlp&#39;:
        construct_net = &#39;_inv_net_dense_resnet_block&#39;
    if isinstance(construct_net, str):
        construct_net = globals()[construct_net]
    if &#39;in_channels&#39; in construct_kwargs and &#39;out_channels&#39; not in construct_kwargs:
        construct_kwargs[&#39;out_channels&#39;] = construct_kwargs[&#39;in_channels&#39;]
    return InvertibleNet(num_blocks, construct_net, construct_kwargs)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.load_state"><code class="name flex">
<span>def <span class="ident">load_state</span></span>(<span>obj:Â Union[keras.src.engine.training.Model,Â keras.src.optimizers.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.load_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        with open(path, &#39;rb&#39;) as f:
            weights = pickle.load(f)
        obj.set_weights(weights)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.mlp"><code class="name flex">
<span>def <span class="ident">mlp</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm=False, activation='ReLU', softmax=False) â€‘>Â keras.src.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mlp(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation=&#39;ReLU&#39;,
              softmax=False) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    keras_layers = []
    for neuron_count in layers:
        keras_layers.append(kl.Dense(neuron_count, activation=activation))
        if batch_norm:
            keras_layers.append(kl.BatchNormalization())
    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
                                    *keras_layers,
                                    kl.Dense(out_channels, activation=&#39;linear&#39;),
                                    *([kl.Softmax()] if softmax else [])])</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.pad_periodic"><code class="name flex">
<span>def <span class="ident">pad_periodic</span></span>(<span>x:Â tensorflow.python.framework.ops.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_periodic(x: Tensor):
    d = len(x.shape) - 2
    if d &gt;= 1:
        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
    if d &gt;= 2:
        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
    if d &gt;= 3:
        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
                      axis=3)
    return x</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.res_net"><code class="name flex">
<span>def <span class="ident">res_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â Callable]Â =Â 'ReLU', periodic=False, in_spatial:Â Union[int,Â tuple]Â =Â 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, Callable] = &#39;ReLU&#39;,
            periodic=False,
            in_spatial: Union[int, tuple] = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    if len(layers) &lt; 1:
        layers.append(out_channels)
    out = resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d)(x)
    for i in range(1, len(layers)):
        out = resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d)(out)
    out = CONV[d](out_channels, 1)(out)
    return keras.Model(inputs, out)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.resnet_block"><code class="name flex">
<span>def <span class="ident">resnet_block</span></span>(<span>in_channels:Â int, out_channels:Â int, periodic:Â bool, batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â Callable]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_block(in_channels: int,
                 out_channels: int,
                 periodic: bool,
                 batch_norm: bool = False,
                 activation: Union[str, Callable] = &#39;ReLU&#39;,
                 in_spatial: Union[int, tuple] = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    x = x_1 = inputs = keras.Input(shape=(None,) * d + (in_channels,))
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    if in_channels != out_channels:
        x_1 = CONV[d](out_channels, 1)(x_1)
        if batch_norm:
            x_1 = kl.BatchNormalization()(x_1)
    x = kl.Add()([x, x_1])
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.rmsprop"><code class="name flex">
<span>def <span class="ident">rmsprop</span></span>(<span>net:Â keras.src.engine.training.Model, learning_rate:Â floatÂ =Â 0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    assert weight_decay == 0
    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.save_state"><code class="name flex">
<span>def <span class="ident">save_state</span></span>(<span>obj:Â Union[keras.src.engine.training.Model,Â keras.src.optimizers.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.save_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        weights = obj.get_weights()
        with open(path, &#39;wb&#39;) as f:
            pickle.dump(weights, f)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.sgd"><code class="name flex">
<span>def <span class="ident">sgd</span></span>(<span>net:Â keras.src.engine.training.Model, learning_rate:Â floatÂ =Â 0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    assert dampening == 0
    assert weight_decay == 0
    return keras.optimizers.SGD(learning_rate, momentum, nesterov)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.u_net"><code class="name flex">
<span>def <span class="ident">u_net</span></span>(<span>in_channels:Â int, out_channels:Â int, levels:Â intÂ =Â 4, filters:Â Union[int,Â tuple,Â list]Â =Â 16, batch_norm:Â boolÂ =Â True, activation:Â Union[str,Â Callable]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False, use_res_blocks:Â boolÂ =Â False) â€‘>Â keras.src.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, tuple, list] = 16,
          batch_norm: bool = True,
          activation: Union[str, Callable] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    # --- Construct the U-Net ---
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    x = resnet_block(x.shape[-1], filters[0], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation, periodic)
    xs = [x]
    for i in range(1, levels):
        x = MAX_POOL[d](2, padding=&#34;same&#34;)(x)
        x = resnet_block(x.shape[-1], filters[i], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation, periodic)
        xs.insert(0, x)
    for i in range(1, levels):
        x = UPSAMPLE[d](2)(x)
        x = kl.Concatenate()([x, xs[i]])
        x = resnet_block(x.shape[-1], filters[i - 1], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="unifyml.backend.tensorflow.nets.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>net:Â keras.src.engine.training.Model, optimizer:Â keras.src.optimizers.optimizer.Optimizer, loss_function:Â Callable, *loss_args, **loss_kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
    with tf.GradientTape() as tape:
        output = loss_function(*loss_args, **loss_kwargs)
        loss = output[0] if isinstance(output, tuple) else output
        gradients = tape.gradient(loss.sum, net.trainable_variables)
    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="unifyml.backend.tensorflow.nets.CouplingLayer"><code class="flex name class">
<span>class <span class="ident">CouplingLayer</span></span>
<span>(</span><span>construct_net:Â Callable, construction_kwargs:Â dict, reverse_mask)</span>
</code></dt>
<dd>
<div class="desc"><p>A model grouping layers into an object with training/inference features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>The input(s) of the model: a <code>keras.Input</code> object or a
combination of <code>keras.Input</code> objects in a dict, list or tuple.</dd>
<dt><strong><code>outputs</code></strong></dt>
<dd>The output(s) of the model: a tensor that originated from
<code>keras.Input</code> objects or a combination of such tensors in a dict,
list or tuple. See Functional API example below.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String, the name of the model.</dd>
</dl>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="language-python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>Note: Only dicts, lists, and tuples of input tensors are supported. Nested
inputs are not supported (e.g. lists of list or dicts of dict).</p>
<p>A new Functional API model can also be created by using the
intermediate tensors. This enables you to quickly extract sub-components
of the model.</p>
<p>Example:</p>
<pre><code class="language-python">inputs = keras.Input(shape=(None, None, 3))
processed = keras.layers.RandomCrop(width=32, height=32)(inputs)
conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)
pooling = keras.layers.GlobalAveragePooling2D()(conv)
feature = keras.layers.Dense(10)(pooling)

full_model = keras.Model(inputs, feature)
backbone = keras.Model(processed, conv)
activations = keras.Model(conv, feature)
</code></pre>
<p>Note that the <code>backbone</code> and <code>activations</code> models are not
created with <code>keras.Input</code> objects, but with the tensors that are originated
from <code>keras.Input</code> objects. Under the hood, the layers and weights will
be shared across these models, so that user can train the <code>full_model</code>, and
use <code>backbone</code> or <code>activations</code> to do feature extraction.
The inputs and outputs of the model can be nested structures of tensors as
well, and the created models are standard Functional API models that support
all the existing APIs.</p>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__()</code> and you should implement the model's forward pass
in <code>call()</code>.</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super().__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call()</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super().__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CouplingLayer(keras.Model):

    def __init__(self, construct_net: Callable, construction_kwargs: dict, reverse_mask):
        super().__init__()
        self.reverse_mask = reverse_mask
        self.s1 = construct_net(**construction_kwargs)
        self.t1 = construct_net(**construction_kwargs)
        self.s2 = construct_net(**construction_kwargs)
        self.t2 = construct_net(**construction_kwargs)

    def call(self, x, invert=False):
        mask = tf.cast(get_mask(x, self.reverse_mask, &#39;NCHW&#39;), x.dtype)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * tf.math.exp(tf.tanh(-self.s1(v1)))
            u1 = mask * (v1 - self.t2(u2)) * tf.math.exp(tf.tanh(-self.s2(u2)))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * tf.math.exp(tf.tanh(self.s2(u2))) + self.t2(u2))
            v2 = (1 - mask) * (u2 * tf.math.exp(tf.tanh(self.s1(v1))) + self.t1(v1))
            return v1 + v2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.engine.training.Model</li>
<li>keras.src.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.utils.version_utils.LayerVersionSelector</li>
<li>keras.src.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="unifyml.backend.tensorflow.nets.CouplingLayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x, invert=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls the model on new inputs and returns the outputs as tensors.</p>
<p>In this case <code>call()</code> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <code>tf.keras.Model</code>.
To call a model on an input, always use the <code>__call__()</code> method,
i.e. <code>model(inputs)</code>, which relies on the underlying <code>call()</code> method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or dict/list/tuple of input tensors.</dd>
<dt><strong><code>training</code></strong></dt>
<dd>Boolean or boolean scalar tensor, indicating whether to
run the <code>Network</code> in training mode or inference mode.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>A mask or list of masks. A mask can be either a boolean tensor
or None (no mask). For more details, check the guide
<a href="https://www.tensorflow.org/guide/keras/masking_and_padding">here</a>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x, invert=False):
    mask = tf.cast(get_mask(x, self.reverse_mask, &#39;NCHW&#39;), x.dtype)
    if invert:
        v1 = x * mask
        v2 = x * (1 - mask)
        u2 = (1 - mask) * (v2 - self.t1(v1)) * tf.math.exp(tf.tanh(-self.s1(v1)))
        u1 = mask * (v1 - self.t2(u2)) * tf.math.exp(tf.tanh(-self.s2(u2)))
        return u1 + u2
    else:
        u1 = x * mask
        u2 = x * (1 - mask)
        v1 = mask * (u1 * tf.math.exp(tf.tanh(self.s2(u2))) + self.t2(u2))
        v2 = (1 - mask) * (u2 * tf.math.exp(tf.tanh(self.s1(v1))) + self.t1(v1))
        return v1 + v2</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="unifyml.backend.tensorflow.nets.InvertibleNet"><code class="flex name class">
<span>class <span class="ident">InvertibleNet</span></span>
<span>(</span><span>num_blocks:Â int, construct_net, construction_kwargs:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>A model grouping layers into an object with training/inference features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>The input(s) of the model: a <code>keras.Input</code> object or a
combination of <code>keras.Input</code> objects in a dict, list or tuple.</dd>
<dt><strong><code>outputs</code></strong></dt>
<dd>The output(s) of the model: a tensor that originated from
<code>keras.Input</code> objects or a combination of such tensors in a dict,
list or tuple. See Functional API example below.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String, the name of the model.</dd>
</dl>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="language-python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>Note: Only dicts, lists, and tuples of input tensors are supported. Nested
inputs are not supported (e.g. lists of list or dicts of dict).</p>
<p>A new Functional API model can also be created by using the
intermediate tensors. This enables you to quickly extract sub-components
of the model.</p>
<p>Example:</p>
<pre><code class="language-python">inputs = keras.Input(shape=(None, None, 3))
processed = keras.layers.RandomCrop(width=32, height=32)(inputs)
conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)
pooling = keras.layers.GlobalAveragePooling2D()(conv)
feature = keras.layers.Dense(10)(pooling)

full_model = keras.Model(inputs, feature)
backbone = keras.Model(processed, conv)
activations = keras.Model(conv, feature)
</code></pre>
<p>Note that the <code>backbone</code> and <code>activations</code> models are not
created with <code>keras.Input</code> objects, but with the tensors that are originated
from <code>keras.Input</code> objects. Under the hood, the layers and weights will
be shared across these models, so that user can train the <code>full_model</code>, and
use <code>backbone</code> or <code>activations</code> to do feature extraction.
The inputs and outputs of the model can be nested structures of tensors as
well, and the created models are standard Functional API models that support
all the existing APIs.</p>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__()</code> and you should implement the model's forward pass
in <code>call()</code>.</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super().__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call()</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super().__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InvertibleNet(keras.Model):

    def __init__(self, num_blocks: int, construct_net, construction_kwargs: dict):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        self.layer_dict = {}
        for i in range(num_blocks):
            self.layer_dict[f&#39;coupling_block{i + 1}&#39;] = CouplingLayer(construct_net, construction_kwargs, (i % 2 == 0))

    def call(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = self.layer_dict[f&#39;coupling_block{i}&#39;](x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = self.layer_dict[f&#39;coupling_block{i}&#39;](x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.engine.training.Model</li>
<li>keras.src.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.utils.version_utils.LayerVersionSelector</li>
<li>keras.src.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="unifyml.backend.tensorflow.nets.InvertibleNet.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x, backward=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls the model on new inputs and returns the outputs as tensors.</p>
<p>In this case <code>call()</code> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <code>tf.keras.Model</code>.
To call a model on an input, always use the <code>__call__()</code> method,
i.e. <code>model(inputs)</code>, which relies on the underlying <code>call()</code> method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or dict/list/tuple of input tensors.</dd>
<dt><strong><code>training</code></strong></dt>
<dd>Boolean or boolean scalar tensor, indicating whether to
run the <code>Network</code> in training mode or inference mode.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>A mask or list of masks. A mask can be either a boolean tensor
or None (no mask). For more details, check the guide
<a href="https://www.tensorflow.org/guide/keras/masking_and_padding">here</a>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x, backward=False):
    if backward:
        for i in range(self.num_blocks, 0, -1):
            x = self.layer_dict[f&#39;coupling_block{i}&#39;](x, backward)
    else:
        for i in range(1, self.num_blocks + 1):
            x = self.layer_dict[f&#39;coupling_block{i}&#39;](x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="unifyml.backend.tensorflow" href="index.html">unifyml.backend.tensorflow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="unifyml.backend.tensorflow.nets.adagrad" href="#unifyml.backend.tensorflow.nets.adagrad">adagrad</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.adam" href="#unifyml.backend.tensorflow.nets.adam">adam</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.conv_classifier" href="#unifyml.backend.tensorflow.nets.conv_classifier">conv_classifier</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.conv_net" href="#unifyml.backend.tensorflow.nets.conv_net">conv_net</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.double_conv" href="#unifyml.backend.tensorflow.nets.double_conv">double_conv</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.get_mask" href="#unifyml.backend.tensorflow.nets.get_mask">get_mask</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.get_parameters" href="#unifyml.backend.tensorflow.nets.get_parameters">get_parameters</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.invertible_net" href="#unifyml.backend.tensorflow.nets.invertible_net">invertible_net</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.load_state" href="#unifyml.backend.tensorflow.nets.load_state">load_state</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.mlp" href="#unifyml.backend.tensorflow.nets.mlp">mlp</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.pad_periodic" href="#unifyml.backend.tensorflow.nets.pad_periodic">pad_periodic</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.res_net" href="#unifyml.backend.tensorflow.nets.res_net">res_net</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.resnet_block" href="#unifyml.backend.tensorflow.nets.resnet_block">resnet_block</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.rmsprop" href="#unifyml.backend.tensorflow.nets.rmsprop">rmsprop</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.save_state" href="#unifyml.backend.tensorflow.nets.save_state">save_state</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.sgd" href="#unifyml.backend.tensorflow.nets.sgd">sgd</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.u_net" href="#unifyml.backend.tensorflow.nets.u_net">u_net</a></code></li>
<li><code><a title="unifyml.backend.tensorflow.nets.update_weights" href="#unifyml.backend.tensorflow.nets.update_weights">update_weights</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="unifyml.backend.tensorflow.nets.CouplingLayer" href="#unifyml.backend.tensorflow.nets.CouplingLayer">CouplingLayer</a></code></h4>
<ul class="">
<li><code><a title="unifyml.backend.tensorflow.nets.CouplingLayer.call" href="#unifyml.backend.tensorflow.nets.CouplingLayer.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="unifyml.backend.tensorflow.nets.InvertibleNet" href="#unifyml.backend.tensorflow.nets.InvertibleNet">InvertibleNet</a></code></h4>
<ul class="">
<li><code><a title="unifyml.backend.tensorflow.nets.InvertibleNet.call" href="#unifyml.backend.tensorflow.nets.InvertibleNet.call">call</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>