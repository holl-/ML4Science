<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>phiml.backend.torch.nets API documentation</title>
<meta name="description" content="PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phiml.backend.torch.nets</code></h1>
</header>
<section id="section-intro">
<p>PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.</p>
<p>For API documentation, see <code><a title="phiml.nn" href="../../nn.html">phiml.nn</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.

For API documentation, see `phiml.nn`.
&#34;&#34;&#34;
from typing import Callable, Union, Sequence

import numpy as np
import torch
import torch.nn as nn
from torch import optim

from . import TORCH
from ._torch_backend import register_module_call
from ... import math


def get_parameters(net: nn.Module, wrap=True) -&gt; dict:
    if not wrap:
        return {name: param for name, param in net.named_parameters()}
    result = {}
    for name, param in net.named_parameters():
        if name.endswith(&#39;.weight&#39;):
            order = [
                None,
                &#39;output&#39;,
                &#39;input,output&#39;,
                &#39;x,input,output&#39;,
                &#39;x,y,input,output&#39;,
                &#39;x,y,z,input,output&#39;
            ][param.ndim]
            uml_tensor = math.wrap(param, math.channel(order))
        elif name.endswith(&#39;.bias&#39;):
            uml_tensor = math.wrap(param, math.channel(&#39;output&#39;))
        else:
            raise NotImplementedError
        result[name] = uml_tensor
    return result


def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    torch.save(obj.state_dict(), path)


def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    obj.load_state_dict(torch.load(path))


def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs):
    optimizer.zero_grad()
    output = loss_function(*loss_args, **loss_kwargs)
    loss = output[0] if isinstance(output, tuple) else output
    loss.sum.backward()
    if isinstance(optimizer, optim.LBFGS):
        def closure():
            result = loss_function(*loss_args, **loss_kwargs)
            loss_val = result[0] if isinstance(result, tuple) else result
            return loss_val.sum
        optimizer.step(closure=closure)
    else:
        if check_nan:
            for p in net.parameters():
                if not torch.all(torch.isfinite(p.grad)):
                    raise RuntimeError(f&#34;NaN in network gradient detected. Parameter: {p}&#34;)
        optimizer.step()
    return output


def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)


def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)


def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)


def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)


def _bias0(conv):
    def initialize(*args, **kwargs):
        module = conv(*args, **kwargs)
        module.bias.data.fill_(0)
        return module
    return initialize


CONV = [None, _bias0(nn.Conv1d), _bias0(nn.Conv2d), _bias0(nn.Conv3d)]
NORM = [None, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]
ACTIVATIONS = {&#39;ReLU&#39;: nn.ReLU, &#39;Sigmoid&#39;: nn.Sigmoid, &#39;tanh&#39;: nn.Tanh, &#39;SiLU&#39;: nn.SiLU, &#39;GeLU&#39;: nn.GELU}


def mlp(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation: Union[str, Callable] = &#39;ReLU&#39;,
              softmax=False) -&gt; nn.Module:
    layers = [in_channels, *layers, out_channels]
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = DenseNet(layers, activation, batch_norm, softmax)
    return net.to(TORCH.get_default_device().ref)


class DenseNet(nn.Module):

    def __init__(self,
                 layers: list,
                 activation: type,
                 batch_norm: bool,
                 use_softmax: bool):
        super(DenseNet, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))
        self.add_module(f&#39;linear_out&#39;, _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
        self.softmax = nn.Softmax() if use_softmax else None

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        for i in range(len(self._layers) - 2):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        x = getattr(self, f&#39;linear_out&#39;)(x)
        if self.softmax:
            x = self.softmax(x)
        return x


class DenseResNetBlock(nn.Module):

    def __init__(self, layers, batch_norm, activation):
        super(DenseResNetBlock, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-1], layers[1:])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))

    def forward(self, x):
        x0 = x
        for i in range(len(self._layers) - 1):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        return x + x0


def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, Sequence] = 16,
          batch_norm: bool = True,
          activation: Union[str, type] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False,
          down_kernel_size=3,
          up_kernel_size=3) -&gt; nn.Module:
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks, down_kernel_size, up_kernel_size)
    return net.to(TORCH.get_default_device().ref)


class UNet(nn.Module):

    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, down_kernel_size: int, up_kernel_size: int):
        super(UNet, self).__init__()
        self._levels = len(filters)
        self._spatial_rank = d
        if use_res_blocks:
            self.add_module(&#39;inc&#39;, ResNetBlock(d, in_channels, filters[0], batch_norm, activation, periodic, down_kernel_size))
        else:
            self.add_module(&#39;inc&#39;, DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic, down_kernel_size))
        for i in range(1, self._levels):
            self.add_module(f&#39;down{i}&#39;, Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks, down_kernel_size))
            self.add_module(f&#39;up{i}&#39;, Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks, up_kernel_size))
        self.add_module(&#39;outc&#39;, CONV[d](filters[0], out_channels, kernel_size=1))

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        x = self.inc(x)
        xs = [x]
        for i in range(1, self._levels):
            x = getattr(self, f&#39;down{i}&#39;)(x)
            xs.insert(0, x)
        for i in range(1, self._levels):
            x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
        x = self.outc(x)
        return x


class DoubleConv(nn.Module):
    &#34;&#34;&#34;(convolution =&gt; [BN] =&gt; ReLU) * 2&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool, kernel_size=3):
        super().__init__()
        self.add_module(&#39;double_conv&#39;, nn.Sequential(
            CONV[d](in_channels, mid_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](mid_channels) if batch_norm else nn.Identity(),
            activation(),
            CONV[d](mid_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True)
        ))

    def forward(self, x):
        return self.double_conv(x)


MAX_POOL = [None, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]


class Down(nn.Module):
    &#34;&#34;&#34;Downscaling with maxpool then double conv or resnet_block&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic, kernel_size: int):
        super().__init__()
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        if use_res_blocks:
            self.add_module(&#39;conv&#39;, ResNetBlock(d, in_channels, out_channels, batch_norm, activation, periodic, kernel_size))
        else:
            self.add_module(&#39;conv&#39;, DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic, kernel_size))

    def forward(self, x):
        x = self.maxpool(x)
        return self.conv(x)


class Up(nn.Module):
    &#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;

    _MODES = [None, &#39;linear&#39;, &#39;bilinear&#39;, &#39;trilinear&#39;]

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, kernel_size: int):
        super().__init__()
        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
        if use_res_blocks:
            conv = ResNetBlock(d, in_channels, out_channels, batch_norm, activation, periodic, kernel_size)
        else:
            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic, kernel_size)
        self.add_module(&#39;up&#39;, up)
        self.add_module(&#39;conv&#39;, conv)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
        #                 diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class ConvNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ConvNet, self).__init__()
        activation = ACTIVATIONS[activation]
        if len(layers) &lt; 1:
            layers.append(out_channels)
        self.layers = layers
        self.add_module(f&#39;Conv_in&#39;, nn.Sequential(
            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
            activation()))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Conv{i}&#39;, nn.Sequential(
                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
                activation()))
        self.add_module(f&#39;Conv_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = getattr(self, f&#39;Conv_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Conv{i}&#39;)(x)
        x = getattr(self, f&#39;Conv_out&#39;)(x)
        return x


def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, type] = &#39;ReLU&#39;,
             in_spatial: Union[int, tuple] = 2,
             periodic=False) -&gt; nn.Module:
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net


class ResNetBlock(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool, kernel_size=3):
        # Since in_channels and out_channels might be different, we need a sampling layer for up/down sampling input in order to add it as a skip connection
        super(ResNetBlock, self).__init__()
        if in_channels != out_channels:
            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        else:
            self.sample_input = nn.Identity()
            self.bn_sample = nn.Identity()
        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)
        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.conv1(x)))
        out = self.activation()(self.bn2(self.conv2(out)))
        out = (out + self.bn_sample(self.sample_input(x)))
        return out


def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = torch.arange(0, N)
        even_ind = range_n % 2
        checker = torch.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]
        range_h = torch.arange(0, H)
        range_w = torch.arange(0, W)
        even_ind_h = range_h % 2
        even_ind_w = range_w % 2
        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)
        checker = torch.logical_xor(ind_h, ind_w)
        checker = checker.reshape(1, 1, H, W) if data_format == &#39;NCHW&#39; else checker.reshape(1, H, W, 1)
        checker = checker.long()
    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be 2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))
    if reverse_mask:
        checker = 1 - checker
    return checker.to(TORCH.get_default_device().ref)


class ResNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ResNet, self).__init__()
        self.layers = layers
        if len(self.layers) &lt; 1:
            layers.append(out_channels)
        self.add_module(&#39;Res_in&#39;, ResNetBlock(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Res{i}&#39;, ResNetBlock(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
        self.add_module(&#39;Res_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = TORCH.as_tensor(x)
        x = getattr(self, &#39;Res_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Res{i}&#39;)(x)
        x = getattr(self, &#39;Res_out&#39;)(x)
        return x


def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, type] = &#39;ReLU&#39;,
            in_spatial: Union[int, tuple] = 2,
            periodic=False) -&gt; nn.Module:
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net


def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, block_sizes, dense_layers, periodic, activation)
    return net.to(TORCH.get_default_device().ref)


class ConvClassifier(nn.Module):

    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation):
        super(ConvClassifier, self).__init__()
        d = len(in_spatial)
        self.in_spatial = in_spatial
        self._blocks = blocks
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
            block_size = block_sizes[i]
            layers = []
            for j in range(block_size):
                layers.append(CONV[d](prev if j == 0 else next, next, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;))
                layers.append(NORM[d](next) if batch_norm else nn.Identity())
                layers.append(activation())
            self.add_module(f&#39;conv{i+1}&#39;, nn.Sequential(*layers))
        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
        self.mlp = mlp(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
        self.flatten = nn.Flatten()

    def forward(self, x):
        for i in range(len(self._blocks)):
            x = getattr(self, f&#39;conv{i+1}&#39;)(x)
            x = self.maxpool(x)
        x = self.flatten(x)
        x = self.mlp(x)
        return x


NET = {&#39;u_net&#39;: u_net, &#39;res_net&#39;: res_net, &#39;conv_net&#39;: conv_net}


class CouplingLayer(nn.Module):

    def __init__(self, construct_net: Callable, construction_kwargs: dict, reverse_mask):
        super(CouplingLayer, self).__init__()
        self.reverse_mask = reverse_mask
        self.s1 = construct_net(**construction_kwargs)
        self.t1 = construct_net(**construction_kwargs)
        self.s2 = construct_net(**construction_kwargs)
        self.t2 = construct_net(**construction_kwargs)

    def forward(self, x, invert=False):
        x = TORCH.as_tensor(x)
        mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
            return v1 + v2


class InvertibleNet(nn.Module):
    def __init__(self, num_blocks: int, construct_net, construction_kwargs: dict):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        for i in range(num_blocks):
            self.add_module(f&#39;coupling_block{i + 1}&#39;, CouplingLayer(construct_net, construction_kwargs, (i % 2 == 0)))

    def forward(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        return x


def invertible_net(num_blocks: int,
                   construct_net: Union[str, Callable],
                   **construct_kwargs):  # mlp, u_net, res_net, conv_net
    if construct_net == &#39;mlp&#39;:
        def construct_net(in_channels: int, layers: Sequence[int], batch_norm=False, activation=&#39;ReLU&#39;, softmax=False, out_channels: int = None):
            assert not softmax, &#34;Softmax not supported inside invertible net&#34;
            assert out_channels is None or out_channels == in_channels, &#34;out_channels must match in_channels or be unspecified&#34;
            activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
            layers = [in_channels, *layers, in_channels]
            return DenseResNetBlock(layers, batch_norm=batch_norm, activation=activation)
    if isinstance(construct_net, str):
        construct_net = globals()[construct_net]
    if &#39;in_channels&#39; in construct_kwargs and &#39;out_channels&#39; not in construct_kwargs:
        construct_kwargs[&#39;out_channels&#39;] = construct_kwargs[&#39;in_channels&#39;]
    return InvertibleNet(num_blocks, construct_net, construct_kwargs).to(TORCH.get_default_device().ref)


def coupling_layer(in_channels: int,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   batch_norm=False,
                   reverse_mask=False,
                   in_spatial: Union[tuple, int] = 2):
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
    net = net.to(TORCH.get_default_device().ref)
    return net


# class SpectralConv(nn.Module):
#
#     def __init__(self, in_channels, out_channels, modes, in_spatial):
#         super(SpectralConv, self).__init__()
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.in_spatial = in_spatial
#         assert in_spatial &gt;= 1 and in_spatial &lt;= 3
#         if isinstance(modes, int):
#             mode = modes
#             modes = [mode for i in range(in_spatial)]
#         self.scale = 1 / (in_channels * out_channels)
#         self.modes = {i + 1: modes[i] for i in range(len(modes))}
#         self.weights = {}
#         rand_shape = [in_channels, out_channels]
#         rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
#         for i in range(2 ** (in_spatial - 1)):
#             self.weights[f&#39;w{i + 1}&#39;] = nn.Parameter(self.scale * torch.randn(rand_shape, dtype=torch.cfloat))
#
#     def complex_mul(self, input, weights):
#         if self.in_spatial == 1:
#             return torch.einsum(&#34;bix,iox-&gt;box&#34;, input, weights)
#         elif self.in_spatial == 2:
#             return torch.einsum(&#34;bixy,ioxy-&gt;boxy&#34;, input, weights)
#         elif self.in_spatial == 3:
#             return torch.einsum(&#34;bixyz,ioxyz-&gt;boxyz&#34;, input, weights)
#
#     def forward(self, x):
#         batch_size = x.shape[0]
#         # --- Convert to Fourier space ---
#         dims = [-i for i in range(self.in_spatial, 0, -1)]
#         x_ft = torch.fft.rfftn(x, dim=dims)
#         outft_dims = [batch_size, self.out_channels] + [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
#         out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
#         # --- Multiply relevant fourier modes ---
#         if self.in_spatial == 1:
#             out_ft[:, :, :self.modes[1]] = self.complex_mul(x_ft[:, :, :self.modes[1]], self.weights[&#39;w1&#39;].to(x_ft.device))
#         elif self.in_spatial == 2:
#             out_ft[:, :, :self.modes[1], :self.modes[2]] = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]], self.weights[&#39;w1&#39;].to(x_ft.device))
#             out_ft[:, :, -self.modes[1]:, :self.modes[2]] = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]], self.weights[&#39;w2&#39;].to(x_ft.device))
#         elif self.in_spatial == 3:
#             out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]], self.weights[&#39;w1&#39;].to(x_ft.device))
#             out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]], self.weights[&#39;w2&#39;].to(x_ft.device))
#             out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]], self.weights[&#39;w3&#39;].to(x_ft.device))
#             out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]], self.weights[&#39;w4&#39;].to(x_ft.device))
#         # --- Return to Physical Space ---
#         x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
#         return x
#
#
# class FNO(nn.Module):
#     &#34;&#34;&#34;
#     Fourier Neural Operators
#     source: https://github.com/zongyi-li/fourier_neural_operator
#
#     The overall network contains 4 layers of the [&#34;Fourier layer&#34;](https://github.com/zongyi-li/fourier_neural_operator).
#     1. Lift the input to the desire channel dimension by self.fc0 .
#     2. 4 layers of the integral operators u&#39; = (W + K)(u).
#         W defined by self.w; K defined by self.conv .
#     3. Project from the channel space to the output space by self.fc1 and self.fc2.
#
#     input shape and output shape: (batchsize b, channels c, *spatial)
#     &#34;&#34;&#34;
#
#     def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
#         super(FNO, self).__init__()
#         self.activation = activation
#         self.width = width
#         self.in_spatial = in_spatial
#         self.fc0 = _bias0(nn.Linear)(in_channels + in_spatial, self.width)
#         for i in range(4):
#             self.add_module(f&#39;conv{i}&#39;, SpectralConv(self.width, self.width, modes, in_spatial))
#             self.add_module(f&#39;w{i}&#39;, CONV[in_spatial](self.width, self.width, kernel_size=1))
#             self.add_module(f&#39;bn{i}&#39;, NORM[in_spatial](self.width) if batch_norm else nn.Identity())
#         self.fc1 = _bias0(nn.Linear)(self.width, 128)
#         self.fc2 = _bias0(nn.Linear)(128, out_channels)
#
#     # Adding extra spatial channels eg. x, y, z, .... to input x
#     def get_grid(self, shape, device):
#         batch_size = shape[0]
#         grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
#         self.grid_channels = {}
#         for i in range(self.in_spatial):
#             self.grid_channels[f&#39;dim{i}&#39;] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]), dtype=torch.float)
#             reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
#             repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in range(self.in_spatial)]
#             self.grid_channels[f&#39;dim{i}&#39;] = self.grid_channels[f&#39;dim{i}&#39;].reshape(reshape_dim_tuple).repeat(repeat_dim_tuple)
#         return torch.cat([self.grid_channels[f&#39;dim{i}&#39;] for i in range(self.in_spatial)], dim=1).to(device)
#
#     def forward(self, x):
#         grid = self.get_grid(x.shape, x.device)
#         x = torch.cat([x, grid], dim=1)
#         permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
#         permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
#         # Transpose x such that channels shape lies at the end to pass it through linear layers
#         x = x.permute(permute_tuple)
#         x = self.fc0(x)
#         # Transpose x back to its original shape to pass it through convolutional layers
#         x = x.permute(permute_tuple_reverse)
#         for i in range(4):
#             x1 = getattr(self, f&#39;w{i}&#39;)(x)
#             x2 = getattr(self, f&#39;conv{i}&#39;)(x)
#             x = getattr(self, f&#39;bn{i}&#39;)(x1) + getattr(self, f&#39;bn{i}&#39;)(x2)
#             x = self.activation()(x)
#         x = x.permute(permute_tuple)
#         x = self.activation()(self.fc1(x))
#         x = self.fc2(x)
#         x = x.permute(permute_tuple_reverse)
#         return x
#
#
# def fno(in_channels: int,
#         out_channels: int,
#         mid_channels: int,
#         modes: Sequence[int],
#         activation: Union[str, type] = &#39;ReLU&#39;,
#         batch_norm: bool = False,
#         in_spatial: int = 2):
#     activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
#     net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
#     net = net.to(TORCH.get_default_device().ref)
#     return net</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phiml.backend.torch.nets.adagrad"><code class="name flex">
<span>def <span class="ident">adagrad</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.adam"><code class="name flex">
<span>def <span class="ident">adam</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, betas=(0.9, 0.999), epsilon=1e-07)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.conv_classifier"><code class="name flex">
<span>def <span class="ident">conv_classifier</span></span>(<span>in_features:Â int, in_spatial:Â Union[tuple,Â list], num_classes:Â int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, block_sizes, dense_layers, periodic, activation)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.conv_net"><code class="name flex">
<span>def <span class="ident">conv_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, type] = &#39;ReLU&#39;,
             in_spatial: Union[int, tuple] = 2,
             periodic=False) -&gt; nn.Module:
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.coupling_layer"><code class="name flex">
<span>def <span class="ident">coupling_layer</span></span>(<span>in_channels:Â int, activation:Â Union[str,Â type]Â =Â 'ReLU', batch_norm=False, reverse_mask=False, in_spatial:Â Union[int,Â tuple]Â =Â 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coupling_layer(in_channels: int,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   batch_norm=False,
                   reverse_mask=False,
                   in_spatial: Union[tuple, int] = 2):
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.get_mask"><code class="name flex">
<span>def <span class="ident">get_mask</span></span>(<span>inputs, reverse_mask, data_format='NHWC')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute mask for slicing input feature map for Invertible Nets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = torch.arange(0, N)
        even_ind = range_n % 2
        checker = torch.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]
        range_h = torch.arange(0, H)
        range_w = torch.arange(0, W)
        even_ind_h = range_h % 2
        even_ind_w = range_w % 2
        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)
        checker = torch.logical_xor(ind_h, ind_w)
        checker = checker.reshape(1, 1, H, W) if data_format == &#39;NCHW&#39; else checker.reshape(1, H, W, 1)
        checker = checker.long()
    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be 2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))
    if reverse_mask:
        checker = 1 - checker
    return checker.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>net:Â torch.nn.modules.module.Module, wrap=True) â€‘>Â dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_parameters(net: nn.Module, wrap=True) -&gt; dict:
    if not wrap:
        return {name: param for name, param in net.named_parameters()}
    result = {}
    for name, param in net.named_parameters():
        if name.endswith(&#39;.weight&#39;):
            order = [
                None,
                &#39;output&#39;,
                &#39;input,output&#39;,
                &#39;x,input,output&#39;,
                &#39;x,y,input,output&#39;,
                &#39;x,y,z,input,output&#39;
            ][param.ndim]
            uml_tensor = math.wrap(param, math.channel(order))
        elif name.endswith(&#39;.bias&#39;):
            uml_tensor = math.wrap(param, math.channel(&#39;output&#39;))
        else:
            raise NotImplementedError
        result[name] = uml_tensor
    return result</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.invertible_net"><code class="name flex">
<span>def <span class="ident">invertible_net</span></span>(<span>num_blocks:Â int, construct_net:Â Union[str,Â Callable], **construct_kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invertible_net(num_blocks: int,
                   construct_net: Union[str, Callable],
                   **construct_kwargs):  # mlp, u_net, res_net, conv_net
    if construct_net == &#39;mlp&#39;:
        def construct_net(in_channels: int, layers: Sequence[int], batch_norm=False, activation=&#39;ReLU&#39;, softmax=False, out_channels: int = None):
            assert not softmax, &#34;Softmax not supported inside invertible net&#34;
            assert out_channels is None or out_channels == in_channels, &#34;out_channels must match in_channels or be unspecified&#34;
            activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
            layers = [in_channels, *layers, in_channels]
            return DenseResNetBlock(layers, batch_norm=batch_norm, activation=activation)
    if isinstance(construct_net, str):
        construct_net = globals()[construct_net]
    if &#39;in_channels&#39; in construct_kwargs and &#39;out_channels&#39; not in construct_kwargs:
        construct_kwargs[&#39;out_channels&#39;] = construct_kwargs[&#39;in_channels&#39;]
    return InvertibleNet(num_blocks, construct_net, construct_kwargs).to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.load_state"><code class="name flex">
<span>def <span class="ident">load_state</span></span>(<span>obj:Â Union[torch.nn.modules.module.Module,Â torch.optim.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    obj.load_state_dict(torch.load(path))</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.mlp"><code class="name flex">
<span>def <span class="ident">mlp</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm=False, activation:Â Union[str,Â Callable]Â =Â 'ReLU', softmax=False) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mlp(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation: Union[str, Callable] = &#39;ReLU&#39;,
              softmax=False) -&gt; nn.Module:
    layers = [in_channels, *layers, out_channels]
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = DenseNet(layers, activation, batch_norm, softmax)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.res_net"><code class="name flex">
<span>def <span class="ident">res_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, type] = &#39;ReLU&#39;,
            in_spatial: Union[int, tuple] = 2,
            periodic=False) -&gt; nn.Module:
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.rmsprop"><code class="name flex">
<span>def <span class="ident">rmsprop</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.save_state"><code class="name flex">
<span>def <span class="ident">save_state</span></span>(<span>obj:Â Union[torch.nn.modules.module.Module,Â torch.optim.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    torch.save(obj.state_dict(), path)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.sgd"><code class="name flex">
<span>def <span class="ident">sgd</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.u_net"><code class="name flex">
<span>def <span class="ident">u_net</span></span>(<span>in_channels:Â int, out_channels:Â int, levels:Â intÂ =Â 4, filters:Â Union[int,Â Sequence[+T_co]]Â =Â 16, batch_norm:Â boolÂ =Â True, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False, use_res_blocks:Â boolÂ =Â False, down_kernel_size=3, up_kernel_size=3) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, Sequence] = 16,
          batch_norm: bool = True,
          activation: Union[str, type] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False,
          down_kernel_size=3,
          up_kernel_size=3) -&gt; nn.Module:
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks, down_kernel_size, up_kernel_size)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phiml.backend.torch.nets.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>net:Â torch.nn.modules.module.Module, optimizer:Â torch.optim.optimizer.Optimizer, loss_function:Â Callable, *loss_args, check_nan=False, **loss_kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs):
    optimizer.zero_grad()
    output = loss_function(*loss_args, **loss_kwargs)
    loss = output[0] if isinstance(output, tuple) else output
    loss.sum.backward()
    if isinstance(optimizer, optim.LBFGS):
        def closure():
            result = loss_function(*loss_args, **loss_kwargs)
            loss_val = result[0] if isinstance(result, tuple) else result
            return loss_val.sum
        optimizer.step(closure=closure)
    else:
        if check_nan:
            for p in net.parameters():
                if not torch.all(torch.isfinite(p.grad)):
                    raise RuntimeError(f&#34;NaN in network gradient detected. Parameter: {p}&#34;)
        optimizer.step()
    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phiml.backend.torch.nets.ConvClassifier"><code class="flex name class">
<span>class <span class="ident">ConvClassifier</span></span>
<span>(</span><span>in_features, in_spatial:Â list, num_classes:Â int, batch_norm:Â bool, use_softmax:Â bool, blocks:Â tuple, block_sizes:Â tuple, dense_layers:Â tuple, periodic:Â bool, activation)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvClassifier(nn.Module):

    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation):
        super(ConvClassifier, self).__init__()
        d = len(in_spatial)
        self.in_spatial = in_spatial
        self._blocks = blocks
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
            block_size = block_sizes[i]
            layers = []
            for j in range(block_size):
                layers.append(CONV[d](prev if j == 0 else next, next, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;))
                layers.append(NORM[d](next) if batch_norm else nn.Identity())
                layers.append(activation())
            self.add_module(f&#39;conv{i+1}&#39;, nn.Sequential(*layers))
        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
        self.mlp = mlp(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
        self.flatten = nn.Flatten()

    def forward(self, x):
        for i in range(len(self._blocks)):
            x = getattr(self, f&#39;conv{i+1}&#39;)(x)
            x = self.maxpool(x)
        x = self.flatten(x)
        x = self.mlp(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.ConvClassifier.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ConvClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ConvClassifier.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.ConvClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    for i in range(len(self._blocks)):
        x = getattr(self, f&#39;conv{i+1}&#39;)(x)
        x = self.maxpool(x)
    x = self.flatten(x)
    x = self.mlp(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.ConvNet"><code class="flex name class">
<span>class <span class="ident">ConvNet</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ConvNet, self).__init__()
        activation = ACTIVATIONS[activation]
        if len(layers) &lt; 1:
            layers.append(out_channels)
        self.layers = layers
        self.add_module(f&#39;Conv_in&#39;, nn.Sequential(
            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
            activation()))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Conv{i}&#39;, nn.Sequential(
                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
                activation()))
        self.add_module(f&#39;Conv_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = getattr(self, f&#39;Conv_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Conv{i}&#39;)(x)
        x = getattr(self, f&#39;Conv_out&#39;)(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.ConvNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ConvNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ConvNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.ConvNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = getattr(self, f&#39;Conv_in&#39;)(x)
    for i in range(1, len(self.layers)):
        x = getattr(self, f&#39;Conv{i}&#39;)(x)
    x = getattr(self, f&#39;Conv_out&#39;)(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.CouplingLayer"><code class="flex name class">
<span>class <span class="ident">CouplingLayer</span></span>
<span>(</span><span>construct_net:Â Callable, construction_kwargs:Â dict, reverse_mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CouplingLayer(nn.Module):

    def __init__(self, construct_net: Callable, construction_kwargs: dict, reverse_mask):
        super(CouplingLayer, self).__init__()
        self.reverse_mask = reverse_mask
        self.s1 = construct_net(**construction_kwargs)
        self.t1 = construct_net(**construction_kwargs)
        self.s2 = construct_net(**construction_kwargs)
        self.t2 = construct_net(**construction_kwargs)

    def forward(self, x, invert=False):
        x = TORCH.as_tensor(x)
        mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
            return v1 + v2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.CouplingLayer.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.CouplingLayer.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.CouplingLayer.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.CouplingLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, invert=False) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, invert=False):
    x = TORCH.as_tensor(x)
    mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
    if invert:
        v1 = x * mask
        v2 = x * (1 - mask)
        u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
        u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
        return u1 + u2
    else:
        u1 = x * mask
        u2 = x * (1 - mask)
        v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
        v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
        return v1 + v2</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.DenseNet"><code class="flex name class">
<span>class <span class="ident">DenseNet</span></span>
<span>(</span><span>layers:Â list, activation:Â type, batch_norm:Â bool, use_softmax:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DenseNet(nn.Module):

    def __init__(self,
                 layers: list,
                 activation: type,
                 batch_norm: bool,
                 use_softmax: bool):
        super(DenseNet, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))
        self.add_module(f&#39;linear_out&#39;, _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
        self.softmax = nn.Softmax() if use_softmax else None

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        for i in range(len(self._layers) - 2):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        x = getattr(self, f&#39;linear_out&#39;)(x)
        if self.softmax:
            x = self.softmax(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.DenseNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DenseNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DenseNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.DenseNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    register_module_call(self)
    x = TORCH.as_tensor(x)
    for i in range(len(self._layers) - 2):
        x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
        if self._batch_norm:
            x = getattr(self, f&#39;norm{i}&#39;)(x)
    x = getattr(self, f&#39;linear_out&#39;)(x)
    if self.softmax:
        x = self.softmax(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.DenseResNetBlock"><code class="flex name class">
<span>class <span class="ident">DenseResNetBlock</span></span>
<span>(</span><span>layers, batch_norm, activation)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DenseResNetBlock(nn.Module):

    def __init__(self, layers, batch_norm, activation):
        super(DenseResNetBlock, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-1], layers[1:])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))

    def forward(self, x):
        x0 = x
        for i in range(len(self._layers) - 1):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        return x + x0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.DenseResNetBlock.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DenseResNetBlock.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DenseResNetBlock.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.DenseResNetBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x0 = x
    for i in range(len(self._layers) - 1):
        x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
        if self._batch_norm:
            x = getattr(self, f&#39;norm{i}&#39;)(x)
    return x + x0</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.DoubleConv"><code class="flex name class">
<span>class <span class="ident">DoubleConv</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, mid_channels:Â int, batch_norm:Â bool, activation:Â type, periodic:Â bool, kernel_size=3)</span>
</code></dt>
<dd>
<div class="desc"><p>(convolution =&gt; [BN] =&gt; ReLU) * 2</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoubleConv(nn.Module):
    &#34;&#34;&#34;(convolution =&gt; [BN] =&gt; ReLU) * 2&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool, kernel_size=3):
        super().__init__()
        self.add_module(&#39;double_conv&#39;, nn.Sequential(
            CONV[d](in_channels, mid_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](mid_channels) if batch_norm else nn.Identity(),
            activation(),
            CONV[d](mid_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True)
        ))

    def forward(self, x):
        return self.double_conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.DoubleConv.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DoubleConv.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.DoubleConv.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.DoubleConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    return self.double_conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.Down"><code class="flex name class">
<span>class <span class="ident">Down</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, batch_norm:Â bool, activation:Â Union[str,Â type], use_res_blocks:Â bool, periodic, kernel_size:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Downscaling with maxpool then double conv or resnet_block</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Down(nn.Module):
    &#34;&#34;&#34;Downscaling with maxpool then double conv or resnet_block&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic, kernel_size: int):
        super().__init__()
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        if use_res_blocks:
            self.add_module(&#39;conv&#39;, ResNetBlock(d, in_channels, out_channels, batch_norm, activation, periodic, kernel_size))
        else:
            self.add_module(&#39;conv&#39;, DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic, kernel_size))

    def forward(self, x):
        x = self.maxpool(x)
        return self.conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.Down.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.Down.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.Down.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.Down.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.maxpool(x)
    return self.conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.InvertibleNet"><code class="flex name class">
<span>class <span class="ident">InvertibleNet</span></span>
<span>(</span><span>num_blocks:Â int, construct_net, construction_kwargs:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InvertibleNet(nn.Module):
    def __init__(self, num_blocks: int, construct_net, construction_kwargs: dict):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        for i in range(num_blocks):
            self.add_module(f&#39;coupling_block{i + 1}&#39;, CouplingLayer(construct_net, construction_kwargs, (i % 2 == 0)))

    def forward(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.InvertibleNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.InvertibleNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.InvertibleNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.InvertibleNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, backward=False) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, backward=False):
    if backward:
        for i in range(self.num_blocks, 0, -1):
            x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
    else:
        for i in range(1, self.num_blocks + 1):
            x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.ResNet"><code class="flex name class">
<span>class <span class="ident">ResNet</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ResNet, self).__init__()
        self.layers = layers
        if len(self.layers) &lt; 1:
            layers.append(out_channels)
        self.add_module(&#39;Res_in&#39;, ResNetBlock(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Res{i}&#39;, ResNetBlock(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
        self.add_module(&#39;Res_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = TORCH.as_tensor(x)
        x = getattr(self, &#39;Res_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Res{i}&#39;)(x)
        x = getattr(self, &#39;Res_out&#39;)(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.ResNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ResNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ResNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.ResNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = TORCH.as_tensor(x)
    x = getattr(self, &#39;Res_in&#39;)(x)
    for i in range(1, len(self.layers)):
        x = getattr(self, f&#39;Res{i}&#39;)(x)
    x = getattr(self, &#39;Res_out&#39;)(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.ResNetBlock"><code class="flex name class">
<span>class <span class="ident">ResNetBlock</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, batch_norm, activation, periodic:Â bool, kernel_size=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResNetBlock(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool, kernel_size=3):
        # Since in_channels and out_channels might be different, we need a sampling layer for up/down sampling input in order to add it as a skip connection
        super(ResNetBlock, self).__init__()
        if in_channels != out_channels:
            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        else:
            self.sample_input = nn.Identity()
            self.bn_sample = nn.Identity()
        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)
        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=kernel_size, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.conv1(x)))
        out = self.activation()(self.bn2(self.conv2(out)))
        out = (out + self.bn_sample(self.sample_input(x)))
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.ResNetBlock.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ResNetBlock.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.ResNetBlock.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.ResNetBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = TORCH.as_tensor(x)
    out = self.activation()(self.bn1(self.conv1(x)))
    out = self.activation()(self.bn2(self.conv2(out)))
    out = (out + self.bn_sample(self.sample_input(x)))
    return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.UNet"><code class="flex name class">
<span>class <span class="ident">UNet</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, filters:Â tuple, batch_norm:Â bool, activation:Â type, periodic:Â bool, use_res_blocks:Â bool, down_kernel_size:Â int, up_kernel_size:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UNet(nn.Module):

    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, down_kernel_size: int, up_kernel_size: int):
        super(UNet, self).__init__()
        self._levels = len(filters)
        self._spatial_rank = d
        if use_res_blocks:
            self.add_module(&#39;inc&#39;, ResNetBlock(d, in_channels, filters[0], batch_norm, activation, periodic, down_kernel_size))
        else:
            self.add_module(&#39;inc&#39;, DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic, down_kernel_size))
        for i in range(1, self._levels):
            self.add_module(f&#39;down{i}&#39;, Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks, down_kernel_size))
            self.add_module(f&#39;up{i}&#39;, Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks, up_kernel_size))
        self.add_module(&#39;outc&#39;, CONV[d](filters[0], out_channels, kernel_size=1))

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        x = self.inc(x)
        xs = [x]
        for i in range(1, self._levels):
            x = getattr(self, f&#39;down{i}&#39;)(x)
            xs.insert(0, x)
        for i in range(1, self._levels):
            x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
        x = self.outc(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.UNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.UNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.UNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.UNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    register_module_call(self)
    x = TORCH.as_tensor(x)
    x = self.inc(x)
    xs = [x]
    for i in range(1, self._levels):
        x = getattr(self, f&#39;down{i}&#39;)(x)
        xs.insert(0, x)
    for i in range(1, self._levels):
        x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
    x = self.outc(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.backend.torch.nets.Up"><code class="flex name class">
<span>class <span class="ident">Up</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, batch_norm:Â bool, activation:Â type, periodic:Â bool, use_res_blocks:Â bool, kernel_size:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Upscaling then double conv</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Up(nn.Module):
    &#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;

    _MODES = [None, &#39;linear&#39;, &#39;bilinear&#39;, &#39;trilinear&#39;]

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, kernel_size: int):
        super().__init__()
        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
        if use_res_blocks:
            conv = ResNetBlock(d, in_channels, out_channels, batch_norm, activation, periodic, kernel_size)
        else:
            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic, kernel_size)
        self.add_module(&#39;up&#39;, up)
        self.add_module(&#39;conv&#39;, conv)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
        #                 diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phiml.backend.torch.nets.Up.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.Up.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.torch.nets.Up.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.torch.nets.Up.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2):
    x1 = self.up(x1)
    # input is CHW
    # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
    # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
    #                 diffY // 2, diffY - diffY // 2])
    # if you have padding issues, see
    # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
    # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
    x = torch.cat([x2, x1], dim=1)
    return self.conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phiml.backend.torch" href="index.html">phiml.backend.torch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="phiml.backend.torch.nets.adagrad" href="#phiml.backend.torch.nets.adagrad">adagrad</a></code></li>
<li><code><a title="phiml.backend.torch.nets.adam" href="#phiml.backend.torch.nets.adam">adam</a></code></li>
<li><code><a title="phiml.backend.torch.nets.conv_classifier" href="#phiml.backend.torch.nets.conv_classifier">conv_classifier</a></code></li>
<li><code><a title="phiml.backend.torch.nets.conv_net" href="#phiml.backend.torch.nets.conv_net">conv_net</a></code></li>
<li><code><a title="phiml.backend.torch.nets.coupling_layer" href="#phiml.backend.torch.nets.coupling_layer">coupling_layer</a></code></li>
<li><code><a title="phiml.backend.torch.nets.get_mask" href="#phiml.backend.torch.nets.get_mask">get_mask</a></code></li>
<li><code><a title="phiml.backend.torch.nets.get_parameters" href="#phiml.backend.torch.nets.get_parameters">get_parameters</a></code></li>
<li><code><a title="phiml.backend.torch.nets.invertible_net" href="#phiml.backend.torch.nets.invertible_net">invertible_net</a></code></li>
<li><code><a title="phiml.backend.torch.nets.load_state" href="#phiml.backend.torch.nets.load_state">load_state</a></code></li>
<li><code><a title="phiml.backend.torch.nets.mlp" href="#phiml.backend.torch.nets.mlp">mlp</a></code></li>
<li><code><a title="phiml.backend.torch.nets.res_net" href="#phiml.backend.torch.nets.res_net">res_net</a></code></li>
<li><code><a title="phiml.backend.torch.nets.rmsprop" href="#phiml.backend.torch.nets.rmsprop">rmsprop</a></code></li>
<li><code><a title="phiml.backend.torch.nets.save_state" href="#phiml.backend.torch.nets.save_state">save_state</a></code></li>
<li><code><a title="phiml.backend.torch.nets.sgd" href="#phiml.backend.torch.nets.sgd">sgd</a></code></li>
<li><code><a title="phiml.backend.torch.nets.u_net" href="#phiml.backend.torch.nets.u_net">u_net</a></code></li>
<li><code><a title="phiml.backend.torch.nets.update_weights" href="#phiml.backend.torch.nets.update_weights">update_weights</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phiml.backend.torch.nets.ConvClassifier" href="#phiml.backend.torch.nets.ConvClassifier">ConvClassifier</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.ConvClassifier.call_super_init" href="#phiml.backend.torch.nets.ConvClassifier.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvClassifier.dump_patches" href="#phiml.backend.torch.nets.ConvClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvClassifier.forward" href="#phiml.backend.torch.nets.ConvClassifier.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvClassifier.training" href="#phiml.backend.torch.nets.ConvClassifier.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.ConvNet" href="#phiml.backend.torch.nets.ConvNet">ConvNet</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.ConvNet.call_super_init" href="#phiml.backend.torch.nets.ConvNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvNet.dump_patches" href="#phiml.backend.torch.nets.ConvNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvNet.forward" href="#phiml.backend.torch.nets.ConvNet.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ConvNet.training" href="#phiml.backend.torch.nets.ConvNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.CouplingLayer" href="#phiml.backend.torch.nets.CouplingLayer">CouplingLayer</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.CouplingLayer.call_super_init" href="#phiml.backend.torch.nets.CouplingLayer.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.CouplingLayer.dump_patches" href="#phiml.backend.torch.nets.CouplingLayer.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.CouplingLayer.forward" href="#phiml.backend.torch.nets.CouplingLayer.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.CouplingLayer.training" href="#phiml.backend.torch.nets.CouplingLayer.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.DenseNet" href="#phiml.backend.torch.nets.DenseNet">DenseNet</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.DenseNet.call_super_init" href="#phiml.backend.torch.nets.DenseNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseNet.dump_patches" href="#phiml.backend.torch.nets.DenseNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseNet.forward" href="#phiml.backend.torch.nets.DenseNet.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseNet.training" href="#phiml.backend.torch.nets.DenseNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.DenseResNetBlock" href="#phiml.backend.torch.nets.DenseResNetBlock">DenseResNetBlock</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.DenseResNetBlock.call_super_init" href="#phiml.backend.torch.nets.DenseResNetBlock.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseResNetBlock.dump_patches" href="#phiml.backend.torch.nets.DenseResNetBlock.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseResNetBlock.forward" href="#phiml.backend.torch.nets.DenseResNetBlock.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DenseResNetBlock.training" href="#phiml.backend.torch.nets.DenseResNetBlock.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.DoubleConv" href="#phiml.backend.torch.nets.DoubleConv">DoubleConv</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.DoubleConv.call_super_init" href="#phiml.backend.torch.nets.DoubleConv.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DoubleConv.dump_patches" href="#phiml.backend.torch.nets.DoubleConv.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DoubleConv.forward" href="#phiml.backend.torch.nets.DoubleConv.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.DoubleConv.training" href="#phiml.backend.torch.nets.DoubleConv.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.Down" href="#phiml.backend.torch.nets.Down">Down</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.Down.call_super_init" href="#phiml.backend.torch.nets.Down.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Down.dump_patches" href="#phiml.backend.torch.nets.Down.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Down.forward" href="#phiml.backend.torch.nets.Down.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Down.training" href="#phiml.backend.torch.nets.Down.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.InvertibleNet" href="#phiml.backend.torch.nets.InvertibleNet">InvertibleNet</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.InvertibleNet.call_super_init" href="#phiml.backend.torch.nets.InvertibleNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.InvertibleNet.dump_patches" href="#phiml.backend.torch.nets.InvertibleNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.InvertibleNet.forward" href="#phiml.backend.torch.nets.InvertibleNet.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.InvertibleNet.training" href="#phiml.backend.torch.nets.InvertibleNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.ResNet" href="#phiml.backend.torch.nets.ResNet">ResNet</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.ResNet.call_super_init" href="#phiml.backend.torch.nets.ResNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNet.dump_patches" href="#phiml.backend.torch.nets.ResNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNet.forward" href="#phiml.backend.torch.nets.ResNet.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNet.training" href="#phiml.backend.torch.nets.ResNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.ResNetBlock" href="#phiml.backend.torch.nets.ResNetBlock">ResNetBlock</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.ResNetBlock.call_super_init" href="#phiml.backend.torch.nets.ResNetBlock.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNetBlock.dump_patches" href="#phiml.backend.torch.nets.ResNetBlock.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNetBlock.forward" href="#phiml.backend.torch.nets.ResNetBlock.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.ResNetBlock.training" href="#phiml.backend.torch.nets.ResNetBlock.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.UNet" href="#phiml.backend.torch.nets.UNet">UNet</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.UNet.call_super_init" href="#phiml.backend.torch.nets.UNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.UNet.dump_patches" href="#phiml.backend.torch.nets.UNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.UNet.forward" href="#phiml.backend.torch.nets.UNet.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.UNet.training" href="#phiml.backend.torch.nets.UNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.torch.nets.Up" href="#phiml.backend.torch.nets.Up">Up</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.torch.nets.Up.call_super_init" href="#phiml.backend.torch.nets.Up.call_super_init">call_super_init</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Up.dump_patches" href="#phiml.backend.torch.nets.Up.dump_patches">dump_patches</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Up.forward" href="#phiml.backend.torch.nets.Up.forward">forward</a></code></li>
<li><code><a title="phiml.backend.torch.nets.Up.training" href="#phiml.backend.torch.nets.Up.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>