{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Unified Neural Network Training\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Networks.ipynb)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)\n",
    "\n",
    "Œ¶<sub>ML</sub> provides basic tools to set up and train neural networks.\n",
    "Users can choose one of several standard network architectures and configure them in one line of code.\n",
    "After defining a loss function, they can then be trained in a unified way or using library-specific functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install phiml\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Architectures\n",
    "\n",
    "The [neural network API](phiml/nn) includes multiple common network architectures, such as [multi-layer perceptrons](phiml/nn#phiml.nn.mlp),\n",
    "[U-Nets](phiml/nn#phiml.nn.u_net), [convolutional networks](phiml/nn#phiml.nn.conv_net), [ResNets](phiml/nn#phiml.nn.res_net), [invertible networks](phiml/nn#phiml.nn.invertible_net), and [VGG-like networks](phiml/nn#phiml.nn.conv_classifier).\n",
    "All networks operating on grids are supported in 1D, 2D and 3D for periodic and non-periodic domains."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from phiml import math, nn\n",
    "math.use('torch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training an MLP\n",
    "\n",
    "Let's create an MLP and train it to learn a 1D sine function.\n",
    "First, we set up the network with two hidden layers of 64 neurons each."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlp = nn.mlp(in_channels=1, out_channels=1, layers=[64, 64], activation='ReLU')\n",
    "mlp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this is a standard neural network of the chosen library (a PyTorch Module in this case).\n",
    "This allows you to quickly try many network designs without writing much code, even if you don't use Œ¶<sub>ML</sub> for anything else.\n",
    "\n",
    "Next, let's generate some training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET = math.batch(examples=100)\n",
    "x = math.random_uniform(DATASET, low=-math.PI, high=math.PI)\n",
    "y = math.sin(x) + math.random_normal(DATASET) * .1\n",
    "plt.scatter(x.numpy(), y.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the loss function, we will use the $L^2$ comparing prediction to labels.\n",
    "We use [`math.native_call()`](phiml/math#phiml.math.native_call) to call the network with native tensors reshaped to the preferences of the current backend library.\n",
    "This will pass tensors of shape BCS to PyTorch and BSC to TensorFlow and Jax, where B denotes the packed batch dimensions, C the packed channel dimensions and S all spatial dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_function(network_input, label):\n",
    "    prediction = math.native_call(mlp, network_input)\n",
    "    return math.l2_loss(prediction - label), prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create an optimizer, such as [`SGD`](phiml/nn#phiml.nn.sgd), [`Adam`](phiml/nn#phiml.nn.adam), [`AdaGrad`](phiml/nn#phiml.nn.adagrad), or [`RMSprop`](phiml/nn#phiml.nn.rmsprop).\n",
    "The returned optimizer is again a library-specific type (in this case a PyTorch optimizer)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = nn.adam(mlp, learning_rate=5e-3)\n",
    "optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we repeatedly update the weights using backprop.\n",
    "Here, we use [`update_weights()`](phiml/nn#phiml.nn.update_weights), passing in the network, optimizer, loss function, and loss function arguments.\n",
    "If you want more control over the training loop, you can use library-specific functions as well, since the network and optimizer do not depend on Œ¶<sub>ML</sub>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    loss, pred = nn.update_weights(mlp, optimizer, loss_function, x, y)\n",
    "plt.scatter(x.numpy(), pred.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a U-Net\n",
    "\n",
    "Following the principles of Œ¶<sub>ML</sub>, U-Nets, as well as many other network architectures, can [operate on 1D, 2D or 3D](N_Dimensional.html) grids.\n",
    "The number of spatial dimensions, `in_spatial`, must be specified when creating the network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "u_net = nn.u_net(in_channels=1, out_channels=1, levels=4, periodic=False, in_spatial=2)\n",
    "optimizer = nn.adam(u_net, 1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train the network to turn a 2D grid of random noise into a Gaussian distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SPATIAL = math.spatial(x=64, y=64)\n",
    "x = math.random_normal(SPATIAL)\n",
    "y = math.exp(-.5 * math.vec_squared((math.meshgrid(SPATIAL) / SPATIAL) - .5))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.imshow(x.numpy('y,x'))\n",
    "ax2.imshow(y.numpy('y,x'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train the network!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_function():\n",
    "    prediction = math.native_call(u_net, x)\n",
    "    return math.l2_loss(prediction - y), prediction\n",
    "\n",
    "for i in range(200):\n",
    "    loss, pred = nn.update_weights(u_net, optimizer, loss_function)\n",
    "\n",
    "plt.imshow(pred.numpy('y,x'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Reading\n",
    "\n",
    "[üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}